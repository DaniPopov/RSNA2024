{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lumbar Spine Degenerative Classification using Deep Learning üíª‚öïÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors: Daniel Popov (315784173), Yotam Gershi (315784173)\n",
    "*Computer Science Students*  \n",
    "*Passionate about leveraging AI to solve challenging problems in the medical field* üë®‚Äç‚öïÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are **Daniel Popov** and **Yotam Gershi**, computer science students passionate about leveraging artificial intelligence to tackle challenging problems in the medical field. In this project, we explore the **RSNA 2024 Lumbar Spine Degenerative Classification competition dataset**, which focuses on classifying degenerative changes in the lumbar spine using medical imaging.\n",
    "\n",
    "The primary goal of this project is to explore the dataset thoroughly and develop deep learning models capable of accurately classifying various types of degenerative changes in the lumbar spine. This work aims to enhance the accuracy and efficiency of diagnostic workflows in clinical settings.\n",
    "\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "Spinal degeneration is becoming increasingly prevalent worldwide, particularly within aging populations. Degenerative changes in the lumbar spine are often associated with chronic pain and diminished quality of life. There is a pressing need for automated tools to assist radiologists in evaluating spine health, improving diagnostic accuracy, and optimizing treatment planning. Our project aligns with this need, aiming to build AI-based tools that support healthcare professionals in managing these conditions more effectively.\n",
    "\n",
    "\n",
    "#### Approach\n",
    "\n",
    "This project will guide you through the following steps:\n",
    "\n",
    "1. **Understanding the problem**: We start by analyzing the dataset's objectives and structure.\n",
    "2. **Exploratory Data Analysis (EDA)**: We‚Äôll conduct a comprehensive analysis of the CSV and image files to gain insights into the dataset, uncover patterns, detect anomalies, and understand class distributions. This step ensures we understand the data's nuances and are prepared to build effective models.\n",
    "3. **Data Preprocessing**: We'll prepare the data for modeling, addressing issues such as missing values, class imbalance, and image normalization.\n",
    "4. **Model Development**: Using state-of-the-art deep learning techniques, we will train and evaluate models to classify degenerative changes in lumbar spine images.\n",
    "5. **Performance Evaluation**: We will assess our models using appropriate metrics and iterate on improvements.\n",
    "\n",
    "\n",
    "#### Impact\n",
    "\n",
    "Our work contributes to the growing field of AI in healthcare, aiming to make spinal health assessment more accurate, efficient, and accessible. By automating aspects of lumbar spine degeneration classification, this project seeks to reduce the workload of radiologists and improve patient outcomes.\n",
    "\n",
    "\n",
    "Let‚Äôs get started! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Degenerative spine conditions significantly impact quality of life, often causing pain, reduced mobility, and diminished overall well-being. Accurate identification and assessment of these conditions using medical imaging are critical for developing effective treatment plans and improving patient outcomes.\n",
    "\n",
    "The **RSNA 2024 Lumbar Spine Degenerative Classification Challenge** focuses on automating the detection and grading of three key types of degenerative conditions in the lumbar spine from medical images:\n",
    "\n",
    "1. **Foraminal Narrowing**  \n",
    "   This condition occurs when the foramina‚Äîthe passageways through which spinal nerves exit the spinal canal‚Äîbecome compressed. This narrowing can occur on either the left or right side and often leads to significant nerve-related symptoms.\n",
    "\n",
    "2. **Subarticular Stenosis**  \n",
    "   Subarticular stenosis refers to the narrowing of the space beneath the articular processes of the spine, where nerve roots pass. This condition can result in nerve compression, causing pain and neurological symptoms depending on whether it occurs on the left or right side.\n",
    "\n",
    "3. **Canal Stenosis**  \n",
    "   Canal stenosis involves narrowing of the central spinal canal, which houses the spinal cord and nerve roots. The severity of symptoms can range from mild discomfort to significant neurological deficits, depending on the degree of compression.\n",
    "\n",
    "Each of these conditions can appear at various levels within the lumbar spine, specifically around each vertebral disc. For example, the **L4/L5** level corresponds to the disc between the fourth (L4) and fifth (L5) lumbar vertebrae. The challenge requires predicting the degree of compression at these levels and classifying them as **normal**, **mild**, **moderate**, or **severe**.\n",
    "\n",
    "The dataset includes MRI images of the lumbar spine, annotated by expert radiologists to indicate the presence and severity of these conditions. Participants are tasked with building machine learning models that can classify these conditions accurately, assisting radiologists in diagnosing degenerative spine conditions more efficiently and consistently.\n",
    "\n",
    "By leveraging advanced computer vision and deep learning techniques, this challenge seeks to improve the diagnostic accuracy and reliability of spinal degeneration assessments, ultimately enhancing patient care.\n",
    "\n",
    "For more details on the dataset and challenge, visit the [RSNA 2024 Lumbar Spine Degenerative Classification competition page](https://www.kaggle.com/competitions/rsna-2024-lumbar-spine-degenerative-classification).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomical Overview ü¶¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spine is divided into four regions:\n",
    "\n",
    "- **Cervical region**: Contains 7 vertebral bodies.\n",
    "- **Thoracic region**: Contains 12 vertebral bodies.\n",
    "- **Lumbar region**: Contains 5 vertebral bodies.\n",
    "- **Sacral region**: Contains 3-5 fused vertebral bodies.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"https://prod-images-static.radiopaedia.org/images/53655832/Gray-square.001_big_gallery.jpeg\" alt=\"Description\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Between each vertebral body in all regions (except the sacrum) lies a **vertebral disc**. These discs act as cushions, providing flexibility and absorbing shock.\n",
    "\n",
    "Along the posterior aspect of each vertebral body lies the **spinal cord**, a vital structure that transmits signals between the brain and the rest of the body. \n",
    "\n",
    "At each vertebral body, **spinal nerves** exit the spinal cord through openings between the vertebral bodies called **foramina**. These nerves are responsible for transmitting sensory and motor information to and from different parts of the body.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <img src=\"https://files.miamineurosciencecenter.com/media/filer_public_thumbnails/filer_public/78/1e/781e78be-8980-466f-8a82-83a5c8350770/herniated_disc_larger.jpg__720.0x600.0_q85_subject_location-360%2C300_subsampling-2.jpg\" alt=\"Description\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Compression of the spinal cord or any of the spinal nerves can cause significant pain and discomfort to patients. Several factors can lead to such compression, including:\n",
    "\n",
    "- **Bulging vertebral disc**: When the disc protrudes beyond its normal boundary, it can press on nearby nerves or the spinal cord.\n",
    "- **Degenerative changes in the bones**: These changes can lead to the growth of bony protrusions (osteophytes) or compression of the vertebrae themselves.\n",
    "- **Trauma**: Injuries to the spine can result in displacement or fractures that compress the spinal cord or nerves.\n",
    "- **Thickening of ligaments**: Ligaments surrounding the spinal cord may thicken over time, contributing to reduced space and nerve compression.\n",
    "\n",
    "### **Foraminal Narrowing**\n",
    "\n",
    "The spinal cord has spinal nerves that exit the spinal canal through openings called **foramina**. The foramina are best viewed in the **sagittal plane**. Occasionally, these openings can become compressed, resulting in **foraminal narrowing**. This compression causes pain along the nerve distribution downstream of the affected area.\n",
    "\n",
    "- **Left image**: A sagittal MRI slice where the foramina are visible. Crosshairs indicate where the foramina exit the spinal canal.  \n",
    "- **Right image**: Grading criteria for the degree of compression (note: for this challenge, **Normal/Mild** is grouped into one label).\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <div style=\"margin-right: 50px; flex: 1; display: flex; justify-content: flex-end;\">\n",
    "        <img src=\"https://i.imgur.com/6c7erNM.png\" alt=\"Image 1 Description\" width=\"300\"/>\n",
    "    </div>\n",
    "    <div style=\"margin-left: 50px; flex: 1; display: flex; justify-content: flex-start;\">\n",
    "        <img src=\"https://i.imgur.com/b1VGiN5.png\" alt=\"Image 2 Description\" width=\"300\"/>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### **Subarticular Stenosis**\n",
    "\n",
    "**Subarticular stenosis** occurs due to compression of the spinal cord in the **subarticular zone**, which can be best visualized in the **axial plane**.\n",
    "\n",
    "- **Left image**: A schematic illustrating the relevant anatomical zone.  \n",
    "- **Right image**: Grading criteria for determining the degree of subarticular stenosis (note: for this challenge, **Normal/Mild** is grouped into one label).\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <div style=\"margin-right: 50px; flex: 1; display: flex; justify-content: flex-end;\">\n",
    "        <img src=\"https://files.miamineurosciencecenter.com/media/filer_public_thumbnails/filer_public/d5/08/d508ae6a-a4f2-4796-be9f-455f8df45fe1/herniation_zones.jpg__1700.0x1308.0_q85_subject_location-850%2C656_subsampling-2.jpg\" alt=\"Image 1 Description\" width=\"300\"/>\n",
    "    </div>\n",
    "    <div style=\"margin-left: 50px; flex: 1; display: flex; justify-content: flex-start;\">\n",
    "        <img src=\"https://i.imgur.com/Usuxgge.png\" alt=\"Image 2 Description\" width=\"300\"/>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "**Canal Stenosis**\n",
    "\n",
    "### **Canal Stenosis**\n",
    "\n",
    "**Canal stenosis** refers to impingement on the **spinal canal**, where the spinal cord travels. Impingement can result from:\n",
    "\n",
    "- **Bulging vertebral disc**: Protrusion of the disc into the canal.\n",
    "- **Trauma**: Injury that alters the alignment or structure of the spinal canal.\n",
    "- **Bony osteophytes**: Outgrowths of the vertebral bodies caused by degenerative changes.\n",
    "- **Ligamental thickening**: Thickening of the ligaments that run along the spinal canal.\n",
    "\n",
    "The degree of compression is best assessed in the **axial plane**.\n",
    "\n",
    "- **Left image**: Canal stenosis visible in the sagittal plane, providing an overview of its appearance.  \n",
    "- **Right image**: Grading criteria for canal stenosis (note: for this challenge, **Normal/Mild** is grouped into one label).\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; align-items: center;\">\n",
    "    <div style=\"margin-right: 50px; flex: 1; display: flex; justify-content: flex-end;\">\n",
    "        <img src=\"https://prod-images-static.radiopaedia.org/images/940993/f7a8adca63efae788f621869cc21e8_big_gallery.jpg\" alt=\"Image 1 Description\" width=\"300\"/>\n",
    "    </div>\n",
    "    <div style=\"margin-left: 50px; flex: 1; display: flex; justify-content: flex-start;\">\n",
    "        <img src=\"https://i.imgur.com/opjnAwl.png\" alt=\"Image 2 Description\" width=\"300\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imaging Overview ü©ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRI imaging of the spine can be performed in three planes: the **axial plane**, the **sagittal plane**, and the **coronal plane**. In our dataset, the two primary image types are from the axial and sagittal planes:\n",
    "\n",
    "- **Axial Plane**: Captures horizontal slices (perpendicular to the spine) across the body from top to bottom. These images are useful for assessing the spine and surrounding structures in cross-section.\n",
    "- **Sagittal Plane**: Captures vertical slices (parallel to the spine) from left to right. Sagittal images provide a side view of the spine, which is essential for evaluating the alignment and curvature of the spinal column.\n",
    "\n",
    "MRI images are typically classified as either **T1-weighted** or **T2-weighted**:\n",
    "\n",
    "- **T1-Weighted Images**: Highlight fat as bright. For example, the inner parts of bones, which contain fatty marrow, appear brighter on T1 images. These images are often used to evaluate the anatomy of the spine and the surrounding soft tissues.\n",
    "- **T2-Weighted Images**: Highlight water as bright, making fluids like the cerebrospinal fluid (CSF) in the spinal canal appear brighter. These images are particularly useful for detecting abnormalities related to water content, such as inflammation, edema, and other pathological changes.\n",
    "\n",
    "> Unlike CT images, MRI images are not standardized regarding pixel values. The intensity values in MRI images do not have a fixed scale and can vary between different scanners and settings. As a result, standardizing these images before analysis might be necessary depending on the approach.\n",
    "\n",
    "The images in this dataset are stored in the **DICOM format** (Digital Imaging and Communications in Medicine), which is the standard format for storing and transmitting medical images. DICOM is widely used in hospitals and clinics for its ability to encapsulate both image data and metadata.\n",
    "\n",
    "**Key Features of DICOM Images**:\n",
    "- **Embedded Metadata**: Each DICOM file contains image data along with metadata, such as patient demographics, study descriptions, imaging modality, acquisition parameters, and more. This metadata is critical for accurate interpretation and analysis.\n",
    "- **Multi-frame Support**: DICOM files can store a series of images (e.g., multiple slices from an MRI scan) within a single file, allowing for efficient management of large datasets.\n",
    "- **Lossless Compression**: DICOM images often use lossless compression to preserve full fidelity, ensuring no critical information is lost during storage or transmission.\n",
    "- **Interoperability**: The DICOM standard ensures compatibility across different imaging equipment and software, making it easier to manage and analyze medical images from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is organized into the following directory structure, which includes folders for training and test images, as well as several CSV files containing metadata and labels:\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ /kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/\n",
    "    ‚îú‚îÄ‚îÄ test_images/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ 1005139/\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 609308237/\n",
    "    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 1.dcm\n",
    "    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îú‚îÄ‚îÄ test_series_descriptions.csv\n",
    "    ‚îú‚îÄ‚îÄ train_images/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ 4003253/\n",
    "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 702807833/\n",
    "    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ 1.dcm\n",
    "    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "    ‚îú‚îÄ‚îÄ train_label_coordinates.csv\n",
    "    ‚îú‚îÄ‚îÄ train_series_descriptions.csv\n",
    "    ‚îî‚îÄ‚îÄ train.csv\n",
    "```\n",
    "\n",
    "### **Explanation of Key Components**\n",
    "\n",
    "- **`train_images`**:  \n",
    "  This directory contains the MRI images for the training set. The images are organized into subdirectories, where each subdirectory corresponds to a unique study ID. Within each patient-specific folder, there are additional subfolders representing different imaging series or studies. The actual MRI slices are stored as DICOM (`.dcm`) files, the standard format for medical imaging.\n",
    "\n",
    "- **`test_images`**:  \n",
    "  This directory contains the MRI images for the test set. The images are organized into subdirectories, where each subdirectory corresponds to a unique study ID. Within each patient-specific folder, there are additional subfolders representing different imaging series or studies. The actual MRI slices are stored as DICOM (`.dcm`) files, the standard format for medical imaging.\n",
    "\n",
    "- **`train.csv`**:  \n",
    "  This file contains patient study identifiers (`study_id`) and labels for various degenerative spine conditions at different lumbar levels. Each condition's severity is classified as **Normal/Mild**, **Moderate**, or **Severe** across multiple spinal regions.\n",
    "\n",
    "- **`train_label_coordinates.csv`**:  \n",
    "  This file contains detailed information about specific regions of interest (ROIs) within the training images. The coordinates provided in this file localize areas affected by degenerative conditions, helping to understand the spatial distribution and severity of these changes.\n",
    "\n",
    "- **`train_series_descriptions.csv`**:  \n",
    "  This CSV file includes metadata for each series in the training set. It provides additional information about the MRI sequences, such as the imaging plane (axial or sagittal), sequence type (T1-weighted or T2-weighted), and other acquisition parameters.\n",
    "\n",
    "- **`test_series_descriptions.csv`**:  \n",
    "  This CSV file includes metadata for each series in the test set. It provides additional information about the MRI sequences, such as the imaging plane (axial or sagittal), sequence type (T1-weighted or T2-weighted), and other acquisition parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports üì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import cv2\n",
    "import pydicom\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPyImage, display\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import timm\n",
    "\n",
    "import yaml\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a custom color scheme to the plots\n",
    "import urllib.request\n",
    "url = \"https://raw.githubusercontent.com/h4pZ/rose-pine-matplotlib/main/themes/rose-pine-dawn.mplstyle\"\n",
    "save_path = \"/Users/danipopov/Projects/RSNA2024/data/rose-pine-dawn.mplstyle\"  # Include the file name\n",
    "urllib.request.urlretrieve(url, save_path)\n",
    "plt.style.use(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exploratory Data Analysis (EDA) üîç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data üîÑ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset for the Lumbar Spine Degenerative Classification problem. We load the train, label coordinates, and series descriptions CSV files, which provide details about the spinal conditions and image metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/Users/danipopov/Projects/RSNA2024/data/train.csv\") \n",
    "label_coords_df = pd.read_csv(\"/Users/danipopov/Projects/RSNA2024/data/train_label_coordinates.csv\")  \n",
    "series_desc_df = pd.read_csv(\"/Users/danipopov/Projects/RSNA2024/data/train_series_descriptions.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for CSV Files üïµüìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train.csv üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train.csv` file provides the primary labels for the training dataset. It includes a unique identifier for each patient's study (`study_id`) and columns representing various degenerative conditions at different spinal levels, such as spinal canal stenosis, neural foraminal narrowing, and subarticular stenosis, each labeled by specific lumbar spine levels (e.g., L1/L2, L2/L3, etc.). The values in each of these columns indicate the severity of the degeneration, categorized as `Normal/Mild`, `Moderate`, or `Severe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show five first row of the train_df\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we discover that we have 1975 rows and 10 columns, which means that we have 1975 studies with 25 labels for different conditions.\n",
    "Let's check if there are any null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we have some missing data, which means that for some `study_id`, we have missing labels for some conditions.\n",
    "\n",
    "A heatmap is a great tool to effectively visualize missing data, showing where values are present or missing. Dark purple areas indicate data is available, while yellow lines highlight missing values, helping identify patterns to address before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(train_df.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Heatmap of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we can observe is that, on average, the **missing rows have 2-3 missing values**, with some rows having more missing values. Most of the missing values occur in the `subarticular stenosis`. We will remember to address this later.\n",
    "\n",
    "Next, we will visualize the distribution of severity labels (Normal/Mild, Moderate, and Severe) across foraminal narrowing, subarticular stenosis, and spinal canal stenosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude 'study_id' from the columns list\n",
    "columns = [col for col in train_df.columns if col != 'study_id'] \n",
    "\n",
    "# Set up subplots for the three conditions\n",
    "figure, axis = plt.subplots(1, 3, figsize=(12, 8))\n",
    "# Loop through the conditions and plot each one\n",
    "for idx, d in enumerate(['foraminal', 'subarticular', 'canal']):\n",
    "    # Select diagnosis columns related to the current condition\n",
    "    diagnosis = [col for col in train_df.columns if d in col]\n",
    "    # Melt the DataFrame to convert columns into rows for easier plotting\n",
    "    melted_df = pd.melt(train_df[diagnosis], value_vars=diagnosis, var_name='diagnosis', value_name='severity')\n",
    "    # Countplot for the current diagnosis\n",
    "    sns.countplot(data=melted_df, x='diagnosis', hue='severity', ax=axis[idx], palette='muted', hue_order=['Normal/Mild', 'Moderate', 'Severe'])\n",
    "    axis[idx].set_title(f'{d.capitalize()} Distribution')\n",
    "    axis[idx].tick_params(axis='x', rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of labels fall into the `Normal/Mild` category, highlighting a potential **class imbalance** that may impact the model's ability to detect the less frequent but clinically significant \"Moderate\" and \"Severe\" cases. To address this imbalance, we may apply techniques like class weighting, oversampling/undersampling, or stratified splitting during model training.\n",
    "\n",
    "Another method to visualize the distribution of the \"Normal/Mild,\" \"Moderate,\" and \"Severe\" labels across various diagnoses is by utilizing pie charts. This approach may provide additional insights compared to our previous visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_charts(diagnosis_type, num_rows, num_cols, fig_size):\n",
    "\n",
    "    figure, axis = plt.subplots(num_rows, num_cols, figsize=fig_size)\n",
    "    \n",
    "    # Filter diagnosis columns based on the given diagnosis type\n",
    "    diagnosis_cols = [col for col in train_df.columns if diagnosis_type in col]\n",
    "    \n",
    "    # Loop through each diagnosis column and create a pie chart\n",
    "    for idx, d in enumerate(diagnosis_cols):\n",
    "        dff = train_df[d]\n",
    "        value_counts = dff.value_counts()\n",
    "        \n",
    "        # Plot the pie chart\n",
    "        axis[idx//num_cols, idx%num_cols].pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
    "        axis[idx//num_cols, idx%num_cols].set_title(d)\n",
    "    \n",
    "    # Adjust layout for better visibility\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for foraminal narrowing\n",
    "plot_pie_charts('foraminal', 5, 2, (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for subarticular stenosis\n",
    "plot_pie_charts('subarticular', 5, 2, (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for spinal canal stenosis\n",
    "plot_pie_charts('canal', 3, 2, (10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis indicates that across all diagnoses, including subarticular stenosis, foraminal narrowing, and canal stenosis, certain spine levels exhibit a small percentage of Severe cases. However, a significant imbalance is observed within the Moderate category, suggesting that both Severe and Moderate cases warrant further consideration during the data preprocessing phase.\n",
    "\n",
    "Following our exploration of the label distribution across various conditions and the examination of the correlation matrix, we will now proceed to analyze the distribution of labels by spinal level. This analysis will enable us to assess how the severity of conditions (Normal/Mild, Moderate, Severe) is distributed across different spinal levels: L1/L2, L2/L3, L3/L4, L4/L5, and L5/S1. \n",
    "\n",
    "Understanding this distribution is crucial for identifying which spinal levels are most significantly impacted by degeneration and determining whether certain levels exhibit a higher prevalence of severe cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the columns of each level\n",
    "l1_l2_cols = [col for col in columns if 'l1_l2' in col]\n",
    "l2_l3_cols = [col for col in columns if 'l2_l3' in col]\n",
    "l3_l4_cols = [col for col in columns if 'l3_l4' in col]\n",
    "l4_l5_cols = [col for col in columns if 'l4_l5' in col]\n",
    "l5_s1_cols = [col for col in columns if 'l5_s1' in col]\n",
    "\n",
    "def plot_label_distribution(df, level, cols, title):\n",
    "    # Melt the DataFrame to convert columns into rows for easier plotting\n",
    "    melted_df = pd.melt(df[cols], value_vars=cols, var_name=f'{level}_diagnosis', value_name='severity')\n",
    "    \n",
    "    # Countplot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(data=melted_df, x=f'{level}_diagnosis', hue='severity', palette='muted', hue_order=['Normal/Mild', 'Moderate', 'Severe'])\n",
    "    plt.title(f'{title} Label Distribution')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for L1/L2\n",
    "plot_label_distribution(train_df, 'L1/L2', l1_l2_cols, 'L1/L2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for L2/L3\n",
    "plot_label_distribution(train_df, 'L2/L3', l2_l3_cols, 'L2/L3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for L3/L4\n",
    "plot_label_distribution(train_df, 'L3/L4', l3_l4_cols, 'L3/L4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for L4/L5\n",
    "plot_label_distribution(train_df, 'L4/L5', l4_l5_cols, 'L4/L5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for L5/S1\n",
    "plot_label_distribution(train_df, 'L5/S1', l5_s1_cols, 'L5/S1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of labels across all spinal levels (L1/L2 to L5/S1) reveals that the majority of cases are categorized as \"Normal/Mild,\" highlighting a significant class imbalance. Moderate and Severe cases are considerably less frequent, with Severe cases being particularly rare. Notably, as we progress to lower spinal levels (e.g., L3/L4, L4/L5, and L5/S1), there is a **slight increase** in the proportion of Moderate and Severe cases, especially for conditions such as neural foraminal narrowing and subarticular stenosis. This imbalance, particularly in the lower spine, underscores the necessity of considering a model structure that can accommodate different spinal levels to effectively learn from the severe cases.\n",
    "\n",
    "\n",
    "We will now proceed to the next tool for exploratory data analysis (EDA), the `correlation matrix`. This tool provides valuable insights into the relationships between various spinal conditions. A correlation value close to 1 indicates a strong positive relationship, while a value close to -1 signifies a strong negative relationship. These insights are instrumental in determining whether certain conditions are interrelated, thereby aiding in our understanding of the progression of spinal degeneration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the df to convert categorical values to numeric\n",
    "train_df_for_corr = train_df.copy()\n",
    "train_df_for_corr.replace({\"Normal/Mild\": 0, \"Moderate\": 1, \"Severe\": 2}, inplace=True)\n",
    "\n",
    "# Find all columns \n",
    "canal_columns = [col for col in columns if 'canal' in col]\n",
    "foraminal_columns = [col for col in columns if 'foraminal' in col]\n",
    "subarticular_columns = [col for col in columns if 'subarticular' in col]\n",
    "\n",
    "# Define a function to plot heatmap\n",
    "def plot_corr_heatmap(data, title, figsize=(10, 8)):\n",
    "    corr_matrix = data.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall correlation matrix\n",
    "plot_corr_heatmap(train_df_for_corr[columns], 'Correlation Matrix', figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canal columns correlation matrix\n",
    "plot_corr_heatmap(train_df_for_corr[canal_columns], 'Correlation Matrix For Canal Columns', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foraminal columns correlation matrix\n",
    "plot_corr_heatmap(train_df_for_corr[foraminal_columns], 'Correlation Matrix For Foraminal Columns', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subarticular columns correlation matrix\n",
    "plot_corr_heatmap(train_df_for_corr[subarticular_columns], 'Correlation Matrix For Subarticular Columns', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrices, we observe that certain spinal conditions demonstrate moderate to strong positive correlations, particularly within the same category (e.g., foraminal narrowing or subarticular stenosis across different levels). This indicates that degeneration at one spinal level is often associated with degeneration at adjacent levels. Furthermore, we note that within the subarticular and foraminal columns, there exists a correlation between the right and left sides. \n",
    "\n",
    "This observation will be taken into account when addressing missing labels and during subsequent preprocessing steps. Understanding these relationships is crucial for predicting the potential progression of degeneration throughout the spine.\n",
    "\n",
    "Next, we will address the imbalance between the 'Severe' and 'Moderate' labels. First, we will count the number of rows that contain at least one occurrence of these labels and then sum the frequency of each label across the dataset. This analysis will enable us to quantify the extent of the imbalance between the two labels.\n",
    "\n",
    "Additionally, we will check for rows containing missing values (NaNs) and save the indices of those rows. This will facilitate the handling of any potential data issues related to incomplete information during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_label_counts = train_df.copy()\n",
    "\n",
    "# Find each rows contain severe or moderate cases\n",
    "train_df_label_counts['contains_Severe'] = train_df.apply(lambda row: 'Severe' in row.values, axis=1)\n",
    "train_df_label_counts['contains_Moderate'] = train_df.apply(lambda row: 'Moderate' in row.values, axis=1)\n",
    "\n",
    "# How much severe and moderate cases in each row\n",
    "train_df_label_counts['Severe_count'] = train_df.apply(lambda x: (x == 'Severe').sum(), axis=1)\n",
    "train_df_label_counts['Moderate_count'] = train_df.apply(lambda x: (x == 'Moderate').sum(), axis=1)\n",
    "\n",
    "# Sum how many rows contains severe and moderate cases\n",
    "severe_count = train_df_label_counts['contains_Severe'].sum()\n",
    "moderate_count = train_df_label_counts['contains_Moderate'].sum()\n",
    "\n",
    "# Count how many rows contain the Severe and Moderate labels\n",
    "print(f'How many rows contain the Severe label: {severe_count}, and the Moderate label: {moderate_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we observe that 980 rows contain the severe label, while 1963 rows contain the moderate label. As noted at the beginning of the exploratory data analysis (EDA), the `train_df` consists of 1975 rows, indicating that nearly all rows have at least one moderate label. This suggests that almost all patients have at least one moderate condition and half of the patients have at least one secere label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for rows with NaN values\n",
    "rows_with_null_labels = train_df_label_counts.isna().any(axis=1)\n",
    "num_rows_with_null = rows_with_null_labels.sum()\n",
    "\n",
    "print(f'How many rows contain at least one Null value: {num_rows_with_null}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered that there are **185** rows containing at least one NaN value. This means we can choose to either remove them, which would reduce the amount of training and validation data and potentially affect model performance, or find a way to fill the NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the study_ids of rows with NaN values\n",
    "indices_of_NaN = train_df_label_counts[train_df_label_counts.isna().any(axis=1)]['study_id'].tolist()\n",
    "\n",
    "# Saving the study_ids of rows containing 'Severe'\n",
    "indices_of_Severe = train_df_label_counts[train_df_label_counts.apply(lambda row: 'Severe' in row.values, axis=1)]['study_id'].tolist()\n",
    "\n",
    "# Saving the study_ids of rows containing 'Moderate'\n",
    "indices_of_Moderate = train_df_label_counts[train_df_label_counts.apply(lambda row: 'Moderate' in row.values, axis=1)]['study_id'].tolist()\n",
    "\n",
    "# Find the study_ids that contain both 'Severe' and 'Moderate'\n",
    "indices_of_Severe_and_Moderate = train_df_label_counts[train_df_label_counts.apply(lambda row: 'Severe' in row.values and 'Moderate' in row.values, axis=1)]['study_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many rows have both Severe and Moderate\n",
    "print(f'Number of rows with both Severe and Moderate: {len(indices_of_Severe_and_Moderate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will examine the distribution of the 'Severe' and 'Moderate' labels within the dataset. Our objective is to understand the frequency of these labels in each row, which represents spinal conditions across various levels. We will visualize the counts of severe and moderate cases to further illustrate the extent of the label imbalance.\n",
    "\n",
    "This analysis will provide valuable insights into the distribution of 'Severe' and 'Moderate' cases among patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_df = pd.DataFrame(train_df_label_counts['Severe_count'])\n",
    "\n",
    "# Plot the Severe count distribution for each row\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(severe_df, x='Severe_count', bins=50)\n",
    "plt.title('Severe Count')\n",
    "plt.xlabel('Number of severe count')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_df = pd.DataFrame(train_df_label_counts['Moderate_count'])\n",
    "\n",
    "# Plot the Moderate count distribution for each row\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(severe_df, x='Moderate_count', bins=30)\n",
    "plt.title('Moderate Count')\n",
    "plt.xlabel('Number of moderate count')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histograms, we observe that most rows have very few or no `Severe` labels (the majority having zero or one severe label), indicating a significant imbalance, with very few instances containing multiple 'Severe' conditions. On the other hand, the distribution of `Moderate` labels is more evenly spread, but there is still a noticeable imbalance, with many rows containing fewer moderate conditions. This further reinforces the need to address this imbalance during model training, possibly through resampling techniques or by adjusting class weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for train.csv analysis:** üí°üìÅ\n",
    "\n",
    "1. **Data Quality:**\n",
    "   - 185 rows (out of 1974 total) contain at least one null value, indicating some missing data.\n",
    "\n",
    "2. **Class Imbalance:**\n",
    "   - Significant imbalance exists, with \"Normal/Mild\" being the most common category.\n",
    "   - Out of 1974 studies:\n",
    "     - 980 contain at least one Severe label\n",
    "     - 1693 contain at least one Moderate label\n",
    "\n",
    "3. **Spinal Level Distribution:**\n",
    "   - \"Normal/Mild\" cases dominate across all spinal levels (L1/L2 to L5/S1).\n",
    "   - Lower spinal levels show a slight increase in Moderate and Severe cases.\n",
    "\n",
    "4. **Correlation Patterns:**\n",
    "   - Moderate to strong positive correlations observed between spinal conditions, especially within the same type.\n",
    "   - Right and left sides show notable correlation for subarticular and foraminal columns.\n",
    "\n",
    "5. **Multi-level Involvement:**\n",
    "   - 952 studies have both Severe and Moderate labels, indicating cases with varying severity across multiple spinal levels.\n",
    "\n",
    "This analysis highlights the need to address class imbalance and missing data in subsequent preprocessing and modeling steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_label_coordinates.csv üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_label_coordinates.csv` file provides essential metadata for the localization of degenerative conditions in the images. It includes the patient's study ID (`study_id`), the series of images (`series_id`), and the specific image number within the series (`instance_number`). For each image, it provides coordinates (`x`, `y`) that can be useful for localization tasks such as identifying the specific areas of degeneration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_coords_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_coords_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we have 48,692 rows and 7 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "label_coords_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! There are no missing values in the label_coords_df, so we don‚Äôt need to worry about any gaps in the data here.\n",
    "\n",
    "In `train.csv`, we only had the `study_id` column, but now with the `train_label_coordinates.csv` file, we also have information about `series_id`. This allows us to explore how many unique series are associated with each study. \n",
    "\n",
    "We will group the data by `study_id` and count the number of unique `series_id` for each study. This step helps us understand the variability in MRI series across studies and may reveal patterns related to patient scans, such as studies with additional imaging series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique series_id for each study_id\n",
    "unique_series_per_study = label_coords_df.groupby('study_id')['series_id'].nunique().reset_index(name='unique_series_count')\n",
    "\n",
    "# Plot the distribution of unique series per study id\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data=unique_series_per_study, x='unique_series_count', bins=20, kde=True)\n",
    "plt.title('Distribution of Unique Series per Study ID')\n",
    "plt.xlabel('Number of Unique Series per Study')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution shows that most studies have exactly three unique series, with a small number of studies having four or five series. This suggests that the majority of studies follow a consistent data collection protocol, with three series per study. However, some studies include additional series, possibly indicating more complex cases or different imaging needs for certain patients.\n",
    "\n",
    "Before moving forward with the exploration, we will check if we have the coordinates for each of the conditions and spinal levels by using a value count on the `study_id` in `train_label_coordinates.csv`. This will help us determine whether every study in the dataset has the expected number of condition labels. Studies with fewer labels may indicate missing information, which could affect model training. By identifying these cases, we can handle them appropriately in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_coords_df['study_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sum of condition per study id \n",
    "sum_of_condition_per_study = label_coords_df['study_id'].value_counts()\n",
    "value_counts_df = pd.DataFrame(sum_of_condition_per_study, columns=['count'])\n",
    "\n",
    "# Plot the distribution of condition per study id \n",
    "plt.figure(figsize=(8,4))\n",
    "sns.histplot(data=value_counts_df, x='count', bins=10)\n",
    "plt.title('Distribution of Value Count for Study IDs')\n",
    "plt.xlabel('Number of conditions per Study ID')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram reveals that the majority of studies have exactly 25 conditions, which aligns with the expected number of spinal levels and conditions. However, a small number of studies have fewer than 25 conditions. This may indicate missing or incomplete data for certain patients, meaning that not all spinal levels have corresponding coordinates.\n",
    "\n",
    "To address these cases later on, we will save the indices of all studies that do not have exactly 25 conditions. These studies might require special handling during the modeling process to ensure we do not introduce bias due to incomplete data.\n",
    "\n",
    "If some of these cases involve the right or left side, we can complete them based on the opposite side‚Äôs coordinates by simply adjusting the value for the x-coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the indices of study IDs that do not have 25 conditions\n",
    "incomplete_study_ids = value_counts_df[value_counts_df['count'] != 25].index.to_list()\n",
    "\n",
    "# Print the number of incomplete studies\n",
    "print(f'Number of studies with fewer than 25 conditions: {len(incomplete_study_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We observed something interesting** there are 185 studies in the `train.csv` that have NaN values, and we also found 185 studies in the `train_label_coordinates.csv` with fewer than 25 conditions. Now, based on the study IDs we saved earlier, we can check if the missing data in `train.csv` aligns with the incomplete data in `train_label_coordinates.csv`. This will help us determine if the missing data is consistent across both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the study_ids with missing values from train_df match with incomplete_study_ids\n",
    "matching_study_ids = set(indices_of_NaN).intersection(set(incomplete_study_ids))\n",
    "\n",
    "print(f'Number of matching study IDs between train_df and label_coords_df: {len(matching_study_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We discovered that 177 study IDs match between `train.csv` and `train_label_coordinates.csv`, meaning most of the studies with missing data align across both datasets, but not all of them. We will now save the `matching_study_ids` for further analysis and also track the study IDs that do not match in each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the non-matching study IDs for each dataset\n",
    "non_matching_train_df_ids = set(indices_of_NaN) - set(matching_study_ids)\n",
    "non_matching_label_coords_df_ids = set(incomplete_study_ids) - set(matching_study_ids)\n",
    "\n",
    "# Print the results\n",
    "print(f'Number of non-matching study IDs in train_df: {len(non_matching_train_df_ids)}')\n",
    "print(f'Number of non-matching study IDs in label_coords_df: {len(non_matching_label_coords_df_ids)}')\n",
    "\n",
    "# Save or use these lists for further analysis\n",
    "matching_study_ids = list(matching_study_ids)\n",
    "non_matching_train_df_ids = list(non_matching_train_df_ids)\n",
    "non_matching_label_coords_df_ids = list(non_matching_label_coords_df_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will proceed to examine the number of unique images (`instance numbers`) utilized per study ID. This analysis will assist us in understanding the variation in the quantity of images available for each study and may uncover interesting patterns, such as whether certain studies possess a greater volume of imaging data than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique series_id for each study_id\n",
    "unique_instance_per_study = label_coords_df.groupby(['study_id'])['instance_number'].nunique().reset_index(name='unique_instance_number_count')\n",
    "\n",
    "# Plot distribution of instance number of images \n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data=unique_instance_per_study, x='unique_instance_number_count',bins=50, kde=True)\n",
    "plt.title('Distribution of Instance Numbers')\n",
    "plt.xlabel('Number of Unique Instances')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot reveals that the number of unique instance numbers per study generally ranges between 8 and 14, with a peak around 10. This suggests that most studies have a similar number of images available, with only a few studies having a higher or lower number of unique instance numbers. The consistency in the number of images per study may indicate a standardized imaging protocol for most studies, while the studies with a higher or lower number of instances might reflect special cases or differing imaging requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize x and y coordinates to detect outliers\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='x', y='y', data=label_coords_df)\n",
    "plt.title('Scatter plot of X and Y coordinates')\n",
    "plt.xlabel('X coordinate')\n",
    "plt.ylabel('Y coordinate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot reveals a dense cluster of coordinates, indicating that most points fall within a certain range of `x` and `y` values. However, we can spot a few outliers, particularly at the lower end of the `x` and `y` scales. This visualization helps us confirm that the majority of coordinates are tightly grouped, with only a few deviations.\n",
    "\n",
    "Now that we have completed exploring the `study_id`, `series_id`, and `instance_number` columns, we will move on to analyze the `x` and `y` coordinate distributions. These coordinates are crucial for localization tasks, and visualizing them will allow us to detect any potential outliers or patterns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='x', y='y', hue='level', data=label_coords_df)\n",
    "plt.title('Scatter plot of X and Y coordinates by Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='x', y='y', hue='condition', data=label_coords_df)\n",
    "plt.title('Scatter plot of X and Y coordinates by condition')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plots grouped by spine level and condition, we observe that the coordinates are fairly well-distributed across the different levels and conditions. However, there are noticeable clusters in specific areas for certain spine levels and conditions, which may be related to the anatomical structure of the spine and the regions typically affected by each condition. \n",
    "\n",
    "There is also a possibility of outliers in the `x` and `y` coordinates. If this is the case, we will need to consider how much we can trust these coordinates for use in our model or how we might address these issues.\n",
    "\n",
    "We will explore the distribution of the `x` and `y` coordinates to gain insights into the spatial characteristics of the dataset. Visualizing the distribution of these values will help us understand the range and common locations of the coordinates across different studies. This can also assist in detecting any outliers or anomalies in the spatial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x coordinaten distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=label_coords_df, x='x', kde=True)\n",
    "plt.title('Distribution of x')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y coordinaten distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=label_coords_df, x='y', kde=True)\n",
    "plt.title('Distribution of y')\n",
    "plt.xlabel('y values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the `x` coordinates shows a clear concentration of values between 150 and 350, indicating that the majority of the spinal coordinates fall within this range. There is a noticeable peak around 200, suggesting that most of the degenerative areas are localized in this region. However, there are some **outliers** toward both lower and higher `x` values, which could represent edge cases or anomalies in the dataset.\n",
    "\n",
    "Similarly, the distribution of `y` coordinates exhibits a comparable pattern, with most of the values concentrated between 150 and 400. The peak at around 200-250 for both `x` and `y` coordinates suggests that the dataset is capturing similar regions of interest across multiple studies. A few **outliers** can also be seen in the higher range, indicating rare cases where the coordinates deviate from the general trend. This visualization helps confirm the consistency in spatial localization across studies, with a few exceptional cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_severity(df):\n",
    "    df['severity'] = None\n",
    "\n",
    "    # Iterate over rows in the dataframe\n",
    "    for idx, coor_row in df.iterrows():\n",
    "        try:\n",
    "            # Construct the patient_severity key\n",
    "            patient_severity = f\"{coor_row['condition'].lower().replace(' ', '_')}_{coor_row['level'].lower().replace('/','_')}\"\n",
    "            \n",
    "            # Filter for the corresponding study_id\n",
    "            study_row = train_df[train_df['study_id'] == int(coor_row['study_id'])]\n",
    "            \n",
    "            # Check if the key exists in train_df for that study\n",
    "            if patient_severity in study_row.columns:\n",
    "                severity = study_row[patient_severity].values[0]\n",
    "                df.at[idx, 'severity'] = severity  # Update the severity\n",
    "            else:\n",
    "                print(f\"Warning: {patient_severity} not found in train_df for study_id {coor_row['study_id']}.\")\n",
    "                df.at[idx, 'severity'] = 'Unknown'  # Set as 'Unknown' if no match found\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing study_id: {coor_row['study_id']} - {e}\")\n",
    "            df.at[idx, 'severity'] = 'Unknown'  # Set as 'Unknown' in case of an error\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "label_coords_df_counts = label_coords_df.copy()\n",
    "label_coords_df_counts = add_severity(label_coords_df_counts)\n",
    "label_coords_df_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "sns.kdeplot(x='x', y='y', data=label_coords_df_counts[label_coords_df_counts['severity'].isin(['Severe', 'Moderate'])], \n",
    "            hue='condition', fill=True, cmap=\"Reds\", thresh=0.05)\n",
    "plt.title('Density Plot of Severe and Moderate Condition Locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the density plot, we can observe the most frequent areas of severe and moderate conditions.\n",
    "These visualizations indicate that certain conditions tend to cluster around similar `x` and `y` \n",
    "coordinate locations, showing potential patterns in the dataset. \n",
    "\n",
    "This information might be \n",
    "valuable when training models that need to understand the spatial relationships of these \n",
    "conditions in medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for train_label_coordinates.csv analysis:** üí°üìÅ\n",
    "\n",
    "1. **Data Quality:**\n",
    "   - No missing values (NaN) were found in the `train_label_coordinates.csv` file..\n",
    "   - Most study_ids are associated with 3 series_ids.\n",
    "   - A smaller portion (approximately 300) have 4 series_ids.\n",
    "   - Not all labels from `train.csv` have corresponding coordinates in this file.\n",
    "   - **177 study_ids** with missing data in `train.csv` match those without complete coordinate sets (25 coordinates per condition) in this file.\n",
    "\n",
    "2. **Study and Series Distribution:**\n",
    "   - Most study_ids are associated with 3 series_ids.\n",
    "   - A smaller portion (approximately 300) have 4 series_ids.\n",
    "   - A very small number of studies have 5 series_ids.\n",
    "\n",
    "3. **Instance Number Distribution:**\n",
    "   - The majority of study_ids use 10-12 instance images for the conditions.\n",
    "   - The overall range of instance numbers is between 6 and 17.\n",
    "\n",
    "4. **Coordinate Distribution:**\n",
    "   - X coordinates:\n",
    "      - Concentrated between 150 and 350.\n",
    "      - Notable peak around 200.\n",
    "   - Y-coordinates:\n",
    "      - Similar pattern, concentrated between 150 and 400.\n",
    "      - Peak observed around 200-250.\n",
    "\n",
    "5. **Implications:**\n",
    "   - The consistent range of coordinates suggests a standardized imaging area across most studies.\n",
    "   - Variations in series and instance numbers may indicate differences in imaging protocols or patient-specific factors.\n",
    "   - The matching missing data between `train.csv` and this file suggests systematic issues in data collection or processing for certain studies.\n",
    "\n",
    "This analysis provides insights into the structure and quality of the coordinate data, highlighting areas that may require attention in preprocessing and modeling stages, particularly regarding missing data and potential standardization of imaging protocols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_series_descriptions.csv üìÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_series_descriptions.csv` file provides essential metadata for the description of each series of images (`series_id`). We will start by checking for missing values in the file and then compare the distribution of the series count per study ID with the one from the `label_coords_df` to ensure consistency across the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_desc_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_desc_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we have 6294 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "series_desc_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the `train_series_descriptions.csv` file, which is great. Since this file contains metadata about the image series, we can now move on to checking the distribution of the series count per study ID and compare it with the distribution from `label_coords_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by study_id to get the count of series per study\n",
    "series_per_study_df = series_desc_df.groupby(['study_id'])['series_id'].count().reset_index(name='series_count')\n",
    "\n",
    "# Plot the distribution of series count per study\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(data=series_per_study_df, x='series_count', kde=True)\n",
    "plt.title('Distribution of Series Count per Study')\n",
    "plt.xlabel('Number of Series per Study')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the series count per study in `series_desc_df` appears to be consistent with the distribution in `label_coords_df`. This confirms that both datasets follow a similar pattern regarding the number of series associated with each study.\n",
    "\n",
    "\n",
    "Next, we will explore the `series_description` column in `series_desc_df`. This column provides categorical information about each series, and understanding its distribution will give us insights into the types of imaging series present in the dataset. We will visualize this using both a pie chart and a bar plot to see the relative proportions and absolute counts of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each series_description\n",
    "series_description_counts = series_desc_df['series_description'].value_counts()\n",
    "\n",
    "# Plot a pie chart to visualize the distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "series_description_counts.plot.pie(autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Series Description')\n",
    "plt.ylabel('')  # Hide the y-label for aesthetics\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot to visualize the series description counts\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=series_description_counts.index, y=series_description_counts.values)\n",
    "plt.title('Frequency of Series Descriptions')\n",
    "plt.xlabel('Series Description')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `series_description` across the dataset shows a fairly even split between three main categories: **Axial T2**, **Sagittal T1**, and **Sagittal T2/STIR**. From the pie chart, we observe that **Axial T2** constitutes the largest portion, representing around 37.2% of the total series. Meanwhile, **Sagittal T1** and **Sagittal T2/STIR** are almost equally represented, with both accounting for roughly 31.4% and 31.5% respectively.\n",
    "\n",
    "The bar plot further confirms this distribution, showing that all three types of imaging series are well-represented in the dataset, with **Axial T2** slightly more prevalent. This balanced distribution suggests that the dataset contains a diverse range of imaging modalities, which could provide robust information for training models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for train_series_descriptions.csv analysis:** üí°üìÅ\n",
    "\n",
    "1.  **Data Quality:**\n",
    "    - No missing values in the train_series_descriptions.csv file, which is excellent for data integrity.\n",
    "\n",
    "2.  **Series Distribution:**\n",
    "    - The distribution of series count per study is consistent with the distribution observed in label_coords_df.\n",
    "    - Most studies have 3 series, with a small portion (close to 300) having 4 series, and a very small portion having 5 series.\n",
    "\n",
    "3.  **Series Description Distribution:**\n",
    "    - The dataset shows a fairly even split between three main categories:\n",
    "        - Axial T2: ~37.2% of the total series.\n",
    "        - Sagittal T1: ~31.4% of the total series.\n",
    "        - Sagittal T2/STIR: ~31.5% of the total series.\n",
    "\n",
    "4.  **Imaging Modalities:**\n",
    "    - The balanced distribution of imaging modalities (Axial T2, Sagittal T1, and Sagittal T2/STIR) suggests a diverse range of imaging data, which could provide robust information for training models.\n",
    "\n",
    "5.  **Data Structure:**\n",
    "    - The file contains essential metadata for each series of images, including study_id, series_id, and series_description.\n",
    "\n",
    "This analysis reveals a well-structured dataset with a balanced distribution of imaging modalities. The absence of missing values and the consistency with other parts of the dataset (like label_coords_df) indicate good data quality. The even distribution of different imaging types (Axial T2, Sagittal T1, and Sagittal T2/STIR) suggests that the dataset contains a comprehensive range of spinal imaging data, which could be beneficial for developing robust machine learning models for spinal condition analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can proceed to the EDA of the images ü©ªü©ª."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA for Images üïµü©ª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll explore the DICOM (Digital Imaging and Communications in Medicine) images in our dataset. DICOM is the standard format for medical imaging.\n",
    "\n",
    "Understanding these images is essential for our analysis, as they contain valuable information about patient health and potential abnormalities. By visualizing and examining the DICOM files, we'll gain insights into the structure of the spinal cord and assess the quality of our imaging data. This step is critical for building accurate models and deriving meaningful conclusions in our medical imaging project.\n",
    "\n",
    "We'll use specialized tools to load and display these DICOM images, as well as explore the rich metadata they contain. This metadata provides crucial context about the imaging process, patient positioning, and other technical details that can influence our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display an image\n",
    "image_path  = r'/Users/danipopov/Projects/RSNA2024/data/train_images/4003253/702807833/1.dcm'\n",
    "ds = pydicom.dcmread(image_path)\n",
    "pxl_arr = ds.pixel_array\n",
    "\n",
    "def plot_dicom_img(dicom_array):\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Visualize the image with enhanced contrast using 'bone' colormap\n",
    "    axes[0].imshow(dicom_array, cmap=plt.cm.bone)\n",
    "    axes[0].axis('off')\n",
    "    axes[0].set_title('Bone')\n",
    "\n",
    "    # Visualize the image with enhanced contrast using 'gray' colormap\n",
    "    axes[1].imshow(dicom_array, cmap='gray')\n",
    "    axes[1].axis('off')\n",
    "    axes[1].set_title('Gray')\n",
    "\n",
    "    # Visualize the image with default colormap\n",
    "    axes[2].imshow(dicom_array)\n",
    "    axes[2].axis('off')\n",
    "    axes[2].set_title('Default')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_dicom_img(pxl_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DICOM metadata\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DICOM file contains a wealth of metadata, We can see details such as:\n",
    "\n",
    "- **Patient ID**: Identifies the patient associated with the scan.\n",
    "- **Series Description**: Provides the imaging modality used (e.g., `T2`).\n",
    "- **Slice Thickness**: Specifies the thickness of each MRI slice (`4.0 mm`).\n",
    "- **Spacing Between Slices**: Indicates the distance between consecutive slices (`4.8 mm`).\n",
    "- **Patient Position**: Describes the patient's position during the scan (`Head First-Supine`).\n",
    "- **Image Dimensions**: The image is `640x640` pixels with a pixel spacing of `0.46875 mm`.\n",
    "- **Photometric Interpretation**: The image is in grayscale (`MONOCHROME2`).\n",
    "\n",
    "Understanding this metadata helps us assess the imaging quality and provides valuable context for future image analysis, especially when working with models like U-Net or ResNet, which may require certain image preprocessing steps such as resizing, normalization, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Metadata for Each Study ID will help us to streamline the process of displaying and organizing images for each study id, we will create a structured metadata dictionary. This metadata will help us easily access images based on their `study_id` and `series_id`. For each scan, the metadata object will contain:\n",
    "\n",
    "- The path to the folder containing the images (`folder_path`).\n",
    "- An array of Series Instance IDs that represent different series within the study(`series_ids`).\n",
    "- An array of Series Descriptions that describe the imaging modality or scan type (e.g., T2, T1).\n",
    "\n",
    "```\n",
    "meta_df = {\n",
    "    study_id: {\n",
    "        'folder_path': ...,        # Path to the folder containing the images\n",
    "        'series_ids': [ ... ],     # List of SeriesInstanceUIDs for the study\n",
    "        'series_desc': [ ... ]     # List of descriptions for each series (T2, T1, etc.)\n",
    "    },\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of all the study IDs and paths to their images\n",
    "images_dir_path = r'/Users/danipopov/Projects/RSNA2024/data/train_images'\n",
    "study_id_list = os.listdir(images_dir_path)\n",
    "study_id_paths = [(x, f\"{images_dir_path}/{x}\") for x in study_id_list]\n",
    "\n",
    "# Initialize the metadata dictionary\n",
    "meta_df = {}\n",
    "\n",
    "# Process each study and its series\n",
    "for study_id, study_folder_path in study_id_paths:\n",
    "    series_ids = []\n",
    "    series_descriptions = []\n",
    "    \n",
    "    # Get all the series IDs (folders) within the study folder\n",
    "    try:\n",
    "        series_folders = os.listdir(study_folder_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Folder not found for study {study_id}. Skipping this study.\")\n",
    "        continue  # Skip this study if the folder doesn't exist\n",
    "\n",
    "    # Process each series in the study folder\n",
    "    for series_id in series_folders:\n",
    "        try:\n",
    "            # Fetch the series description from the dataframe\n",
    "            series_description = series_desc_df[series_desc_df['series_id'] == int(series_id)]['series_description'].iloc[0]\n",
    "        except (IndexError, ValueError):\n",
    "            # Handle cases where series_id is not found in the dataframe or can't be converted to int\n",
    "            series_description = 'Unknown'\n",
    "\n",
    "        # Append series ID and description to the lists\n",
    "        series_ids.append(series_id)\n",
    "        series_descriptions.append(series_description)\n",
    "    \n",
    "    # Add metadata for the current study_id\n",
    "    meta_df[int(study_id)] = {\n",
    "        'folder_path': study_folder_path,\n",
    "        'series_ids': series_ids,\n",
    "        'series_descriptions': series_descriptions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of meta_df object\n",
    "keys = list(meta_df.keys())[:4]  # Get the first 4 keys\n",
    "values = [meta_df[key] for key in keys]  # Get the values corresponding to the keys\n",
    "\n",
    "# Display the key-value pairs\n",
    "for key, value in zip(keys, values):\n",
    "    print(f'Key: {key} \\n')\n",
    "    print(f'Value: {value} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, we displayed examples from our `meta_df` structure. Each entry in this dictionary represents a study, identified by the `study_id`, along with its associated folder path, series IDs, and corresponding series descriptions. This metadata structure will facilitate efficient access to and exploration of the DICOM images for each study, allowing us to load and visualize the images based on their `study_id` and `series_id`. This will also streamline any future analysis involving individual series types, such as 'Axial T2' or 'Sagittal T1'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(patient_meta_data, max_images_per_row=4):\n",
    "    # Extract folder path, series IDs, and descriptions from metadata\n",
    "    file_path = patient_meta_data['folder_path']\n",
    "    series_ids = patient_meta_data['series_ids']\n",
    "    series_descriptions = patient_meta_data['series_descriptions']\n",
    "    \n",
    "    # Loop through each series to display images\n",
    "    for idx, series_id in enumerate(series_ids):\n",
    "        # Construct the path to the series folder\n",
    "        series_id_path = os.path.join(file_path, series_id)\n",
    "        # Find all DICOM files in the series directory\n",
    "        images = [img for img in glob.glob(f'{series_id_path}/*.dcm')]\n",
    "        num_images  = len(images)\n",
    "\n",
    "        if num_images == 0:\n",
    "            print(f\"No images found in series: {series_descriptions[idx]} ({series_id})\")\n",
    "            continue  # Skip series if no images are found\n",
    "\n",
    "        num_rows = (num_images + max_images_per_row - 1) // max_images_per_row\n",
    "        # Create a grid of subplots\n",
    "        fig, axes = plt.subplots(num_rows, max_images_per_row, figsize=(5, 1.5 * num_rows))\n",
    "        # Flatten axes if multiple rows, or convert to list for single row\n",
    "        if num_rows > 1:\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes]\n",
    "\n",
    "        # Loop through images and plot each one\n",
    "        for i, image in enumerate(images):\n",
    "            try:\n",
    "                # Read DICOM file and extract the pixel array\n",
    "                ds = pydicom.dcmread(str(image))\n",
    "                pixel_array_numpy = ds.pixel_array\n",
    "                ax = axes[i]\n",
    "                ax.imshow(pixel_array_numpy, cmap=plt.cm.bone)\n",
    "                ax.axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image: {image}, Error: {e}\")     \n",
    "\n",
    "        # Turn off unused subplots\n",
    "        for i in range(num_images, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.suptitle(series_descriptions[idx], fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# Usage Example:\n",
    "key = keys[0]\n",
    "patient_meta_data = meta_df[key]\n",
    "display_images(patient_meta_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we displayed all the images for a selected `study_id` and its associated series. Each series is visualized in a grid layout. This function will be highly useful for visualizing the medical images before further analysis.\n",
    "\n",
    "We can notice that in each series, some images provide more insights into the anatomical structure and offer better visualization of the spine.\n",
    "\n",
    "Additionally, using the DICOM data, we will check the distribution of pixel sizes for each image across different series descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further explore the MRI data, it's useful to view the images in motion, particularly when analyzing slice-by-slice changes across a single series. By converting the series into an animated, we can simulate a 3D continuous scan, which helps in detecting patterns and abnormalities that may not be immediately obvious in static images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mri_3d(study_id, patient_meta_data, series_index=1):\n",
    "\n",
    "    # Extract folder path, series IDs, and descriptions from metadata\n",
    "    file_path = patient_meta_data['folder_path']\n",
    "    series_id = patient_meta_data['series_ids'][series_index]\n",
    "    series_description = patient_meta_data['series_descriptions'][series_index]\n",
    "\n",
    "    def load_dicom(path):\n",
    "        ds = pydicom.dcmread(path)\n",
    "        img = ds.pixel_array\n",
    "        img = cv2.normalize(img, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "        return img\n",
    "\n",
    "    # Construct the path to the series folder\n",
    "    series_id_path = os.path.join(file_path, series_id)\n",
    "\n",
    "    # Get sorted list of DICOM image paths\n",
    "    img_paths = glob.glob(f'{series_id_path}/*.dcm')\n",
    "    img_paths = sorted(img_paths, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "    # Load images from paths\n",
    "    imges = [load_dicom(img_path) for img_path in img_paths]\n",
    "\n",
    "    # Set up the animation\n",
    "    rc('animation', html='jshtml')\n",
    "\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    plt.axis('off')\n",
    "    im = plt.imshow(imges[0])\n",
    "    text = plt.text(0.05, 0.05, f'Slide {1}', transform=fig.transFigure, fontsize=16, color='darkblue')\n",
    "    title = plt.title(f'Study ID: {study_id}\\nSeries ID: {series_id}\\nDescription: {series_description}',\n",
    "                     fontsize=14, color='black')\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_array(imges[i])\n",
    "        text.set_text(f'Slide {i + 1}')\n",
    "        return im, text\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(imges), blit=True)\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    return anim\n",
    "\n",
    "# Usage\n",
    "key = keys[0]\n",
    "patient_meta_data = meta_df[key]\n",
    "display(mri_3d(key, patient_meta_data, series_index=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mri_3d(key, patient_meta_data, series_index=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mri_3d(key, patient_meta_data, series_index=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mri_3d(key, patient_meta_data, series_index=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully creating 3D repersntion of the MRI sequences, we can now visualize the anatomical changes across the slices.\n",
    "\n",
    "This dynamic representation helps us better understand the spatial progression of degeneration or abnormality. Moving forward, we will explore advanced image analyses by incorporating metadata such as slice coordinates and condition severity to detect correlations between slice location and the severity of degeneration.\n",
    "\n",
    "We can observe in the dynamic representation that our previous idea about some images in the layout showing better representation is confirmed. These images indeed provide more detailed information about spinal degenerative problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we recall from the *Introduction*, each of the spinal degenerative conditions can be best viewed from different types of images. Let's remind ourselves:\n",
    "\n",
    "- **Foraminal Narrowing** is best viewed in the sagittal plane\n",
    "\n",
    "- **Subarticular Stenosis** is best visualized in the axial plane\n",
    "\n",
    "- **Canal Stenosis** is also best viewed from the axial plane\n",
    "\n",
    "\n",
    "Next, we will move on to add the seires description and visualize all the coordinates given for a single image using the information in `train_label_coordinates.csv` and `meta_df`. This will help us understand if there's consistency in the values of the coordinates on a single image and potentially provide more insights into the data structure and annotation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_desc(df, meta_df):\n",
    "    df['series_desc'] = None\n",
    "\n",
    "    # Iterate over rows in the dataframe\n",
    "    for idx, coor_row in df.iterrows():\n",
    "        try:\n",
    "            # Find the meta_df for the study_id\n",
    "            meta_info = meta_df[int(coor_row['study_id'])]\n",
    "\n",
    "            # Find the index of the series_id in the meta_info\n",
    "            series_index = meta_info['series_ids'].index(str(coor_row['series_id']))\n",
    "\n",
    "            # Get the corresponding series description\n",
    "            series_desc = meta_info['series_descriptions'][series_index]\n",
    "\n",
    "            # Update the series_desc column\n",
    "            df.at[idx, 'series_desc'] = series_desc\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"Error processing study_id: {coor_row['study_id']} - Study ID not found in meta_df\")\n",
    "            df.at[idx, 'series_desc'] = 'Unknown'\n",
    "        except ValueError:\n",
    "            print(f\"Error processing study_id: {coor_row['study_id']} - Series ID not found in meta_df\")\n",
    "            df.at[idx, 'series_desc'] = 'Unknown'\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing study_id: {coor_row['study_id']} - {e}\")\n",
    "            df.at[idx, 'series_desc'] = 'Unknown'\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the function\n",
    "coords_with_desc = label_coords_df.copy()\n",
    "coords_with_desc = add_desc(coords_with_desc, meta_df)\n",
    "coords_with_desc.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_coor(images_dir_path, filtered_instances, study_id, series_id, instance_number):\n",
    "    from matplotlib.colors import LogNorm\n",
    "    import matplotlib.patches as patches\n",
    "\n",
    "    lag = 20\n",
    "    img_path = os.path.join(images_dir_path,\n",
    "                                str(study_id),\n",
    "                                str(series_id),\n",
    "                                str(instance_number) + '.dcm')\n",
    "\n",
    "    ds = pydicom.dcmread(img_path)\n",
    "    fig, ax  = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "    ax.imshow(ds.pixel_array) # cmap ='CMRmap' or 'bone'\n",
    "    ax.axis('off')\n",
    "    # Create a legend\n",
    "    legend_elements = []\n",
    "\n",
    "    a = 25 * max(ds.pixel_array.shape)/640\n",
    "\n",
    "    for _, row in filtered_instances.iterrows():\n",
    "        x, y = row['x'], row['y']\n",
    "\n",
    "        rect2 = patches.Rectangle((x - a, y - a), 2*a, 2*a, linewidth=2, edgecolor='white', facecolor='none')\n",
    "        rect1 = patches.Rectangle((x - a, y - a), 2*a, 2*a, linewidth=2, facecolor='white', alpha = 0.25)\n",
    "\n",
    "        ax.add_patch(rect2)\n",
    "        ax.add_patch(rect1)\n",
    "\n",
    "        # Add the condition to the legend\n",
    "        legend_elements.append(patches.Patch(facecolor='none', edgecolor='r', ))\n",
    "\n",
    "    # Add title\n",
    "    title = f\"Study: {study_id}, Series: {series_id}, Instance: {instance_number}\"\n",
    "    ax.set_title(title, fontsize=20)\n",
    "\n",
    "    # Display additional columns:\n",
    "    for _, row in filtered_instances.iterrows():\n",
    "        text = f\"level {row['level']}, {row['condition']}\"\n",
    "        ax.text(row['x'] + lag, row['y']+np.random.randint(-15, 15), text, fontsize=10, color='white', verticalalignment='center_baseline')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "filtered_instances = coords_with_desc[\n",
    "    (coords_with_desc['study_id'] == 4003253) &\n",
    "    (coords_with_desc['series_id'] == 702807833) &\n",
    "    (coords_with_desc['instance_number'] == 8)\n",
    "]\n",
    "\n",
    "show_coor(images_dir_path, filtered_instances, 4003253, 702807833, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_instances = coords_with_desc[\n",
    "    (coords_with_desc['study_id'] == 4003253) &\n",
    "    (coords_with_desc['series_id'] == 1054713880) &\n",
    "    (coords_with_desc['instance_number'] == 11)\n",
    "]\n",
    "\n",
    "show_coor(images_dir_path, filtered_instances, 4003253, 1054713880, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_instances = coords_with_desc[\n",
    "    (coords_with_desc['study_id'] == 4003253) &\n",
    "    (coords_with_desc['series_id'] == 2448190387) &\n",
    "    (coords_with_desc['instance_number'] == 11)\n",
    "]\n",
    "show_coor(images_dir_path, filtered_instances, 4003253, 2448190387, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Big conclusion** I gained from this exploration of the coordinates on single images and all conditions is:\n",
    "\n",
    "- **Images for Spinal Canal Stenosis** are maybe all in the sagittal plane.\n",
    "\n",
    "- **Images for Subarticular Stenosis** can be best viewed in the axial plane, but without the **correct** image in the sagittal plane, we can't understand to which **disk-level** the axial plane image refers.\n",
    "\n",
    "- **Images for Neural Foraminal Narrowing** appear to be in the sagittal plane, which is consistent with best practices for visualizing this condition.\n",
    "\n",
    "- There seems to be consistency in coordinate placement across different spine levels for the same condition, which could be useful for developing automated detection algorithms.\n",
    "\n",
    "- The coordinates for different conditions (e.g., Spinal Canal Stenosis vs. Neural Foraminal Narrowing) are placed at different locations within the image, reflecting the distinct anatomical areas affected by each condition.\n",
    "\n",
    "- Some images show multiple coordinates for the same condition at different spine levels, indicating that a single image can be used to assess multiple levels simultaneously.\n",
    "\n",
    "- The variation in y-coordinates across different spine levels (e.g., L1/L2 vs. L5/S1) suggests that the model or annotation process takes into account the natural curvature of the spine.\n",
    "\n",
    "- The above image shows similarity in the left and right coordinates for the same condition and spine level. We can use this information if there are missing coordinates on one of the sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the outlier values (minimum and maximum) for the `x` and `y` coordinates to see how they are visualized as annotations on the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_instance = coords_with_desc[coords_with_desc['y'] == coords_with_desc['y'].min()]\n",
    "show_coor(images_dir_path, outlier_instance, outlier_instance['study_id'].iloc[0], outlier_instance['series_id'].iloc[0], outlier_instance['instance_number'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_instance = coords_with_desc[coords_with_desc['y'] == coords_with_desc['y'].max()]\n",
    "show_coor(images_dir_path, outlier_instance, outlier_instance['study_id'].iloc[0], outlier_instance['series_id'].iloc[0], outlier_instance['instance_number'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_instance = coords_with_desc[coords_with_desc['x'] == coords_with_desc['x'].min()]\n",
    "show_coor(images_dir_path, outlier_instance, outlier_instance['study_id'].iloc[0], outlier_instance['series_id'].iloc[0], outlier_instance['instance_number'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_instance = coords_with_desc[coords_with_desc['x'] == coords_with_desc['x'].max()]\n",
    "show_coor(images_dir_path, outlier_instance, outlier_instance['study_id'].iloc[0], outlier_instance['series_id'].iloc[0], outlier_instance['instance_number'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the visualizations of the minimum and maximum y-coordinate instances, we discoverd that in each `study_id` each `series_id` can have diffrent size for the pixels (like we see above). We will keep it in mind when working with diffrent `series_id` isnide the `study_id`.\n",
    "\n",
    "Now that we've explored the coordinates on the MRI images, we will check our hypothesis\n",
    "that **all images for Spinal Canal Stenosis are in the sagittal plane**. We will do this\n",
    "using `coords_with_desc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_and_plot_conditions(df):\n",
    "    conditions = ['Spinal Canal Stenosis', 'Subarticular Stenosis', 'Neural Foraminal Narrowing']\n",
    "    view_types = ['Sagittal', 'Axial']\n",
    "\n",
    "    # Initialize a dictionary to store the counts\n",
    "    counts = {cond: {view: 0 for view in view_types} for cond in conditions}\n",
    "\n",
    "    # Count occurrences\n",
    "    for _, row in df.iterrows():\n",
    "        condition = row['condition']\n",
    "        series_desc = row['series_desc']\n",
    "\n",
    "        # Normalize condition names\n",
    "        if 'Neural Foraminal Narrowing' in condition:\n",
    "            condition = 'Neural Foraminal Narrowing'\n",
    "        elif 'Subarticular Stenosis' in condition:\n",
    "            condition = 'Subarticular Stenosis'\n",
    "        else:\n",
    "            condition = 'Spinal Canal Stenosis'\n",
    "\n",
    "        if condition in conditions:\n",
    "            view = 'Sagittal' if 'Sagittal' in series_desc else 'Axial'\n",
    "            counts[condition][view] += 1\n",
    "\n",
    "    # Convert counts to DataFrame for easier plotting\n",
    "    plot_data = pd.DataFrame(counts).T\n",
    "\n",
    "    # Create the bar plot\n",
    "    ax = plot_data.plot(kind='bar', figsize=(12, 6), width=0.8)\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.title('Condition Counts by View Type', fontsize=16)\n",
    "    plt.xlabel('Condition', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    plt.legend(title='View Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Add value labels on the bars\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, padding=3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Usage\n",
    "counts = count_and_plot_conditions(coords_with_desc)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can confirm that our hypothesis is correct. The conclusions drawn from our analysis are:\n",
    "\n",
    "-  **Spinal Canal Stenosis**: All images for this condition are indeed in the sagittal plane, which aligns with our initial hypothesis. However, this is not ideal as axial images are typically preferred for this condition. We will need to address this when building our model.\n",
    "\n",
    "- **Subarticular Stenosis**: This condition is exclusively imaged in the axial plane, which is consistent with best practices.\n",
    "\n",
    "- **Neural Foraminal Narrowing**: This condition is exclusively imaged in the sagittal plane, which is consistent with best practices.\n",
    "\n",
    "These findings provide valuable insights into the imaging practices for different spinal conditions. They largely confirm our initial hypotheses and align with established best practices in radiological imaging for spine pathologies. However, the exclusive use of sagittal images for Spinal Canal Stenosis warrants further investigation to understand if this represents a limitation in the dataset or if there are specific cases where sagittal imaging is preferred for this condition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we remeber from the EDA of the `train_label_coordinates.csv` that no a lot of `study_id` has missing coordiantion so will try to find wich are missing and see if we can fill the coordiation if for example it's the left and right side in Axial and we have one sice coordiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [col for col in train_df.columns if col != 'study_id']\n",
    "labels = set(labels)\n",
    "\n",
    "def find_missing_coords(df, label_cols, meta_df):\n",
    "    visited = set()\n",
    "    missing_coords = {study_id: set() for study_id in meta_df.keys()}\n",
    "\n",
    "    for study_id, study_df in tqdm(df.groupby('study_id'), desc=\"Processing studies\"):\n",
    "        if study_id in visited:\n",
    "            continue\n",
    "\n",
    "        visited.add(study_id)\n",
    "\n",
    "        labels = set()\n",
    "        for _, row in study_df.iterrows():\n",
    "            condition = row['condition'].replace(' ', '_').lower()\n",
    "            level = row['level'].replace('/', '_').lower()\n",
    "            label = f\"{condition}_{level}\"\n",
    "            labels.add(label)\n",
    "\n",
    "        missing = label_cols - labels\n",
    "        missing_coords[study_id] = missing\n",
    "\n",
    "    return missing_coords\n",
    "\n",
    "# Usaage\n",
    "missing_coords = find_missing_coords(label_coords_df, labels, meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_labels(missing_coords):\n",
    "    # Create a DataFrame with all possible labels\n",
    "    all_labels = sorted(set().union(*missing_coords.values()))\n",
    "    df = pd.DataFrame(index=missing_coords.keys(), columns=all_labels)\n",
    "\n",
    "    # Fill the DataFrame\n",
    "    for study_id, labels in missing_coords.items():\n",
    "        df.loc[study_id, list(labels)] = True  # Convert set to list here\n",
    "\n",
    "    # Fill NaN with False (present) and True with True (missing)\n",
    "    df = df.fillna(False)\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.heatmap(df, cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Labels Across Study IDs')\n",
    "    plt.xlabel('Labels')\n",
    "    plt.ylabel('Study IDs')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"Total number of study IDs with missing labels: {len(missing_coords)}\")\n",
    "    print(\"\\nMost common missing labels:\")\n",
    "    label_counts = df.sum().sort_values(ascending=False)\n",
    "    for label, count in label_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"{label}: {count}\")\n",
    "\n",
    "# Usage\n",
    "plot_missing_labels(missing_coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Findings from the Heatmap and Missing Labels Analysis**\n",
    "\n",
    "1. **Missing Labels Distribution**: Most study IDs that has missing labels have approximately 3 missing labels.\n",
    "\n",
    "2. **Common Missing Labels**: Predominantly related to Subarticular Stenosis and Spinal Canal Stenosis, particularly at the L1/L2 and L2/L3 levels.\n",
    "\n",
    "3. **Subarticular Stenosis Pattern**:\n",
    "   - Both right and left sides of L1/L2 and L2/L3 are frequently missing.\n",
    "   - All Subarticular Stenosis images are in the axial plane.\n",
    "   - Potential to infer or estimate missing coordinates based on patterns from other disk levels (e.g., L3/L4, L4/L5, L5/S1).\n",
    "\n",
    "4. **Spinal Canal Stenosis Pattern**:\n",
    "   - Significant missing coordinates, especially at L1/L2 (70 missing) and L2/L3 (40 missing) levels.\n",
    "   - All Spinal Canal Stenosis images are in the Sagittal T2/STIR plane.\n",
    "   - Consistency in imaging plane may allow for interpolation or estimation of missing coordinates.\n",
    "\n",
    "5. **Systematic Issue**: The pattern suggests a potential systematic issue in data collection or annotation, particularly for upper lumbar levels (L1/L2 and L2/L3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion for Images analysis:**  üí°ü©ª\n",
    "\n",
    "1. **Image Types and Consistency:**\n",
    "    - Each `study_id` contains three types of MRI sequences: Sagittal T2/STIR, Sagittal T1, and Axial T2.\n",
    "    - The number of images per `series_id` can vary, indicating potential inconsistencies in image acquisition or selection.\n",
    "\n",
    "2. **Image Dimensions:**\n",
    "    - Images within the same `study_id` can have different sizes, which may require standardization during preprocessing.\n",
    "    - Some images have unusually large dimensions, potentially due to high-resolution scans or different acquisition protocols.\n",
    "\n",
    "3. **Coordinate Annotations:**\n",
    "    - Coordinates are available for different severity labels (Normal, Moderate, Severe) across various spinal conditions.\n",
    "    - Not all images in `train_images` have coordinates.\n",
    "    - Some images lack coordinate annotations, but it's possible to fill them using available coordinates for the same `instance_number` or `series_id`.\n",
    "\n",
    "4. **Visualization Insights:**\n",
    "    - 3D-like visualizations (Animations) of image slices provided a comprehensive view of spinal anatomy across different MRI sequences.\n",
    "    - Displaying images with overlaid coordinates helped in understanding the annotation process and potential challenges in identifying specific spinal conditions.\n",
    "\n",
    "5. **Condition-Specific Imaging:**\n",
    "    - Spinal Canal Stenosis: Exclusively imaged in the sagittal plane (T2/STIR), which may not be ideal.\n",
    "    - Subarticular Stenosis: Consistently imaged in the axial plane, aligning with best practices.\n",
    "    - Neural Foraminal Narrowing: Imaged in the sagittal plane, consistent with standard practices.\n",
    "\n",
    "6. **Data Quality and Consistency:**\n",
    "    - The presence of outliers in coordinate data suggests the need for careful data cleaning.\n",
    "    - Systematic missing data, particularly for upper lumbar levels (L1/L2 and L2/L3), indicates a potential bias in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° Conclusions from the Exploratory Data Analysis üí°\n",
    "\n",
    "1. **Data Structure and Composition**:\n",
    "   - Dataset consists of MRI images in DICOM format and corresponding CSV files with labels and coordinates.\n",
    "   - Each `study_id` contains three types of MRI sequences: Sagittal T2/STIR, Sagittal T1, and Axial T2.\n",
    "   - Number of images per `series_id` varies, indicating potential inconsistencies in image acquisition or selection.\n",
    "\n",
    "2. **Image Characteristics**:\n",
    "   - Images within the same `study_id` can have different sizes, requiring standardization during preprocessing.\n",
    "   - Some images have unusually large dimensions, potentially due to high-resolution scans or different acquisition protocols.\n",
    "\n",
    "3. **Coordinate Annotations**:\n",
    "   - Coordinates available for different severity labels (Normal, Moderate, Severe) across various spinal conditions.\n",
    "   - Not all images in `train_images` have coordinates.\n",
    "   - Some images lack coordinate annotations, but can potentially be filled using available coordinates for the same `instance_number` or `series_id`.\n",
    "\n",
    "4. **Condition-Specific Imaging**:\n",
    "   - Spinal Canal Stenosis: Exclusively imaged in sagittal plane (T2/STIR), which may not be ideal.\n",
    "   - Subarticular Stenosis: Consistently imaged in axial plane, aligning with best practices.\n",
    "   - Neural Foraminal Narrowing: Imaged in sagittal plane, consistent with standard practices.\n",
    "\n",
    "5. **Missing Data and Outliers**:\n",
    "   - Presence of outliers in coordinate data suggests need for careful data cleaning.\n",
    "   - Systematic missing data, particularly for upper lumbar levels (L1/L2 and L2/L3), indicates potential bias in dataset.\n",
    "   - Approximately 185 rows in `train_df` have missing values (NaN).\n",
    "\n",
    "6. **Data Quality and Consistency**:\n",
    "   - Dataset shows inconsistencies in image acquisition and annotation, needing address during preprocessing.\n",
    "   - Need for standardization of image sizes and handling of missing coordinates.\n",
    "\n",
    "7. **Class Imbalance**:\n",
    "   - Imbalance between Normal/Mild, Moderate, and Severe cases, needs consideration in model development.\n",
    "\n",
    "These findings provide crucial insights for our next steps. We will explore different approaches to fill the missing coordinates by preprocessing the data and choosing a model to train. This model will fill in the coordinates for all images inside the `train_label_coordinates` dataset. Afterward, we will be able to train efficiently by passing all available images and detect the ROI (Region of Interest).\n",
    "\n",
    "Let's explore! üîçüïµÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding coordinates using YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Stop here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle find all coordinates for each spine level and right, left side in axial plane, I will use the YOLOv8 model. \n",
    "\n",
    "For each plane (Sagittal T1, Sagittal T2/STIR, Axial T2) we will build a dataset for each plane and train a model to recognize the levels of the spine, and for the axial plane we will use the yolo model to detect the left and right side of the of the disk.\n",
    "\n",
    "The dataset will have the following structure:\n",
    "\n",
    "    ```\n",
    "    dataset/\n",
    "    ‚îú‚îÄ‚îÄ images/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patient1_scan1.png    # Original MRI image, NO boxes drawn\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patient1_scan2.png    # Original MRI image, NO boxes drawn\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ patient2_scan1.png    # Original MRI image, NO boxes drawn\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ patient2_scan2.png    # Original MRI image, NO boxes drawn\n",
    "    ‚îú‚îÄ‚îÄ labels/\n",
    "    ‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patient1_scan1.txt    # Contains: \"0 0.5 0.5 0.1 0.1\"\n",
    "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ patient1_scan2.txt    # Contains coordinates\n",
    "    ‚îÇ   ‚îî‚îÄ‚îÄ val/\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ patient2_scan1.txt    # Contains coordinates\n",
    "    ‚îÇ       ‚îú‚îÄ‚îÄ patient2_scan2.txt    # Contains coordinates\n",
    "    ```\n",
    "\n",
    "The content of each label file (e.g., patient1_scan1.txt) will follow this format:\n",
    "0 0.5 0.5 0.1 0.1   # Format: <class> <x_center> <y_center> <width> <height>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to build, we need all images to be the same size. We will do this by converting all the images to PNG format. We need to consider normalization and resizing all the images so they have the same size and pixel value range. My images are usually 640x640 or 320x320. One more thing to consider is that the coordinates for each image are based on the original image size, so if we change it, we will need to address the issue of adjusting the coordinate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagt1_df = coords_with_desc[coords_with_desc['series_desc'] == 'Sagittal T1'].copy()\n",
    "sagt2_df = coords_with_desc[coords_with_desc['series_desc'] == 'Sagittal T2/STIR'].copy()\n",
    "axialt2_df = coords_with_desc[coords_with_desc['series_desc'] == 'Axial T2'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagittal T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SagT1_YOLO:\n",
    "    def __init__(self, df, images_dir, output_dir, img_size=384):\n",
    "        \"\"\"\n",
    "        Initialize Sagittal T1 YOLO detector\n",
    "        Args:\n",
    "            df: DataFrame with annotations\n",
    "            images_dir: Path to DICOM images\n",
    "            output_dir: Path to save processed dataset\n",
    "            img_size: Target image size for YOLO\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Create initial directory structure\n",
    "        self.create_dataset_structure()\n",
    "        \n",
    "        # Process dataset\n",
    "        self.instance_coords_df = self.process_instance_coordinates()\n",
    "        self.processed_df = self.process_spine_dataset()\n",
    "        \n",
    "        # Create YAML and train model\n",
    "        self.yaml_path = self.create_dataset_yaml()\n",
    "        self.model, self.results = self.train_yolo()\n",
    "\n",
    "    def create_dataset_structure(self):\n",
    "        \"\"\"Create YOLO dataset directory structure\"\"\"\n",
    "        for split in ['train', 'val']:\n",
    "            for subdir in ['images', 'labels']:\n",
    "                path = os.path.join(self.output_dir, split, subdir)\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    def process_instance_coordinates(self):\n",
    "        \"\"\"\n",
    "        Process coordinates for Sagittal T1 images with coordinate sharing across instances\n",
    "        Returns DataFrame with coordinates for all instances, sharing information across the series\n",
    "        \"\"\"\n",
    "        result_records = []\n",
    "\n",
    "        # Group by series to process related instances\n",
    "        for (study_id, series_id), series_data in self.df.groupby(['study_id', 'series_id']):\n",
    "            # First, collect all coordinates for each level in the series\n",
    "            series_level_coords = {}\n",
    "            for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']:\n",
    "                level_key = level.lower().replace('/', '_')\n",
    "                level_data = series_data[series_data['level'] == level]\n",
    "\n",
    "                if not level_data.empty:\n",
    "                    right_coords = []\n",
    "                    left_coords = []\n",
    "                    instances_with_data = set()  # Track which instances have data for this level\n",
    "\n",
    "                    for _, row in level_data.iterrows():\n",
    "                        instances_with_data.add(row['instance_number'])\n",
    "                        if 'Right' in row['condition']:\n",
    "                            right_coords.append((row['x'], row['y']))\n",
    "                        elif 'Left' in row['condition']:\n",
    "                            left_coords.append((row['x'], row['y']))\n",
    "\n",
    "                    # Calculate coordinates for this level\n",
    "                    level_coords = {\n",
    "                        'instances': instances_with_data,\n",
    "                        'coords': {\n",
    "                            'right': (np.mean([x for x, _ in right_coords]) if right_coords else None,\n",
    "                                    np.mean([y for _, y in right_coords]) if right_coords else None),\n",
    "                            'left': (np.mean([x for x, _ in left_coords]) if left_coords else None,\n",
    "                                   np.mean([y for _, y in left_coords]) if left_coords else None)\n",
    "                        }\n",
    "                    }\n",
    "\n",
    "                    # Calculate center coordinates if possible\n",
    "                    if right_coords and left_coords:\n",
    "                        level_coords['coords']['center'] = (\n",
    "                            (level_coords['coords']['right'][0] + level_coords['coords']['left'][0]) / 2,\n",
    "                            (level_coords['coords']['right'][1] + level_coords['coords']['left'][1]) / 2\n",
    "                        )\n",
    "                    elif right_coords:\n",
    "                        level_coords['coords']['center'] = level_coords['coords']['right']\n",
    "                    elif left_coords:\n",
    "                        level_coords['coords']['center'] = level_coords['coords']['left']\n",
    "                    else:\n",
    "                        level_coords['coords']['center'] = (None, None)\n",
    "\n",
    "                    series_level_coords[level_key] = level_coords\n",
    "\n",
    "            # Get all unique instance numbers in the series\n",
    "            all_instances = series_data['instance_number'].unique()\n",
    "\n",
    "            # Create records for each instance, sharing coordinates across the series\n",
    "            for instance_number in all_instances:\n",
    "                record = {\n",
    "                    'study_id': study_id,\n",
    "                    'series_id': series_id,\n",
    "                    'instance_number': instance_number\n",
    "                }\n",
    "\n",
    "                # Add source tracking\n",
    "                record['coordinate_sources'] = {}\n",
    "\n",
    "                # Add coordinates for all levels to this instance\n",
    "                for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']:\n",
    "                    level_key = level.lower().replace('/', '_')\n",
    "                    if level_key in series_level_coords:\n",
    "                        level_data = series_level_coords[level_key]\n",
    "\n",
    "                        # Record which instances provided data for this level\n",
    "                        record['coordinate_sources'][level_key] = list(level_data['instances'])\n",
    "\n",
    "                        # Add right coordinates\n",
    "                        if level_data['coords']['right'][0] is not None:\n",
    "                            record[f'{level_key}_right_x'] = level_data['coords']['right'][0]\n",
    "                            record[f'{level_key}_right_y'] = level_data['coords']['right'][1]\n",
    "\n",
    "                        # Add left coordinates\n",
    "                        if level_data['coords']['left'][0] is not None:\n",
    "                            record[f'{level_key}_left_x'] = level_data['coords']['left'][0]\n",
    "                            record[f'{level_key}_left_y'] = level_data['coords']['left'][1]\n",
    "\n",
    "                        # Add center coordinates\n",
    "                        if level_data['coords']['center'][0] is not None:\n",
    "                            record[f'{level_key}_center_x'] = level_data['coords']['center'][0]\n",
    "                            record[f'{level_key}_center_y'] = level_data['coords']['center'][1]\n",
    "\n",
    "                result_records.append(record)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        result_df = pd.DataFrame(result_records)\n",
    "\n",
    "        # Add metadata about coordinate availability\n",
    "        result_df['available_levels'] = result_df.apply(\n",
    "            lambda row: [\n",
    "                level for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "                if not pd.isna(row.get(f\"{level.lower().replace('/', '_')}_center_x\"))\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        result_df['total_levels'] = result_df['available_levels'].apply(len)\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total series processed: {len(result_df['series_id'].unique())}\")\n",
    "        print(f\"Total instances processed: {len(result_df)}\")\n",
    "        print(\"\\nLevel availability:\")\n",
    "        for level in ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']:\n",
    "            count = result_df[f'{level}_center_x'].notna().sum()\n",
    "            print(f\"{level}: {count} instances ({count/len(result_df)*100:.1f}%)\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def create_yolo_annotation(self, row, image_width, image_height):\n",
    "        \"\"\"Create YOLO format annotations for Sagittal T1 images\"\"\"\n",
    "        annotations = []\n",
    "        box_width = 0.05\n",
    "        box_height = 0.05\n",
    "        \n",
    "        level_map = {'l1_l2': 0, 'l2_l3': 1, 'l3_l4': 2, 'l4_l5': 3, 'l5_s1': 4}\n",
    "        \n",
    "        for level, idx in level_map.items():\n",
    "            x_coord = row.get(f'{level}_center_x')\n",
    "            y_coord = row.get(f'{level}_center_y')\n",
    "            \n",
    "            if pd.notna(x_coord) and pd.notna(y_coord):\n",
    "                x_norm = x_coord / image_width\n",
    "                y_norm = y_coord / image_height\n",
    "                annotations.append(f\"{idx} {x_norm:.6f} {y_norm:.6f} {box_width:.6f} {box_height:.6f}\")\n",
    "        \n",
    "        return annotations\n",
    "\n",
    "    def process_spine_dataset(self):\n",
    "        \"\"\"Process and save dataset in YOLO format\"\"\"\n",
    "        # Split studies\n",
    "        studies = self.instance_coords_df['study_id'].unique()\n",
    "        train_studies, val_studies = train_test_split(studies, train_size=0.8, random_state=42)\n",
    "        \n",
    "        processed_counts = {'train': 0, 'val': 0}\n",
    "        failed_cases = []\n",
    "        \n",
    "        for _, row in tqdm(self.instance_coords_df.iterrows(), desc=\"Processing Sagittal T1 images\"):\n",
    "            try:\n",
    "                # Convert IDs to integers for path construction\n",
    "                study_id = str(int(row['study_id']))\n",
    "                series_id = str(int(row['series_id']))\n",
    "                instance_number = str(int(row['instance_number']))\n",
    "                \n",
    "                # Construct image path\n",
    "                img_path = os.path.join(self.images_dir, study_id, series_id, instance_number)\n",
    "                \n",
    "                # Try with and without .dcm extension\n",
    "                if os.path.exists(img_path + '.dcm'):\n",
    "                    img_path = img_path + '.dcm'\n",
    "                elif not os.path.exists(img_path):\n",
    "                    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "                \n",
    "                # Read and process image\n",
    "                ds = pydicom.dcmread(img_path)\n",
    "                image = ds.pixel_array\n",
    "                h, w = image.shape\n",
    "                \n",
    "                # Create annotations\n",
    "                annotations = self.create_yolo_annotation(row, w, h)\n",
    "                if not annotations:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare image\n",
    "                image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                image_resized = cv2.resize(image_normalized, (self.img_size, self.img_size))\n",
    "                \n",
    "                # Save files\n",
    "                is_train = row['study_id'] in train_studies\n",
    "                split = 'train' if is_train else 'val'\n",
    "                \n",
    "                img_filename = f\"{study_id}_{series_id}_{instance_number}.png\"\n",
    "                label_filename = f\"{study_id}_{series_id}_{instance_number}.txt\"\n",
    "                \n",
    "                cv2.imwrite(os.path.join(self.output_dir, split, 'images', img_filename), \n",
    "                           image_resized)\n",
    "                with open(os.path.join(self.output_dir, split, 'labels', label_filename), 'w') as f:\n",
    "                    f.write('\\n'.join(annotations))\n",
    "                \n",
    "                # Add split information to row\n",
    "                row['split'] = split\n",
    "                processed_counts[split] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_cases.append((study_id, series_id, instance_number, str(e)))\n",
    "        \n",
    "        print(f\"\\nProcessing Summary:\")\n",
    "        print(f\"Training images: {processed_counts['train']}\")\n",
    "        print(f\"Validation images: {processed_counts['val']}\")\n",
    "        \n",
    "        if failed_cases:\n",
    "            print(\"\\nFailed cases:\")\n",
    "            for case in failed_cases:\n",
    "                print(f\"Study {case[0]}, Series {case[1]}, Instance {case[2]}: {case[3]}\")\n",
    "        \n",
    "        return self.instance_coords_df\n",
    "\n",
    "    def create_dataset_yaml(self):\n",
    "        \"\"\"Create YOLO dataset configuration file\"\"\"\n",
    "        yaml_content = {\n",
    "            'path': os.path.abspath(self.output_dir),\n",
    "            'train': 'train/images',\n",
    "            'val': 'val/images',\n",
    "            'nc': 5,  # number of classes\n",
    "            'names': {\n",
    "                0: 'L1/L2',\n",
    "                1: 'L2/L3',\n",
    "                2: 'L3/L4',\n",
    "                3: 'L4/L5',\n",
    "                4: 'L5/S1'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        yaml_path = os.path.join(self.output_dir, 'dataset.yaml')\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "        return yaml_path\n",
    "\n",
    "    def train_yolo(self):\n",
    "        \"\"\"Train YOLO model for Sagittal T1 images\"\"\"\n",
    "        try:\n",
    "            model = YOLO('yolov8x.pt')\n",
    "            \n",
    "            config = {\n",
    "                'data': self.yaml_path,\n",
    "                'imgsz': self.img_size,\n",
    "                'batch': 8,\n",
    "                'epochs': 2, \n",
    "                'patience': 5,\n",
    "                'device': 'mps',\n",
    "                'workers': 4,\n",
    "                'project': 'spine_detection',\n",
    "                'name': 'sagittal_t1_yolo',\n",
    "                'exist_ok': True,\n",
    "                'pretrained': True,\n",
    "                'optimizer': 'AdamW',\n",
    "                'verbose': True,\n",
    "                'seed': 42,\n",
    "                'deterministic': True,\n",
    "                'dropout': 0.1,\n",
    "                'lr0': 0.001,\n",
    "                'lrf': 0.01,\n",
    "                'momentum': 0.937,\n",
    "                'weight_decay': 0.0005,\n",
    "                'warmup_epochs': 5,\n",
    "                'warmup_momentum': 0.8,\n",
    "                'box': 7.5,\n",
    "                'cls': 0.5,\n",
    "                'dfl': 1.5,\n",
    "                'close_mosaic': 10,\n",
    "                'amp': True,  \n",
    "                'rect': True,  \n",
    "                'multi_scale': True,  \n",
    "                'val': True, \n",
    "            }\n",
    "            \n",
    "            results = model.train(**config)\n",
    "            return model, results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def get_training_stats(self):\n",
    "        \"\"\"Get statistics about the processed dataset\"\"\"\n",
    "        stats = {\n",
    "            'total_series': len(self.instance_coords_df['series_id'].unique()),\n",
    "            'total_instances': len(self.instance_coords_df),\n",
    "            'train_images': len(self.processed_df[self.processed_df['split'] == 'train']),\n",
    "            'val_images': len(self.processed_df[self.processed_df['split'] == 'val']),\n",
    "            'level_coverage': {}\n",
    "        }\n",
    "        \n",
    "        for level in ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']:\n",
    "            count = self.instance_coords_df[f'{level}_center_x'].notna().sum()\n",
    "            stats['level_coverage'][level] = {\n",
    "                'count': count,\n",
    "                'percentage': count/len(self.instance_coords_df)*100\n",
    "            }\n",
    "            \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagittal T1\n",
    "images_dir  = '/Users/danipopov/Projects/RSNA2024/data/train_images'\n",
    "output_dir = '/Users/danipopov/Projects/RSNA2024/data/spine_dataset_sagt1'\n",
    "\n",
    "sag_t1_model = SagT1_YOLO(\n",
    "    df=sagt1_df,\n",
    "    images_dir=images_dir,\n",
    "    output_dir=output_dir,\n",
    "    img_size=384\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In the notebook we train the model for 1 or 2 epochs, beacuse it takes a long time to train on my personal computer.\n",
    "The acctual model was trained for 100 epochs in the Kaggle notebook and all the other yolo models as well, the function to save the model is below was used in the kaggle notebook to save the weights of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model(model, best_model_path, model_type, save_path='/kaggle/working/'):\n",
    "    \"\"\"\n",
    "    Save the trained YOLO model with simple fixed naming\n",
    "    \n",
    "    Args:\n",
    "        model: YOLO model object\n",
    "        best_model_path: Path to the best model weights\n",
    "        model_type: String indicating model type ('sag_t1', 'sag_t2', or 'axial_t2')\n",
    "        save_path: Base path to save the model\n",
    "    \"\"\"\n",
    "    # Define simple model names\n",
    "    model_names = {\n",
    "        'sag_t1': 'sagittal_t1_spine_detector.pt',\n",
    "        'sag_t2': 'sagittal_t2_spine_detector.pt',\n",
    "        'axial_t2': 'axial_t2_spine_detector.pt'\n",
    "    }\n",
    "    \n",
    "    if model_type not in model_names:\n",
    "        raise ValueError(f\"Invalid model_type: {model_type}. Must be one of {list(model_names.keys())}\")\n",
    "    \n",
    "    try:\n",
    "        if not os.path.exists(best_model_path):\n",
    "            print(f\"Best model weights not found at {best_model_path}\")\n",
    "            return\n",
    "            \n",
    "        final_save_path = os.path.join(save_path, model_names[model_type])\n",
    "        \n",
    "        # Copy the model file\n",
    "        shutil.copy(best_model_path, final_save_path)\n",
    "        print(f\"Model saved to {final_save_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "# Save models with appropriate type\n",
    "save_trained_model(\n",
    "    sag_t1_model.model, \n",
    "    '/kaggle/working/spine_detection/sagittal_t1_yolo/weights/best.pt',\n",
    "    model_type='sag_t1'\n",
    ")\n",
    "\n",
    "#save_trained_model(\n",
    "#    sag_t2_model.model, \n",
    "#    '/kaggle/working/spine_detection/sagittal_t2_yolo/weights/best.pt',\n",
    "#    model_type='sag_t2'\n",
    "#)\n",
    "\n",
    "#save_trained_model(\n",
    "#    axial_t2_model.model, \n",
    "#    '/kaggle/working/spine_detection/axial_t2_yolo/weights/best.pt',\n",
    "#    model_type='axial_t2'\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we remove the files that we don't need anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the folders execpt the weights\n",
    "shutil.rmtree(\"/Users/danipopov/Projects/RSNA2024/spine_detection/sagittal_t1_yolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will test the model on a few images and see how it preforms and can be used to detect the spine levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spine_predictions(image_path, model_path, \n",
    "                         conf_threshold=0.25, iou_threshold=0.45, img_size=384):\n",
    "    \"\"\"\n",
    "    Plot YOLO predictions for spine levels on a DICOM image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to DICOM image\n",
    "        model_path: Path to saved YOLO model\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "        img_size: Image size for model input\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read DICOM\n",
    "    ds = pydicom.dcmread(image_path)\n",
    "    image = ds.pixel_array\n",
    "    \n",
    "    # Normalize and resize\n",
    "    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    image_resized = cv2.resize(image_normalized, (img_size, img_size))\n",
    "    \n",
    "    # Convert grayscale to RGB\n",
    "    image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot image with predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Predictions')\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict(\n",
    "        source=image_rgb,\n",
    "        conf=conf_threshold,\n",
    "        iou=iou_threshold\n",
    "    )\n",
    "    \n",
    "    # Define colors for each level\n",
    "    colors = ['red', 'green', 'blue', 'yellow', 'purple']\n",
    "    level_names = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "    \n",
    "    if results[0].boxes is not None:\n",
    "        boxes = results[0].boxes.cpu().numpy()\n",
    "        \n",
    "        # Sort boxes by y-coordinate to display levels in order\n",
    "        box_data = []\n",
    "        for box in boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = box.conf[0]\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            box_data.append((y1, cls_id, conf, x1, y1, x2, y2))\n",
    "        \n",
    "        box_data.sort()  # Sort by y1 coordinate\n",
    "        \n",
    "        # Plot each detection\n",
    "        for i, (_, cls_id, conf, x1, y1, x2, y2) in enumerate(box_data):\n",
    "            color = colors[cls_id]\n",
    "            level_name = level_names[cls_id]\n",
    "            \n",
    "            # Draw bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                fill=False, color=color, linewidth=2\n",
    "            ))\n",
    "            \n",
    "            # Add label\n",
    "            plt.text(\n",
    "                x2 + 5, (y1 + y2) / 2, \n",
    "                f'{level_name}: {conf:.2f}',\n",
    "                color=color, fontsize=8, verticalalignment='center',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
    "            )\n",
    "            \n",
    "            # Print detection info\n",
    "            print(f\"Found {level_name} with confidence {conf:.2f}\")\n",
    "    else:\n",
    "        print(\"No detections found\")\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_images(image_paths, model_path, \n",
    "                          conf_threshold=0.25, iou_threshold=0.45):\n",
    "    \"\"\"\n",
    "    Process multiple images and display their predictions, skipping images without predictions\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of paths to DICOM images\n",
    "        model_path: Path to saved YOLO model\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "    \"\"\"\n",
    "    # First, filter images that have predictions\n",
    "    valid_images = []\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        ds = pydicom.dcmread(img_path)\n",
    "        image = ds.pixel_array\n",
    "        image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "        image_resized = cv2.resize(image_normalized, (384, 384))\n",
    "        image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "        \n",
    "        results = model.predict(\n",
    "            source=image_rgb,\n",
    "            conf=conf_threshold,\n",
    "            iou=iou_threshold\n",
    "        )\n",
    "        \n",
    "        if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "            valid_images.append(img_path)\n",
    "    \n",
    "    if not valid_images:\n",
    "        print(\"No images with valid predictions found\")\n",
    "        return\n",
    "    \n",
    "    # Process only images with predictions\n",
    "    for idx, img_path in enumerate(valid_images):\n",
    "        if idx == 5:\n",
    "            break\n",
    "        plot_spine_predictions(img_path, model_path, conf_threshold, iou_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example for Sagittal T1:\n",
    "image_paths = glob.glob('/Users/danipopov/Projects/RSNA2024/data/train_images/4646740/3486248476/*.dcm')\n",
    "process_multiple_images(image_paths, '/Users/danipopov/Projects/RSNA2024/models/sagittal_t1_spine_detector.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is able to detect the spine levels, but it is not perfect some images it detects more levels than it should and some it detects less and some it detects none.\n",
    "\n",
    "So we will need to use some postprocessing to make sure that we get the correct number of levels for each image or to fill in the missing levels or remove images that have some levels that are not visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagittal T2/STIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SagT2_YOLO:\n",
    "    def __init__(self, df, images_dir, output_dir, img_size=384):\n",
    "        \"\"\"\n",
    "        Initialize Sagittal T2/STIR YOLO detector\n",
    "        Args:\n",
    "            df: DataFrame with annotations\n",
    "            images_dir: Path to DICOM images\n",
    "            output_dir: Path to save processed dataset\n",
    "            img_size: Target image size for YOLO\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Create initial directory structure\n",
    "        self.create_dataset_structure()\n",
    "        \n",
    "        # Process dataset\n",
    "        self.instance_coords_df = self.process_instance_coordinates()\n",
    "        self.processed_df = self.process_spine_dataset()\n",
    "        \n",
    "        # Create YAML and train model\n",
    "        self.yaml_path = self.create_dataset_yaml()\n",
    "        self.model, self.results = self.train_yolo()\n",
    "\n",
    "    def create_dataset_structure(self):\n",
    "        \"\"\"Create YOLO dataset directory structure\"\"\"\n",
    "        for split in ['train', 'val']:\n",
    "            for subdir in ['images', 'labels']:\n",
    "                path = os.path.join(self.output_dir, split, subdir)\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    def process_instance_coordinates(self):\n",
    "        \"\"\"\n",
    "        Process coordinates for Sagittal T2 images with coordinate sharing across instances\n",
    "        Returns DataFrame with coordinates for all instances, sharing information across the series\n",
    "        \"\"\"\n",
    "        result_records = []\n",
    "\n",
    "        # Group by series to process related instances\n",
    "        for (study_id, series_id), series_data in self.df.groupby(['study_id', 'series_id']):\n",
    "            # First, collect all coordinates for each level in the series\n",
    "            series_level_coords = {}\n",
    "            for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']:\n",
    "                level_data = series_data[series_data['level'] == level]\n",
    "\n",
    "                if not level_data.empty:\n",
    "                    coords = []\n",
    "                    instances_with_data = set()  # Track which instances have data for this level\n",
    "\n",
    "                    for _, row in level_data.iterrows():\n",
    "                        instances_with_data.add(row['instance_number'])\n",
    "                        coords.append((row['x'], row['y']))\n",
    "\n",
    "                    # Calculate average coordinates for this level\n",
    "                    if coords:\n",
    "                        avg_x = np.mean([x for x, _ in coords])\n",
    "                        avg_y = np.mean([y for _, y in coords])\n",
    "                        \n",
    "                        series_level_coords[level] = {\n",
    "                            'instances': instances_with_data,\n",
    "                            'coords': (avg_x, avg_y),\n",
    "                            'original_coords': coords  # Keep original coordinates for reference\n",
    "                        }\n",
    "\n",
    "            # Get all unique instance numbers in the series\n",
    "            all_instances = series_data['instance_number'].unique()\n",
    "\n",
    "            # Create records for each instance\n",
    "            for instance_number in all_instances:\n",
    "                instance_data = series_data[series_data['instance_number'] == instance_number]\n",
    "                \n",
    "                record = {\n",
    "                    'study_id': study_id,\n",
    "                    'series_id': series_id,\n",
    "                    'instance_number': instance_number,\n",
    "                    'coordinate_sources': {}  # Track where coordinates came from\n",
    "                }\n",
    "\n",
    "                # Process each spinal level\n",
    "                for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']:\n",
    "                    level_key = level.replace('/', '_').lower()\n",
    "                    \n",
    "                    # Check if this instance has original coordinates for this level\n",
    "                    instance_level_data = instance_data[instance_data['level'] == level]\n",
    "                    \n",
    "                    if not instance_level_data.empty:\n",
    "                        # Use original coordinates for this instance\n",
    "                        x = instance_level_data.iloc[0]['x']\n",
    "                        y = instance_level_data.iloc[0]['y']\n",
    "                        record['coordinate_sources'][level_key] = 'original'\n",
    "                    elif level in series_level_coords:\n",
    "                        # Use shared coordinates from series\n",
    "                        x, y = series_level_coords[level]['coords']\n",
    "                        record['coordinate_sources'][level_key] = 'shared'\n",
    "                    else:\n",
    "                        # No coordinates available\n",
    "                        x = y = None\n",
    "                        record['coordinate_sources'][level_key] = 'missing'\n",
    "                    \n",
    "                    record[f'{level_key}_x'] = x\n",
    "                    record[f'{level_key}_y'] = y\n",
    "\n",
    "                result_records.append(record)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        result_df = pd.DataFrame(result_records)\n",
    "\n",
    "        # Add metadata about coordinate availability\n",
    "        result_df['available_levels'] = result_df.apply(\n",
    "            lambda row: [\n",
    "                level for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "                if not pd.isna(row[f\"{level.replace('/', '_').lower()}_x\"])\n",
    "            ],\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        result_df['total_levels'] = result_df['available_levels'].apply(len)\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total series processed: {len(result_df['series_id'].unique())}\")\n",
    "        print(f\"Total instances processed: {len(result_df)}\")\n",
    "        print(\"\\nLevel availability:\")\n",
    "        for level in ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']:\n",
    "            count = result_df[f'{level}_x'].notna().sum()\n",
    "            original_count = sum(result_df['coordinate_sources'].apply(\n",
    "                lambda x: x.get(level, '') == 'original'\n",
    "            ))\n",
    "            shared_count = sum(result_df['coordinate_sources'].apply(\n",
    "                lambda x: x.get(level, '') == 'shared'\n",
    "            ))\n",
    "            print(f\"{level}: {count} instances ({count/len(result_df)*100:.1f}%)\")\n",
    "            print(f\"  - Original: {original_count}\")\n",
    "            print(f\"  - Shared: {shared_count}\")\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def create_yolo_annotation(self, row, image_width, image_height):\n",
    "        \"\"\"Create YOLO format annotations for Sagittal T2 images\"\"\"\n",
    "        annotations = []\n",
    "        box_width = 0.05  # Relative box width\n",
    "        box_height = 0.05  # Relative box height\n",
    "        \n",
    "        level_map = {\n",
    "            'l1_l2': 0,\n",
    "            'l2_l3': 1,\n",
    "            'l3_l4': 2,\n",
    "            'l4_l5': 3,\n",
    "            'l5_s1': 4\n",
    "        }\n",
    "        \n",
    "        for level, idx in level_map.items():\n",
    "            x_coord = row.get(f'{level}_x')\n",
    "            y_coord = row.get(f'{level}_y')\n",
    "            \n",
    "            if pd.notna(x_coord) and pd.notna(y_coord):\n",
    "                x_norm = x_coord / image_width\n",
    "                y_norm = y_coord / image_height\n",
    "                annotations.append(f\"{idx} {x_norm:.6f} {y_norm:.6f} {box_width:.6f} {box_height:.6f}\")\n",
    "        \n",
    "        return annotations\n",
    "\n",
    "    def process_spine_dataset(self):\n",
    "        \"\"\"Process and save dataset in YOLO format\"\"\"\n",
    "        # Split studies\n",
    "        studies = self.instance_coords_df['study_id'].unique()\n",
    "        train_studies, val_studies = train_test_split(studies, train_size=0.8, random_state=42)\n",
    "        \n",
    "        processed_counts = {'train': 0, 'val': 0}\n",
    "        failed_cases = []\n",
    "        \n",
    "        for _, row in tqdm(self.instance_coords_df.iterrows(), desc=\"Processing Sagittal T1 images\"):\n",
    "            try:\n",
    "                # Convert IDs to integers for path construction\n",
    "                study_id = str(int(row['study_id']))\n",
    "                series_id = str(int(row['series_id']))\n",
    "                instance_number = str(int(row['instance_number']))\n",
    "                \n",
    "                # Construct image path\n",
    "                img_path = os.path.join(self.images_dir, study_id, series_id, instance_number)\n",
    "                \n",
    "                # Try with and without .dcm extension\n",
    "                if os.path.exists(img_path + '.dcm'):\n",
    "                    img_path = img_path + '.dcm'\n",
    "                elif not os.path.exists(img_path):\n",
    "                    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "                \n",
    "                # Read and process image\n",
    "                ds = pydicom.dcmread(img_path)\n",
    "                image = ds.pixel_array\n",
    "                h, w = image.shape\n",
    "                \n",
    "                # Create annotations\n",
    "                annotations = self.create_yolo_annotation(row, w, h)\n",
    "                if not annotations:\n",
    "                    continue\n",
    "                \n",
    "                # Prepare image\n",
    "                image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                image_resized = cv2.resize(image_normalized, (self.img_size, self.img_size))\n",
    "                \n",
    "                # Save files\n",
    "                is_train = row['study_id'] in train_studies\n",
    "                split = 'train' if is_train else 'val'\n",
    "                \n",
    "                img_filename = f\"{study_id}_{series_id}_{instance_number}.png\"\n",
    "                label_filename = f\"{study_id}_{series_id}_{instance_number}.txt\"\n",
    "                \n",
    "                cv2.imwrite(os.path.join(self.output_dir, split, 'images', img_filename), \n",
    "                           image_resized)\n",
    "                with open(os.path.join(self.output_dir, split, 'labels', label_filename), 'w') as f:\n",
    "                    f.write('\\n'.join(annotations))\n",
    "                \n",
    "                # Add split information to row\n",
    "                row['split'] = split\n",
    "                processed_counts[split] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_cases.append((study_id, series_id, instance_number, str(e)))\n",
    "        \n",
    "        print(f\"\\nProcessing Summary:\")\n",
    "        print(f\"Training images: {processed_counts['train']}\")\n",
    "        print(f\"Validation images: {processed_counts['val']}\")\n",
    "        \n",
    "        if failed_cases:\n",
    "            print(\"\\nFailed cases:\")\n",
    "            for case in failed_cases:\n",
    "                print(f\"Study {case[0]}, Series {case[1]}, Instance {case[2]}: {case[3]}\")\n",
    "        \n",
    "        return self.instance_coords_df\n",
    "\n",
    "    def create_dataset_yaml(self):\n",
    "        \"\"\"Create YOLO dataset configuration file\"\"\"\n",
    "        yaml_content = {\n",
    "            'path': os.path.abspath(self.output_dir),\n",
    "            'train': 'train/images',\n",
    "            'val': 'val/images',\n",
    "            'nc': 5,  # number of classes\n",
    "            'names': {\n",
    "                0: 'L1/L2',\n",
    "                1: 'L2/L3',\n",
    "                2: 'L3/L4',\n",
    "                3: 'L4/L5',\n",
    "                4: 'L5/S1'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        yaml_path = os.path.join(self.output_dir, 'dataset.yaml')\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "        return yaml_path\n",
    "\n",
    "    def train_yolo(self):\n",
    "        \"\"\"Train YOLO model for Sagittal T1 images\"\"\"\n",
    "        try:\n",
    "            model = YOLO('yolov8x.pt')\n",
    "            \n",
    "            config = {\n",
    "                'data': self.yaml_path,\n",
    "                'imgsz': self.img_size,\n",
    "                'batch': 8,\n",
    "                'epochs': 1, \n",
    "                'patience': 5,\n",
    "                'device': 'mps',\n",
    "                'workers': 4,\n",
    "                'project': 'spine_detection',\n",
    "                'name': 'sagittal_t2_yolo',\n",
    "                'exist_ok': True,\n",
    "                'pretrained': True,\n",
    "                'optimizer': 'AdamW',\n",
    "                'verbose': True,\n",
    "                'seed': 42,\n",
    "                'deterministic': True,\n",
    "                'dropout': 0.1,\n",
    "                'lr0': 0.001,\n",
    "                'lrf': 0.01,\n",
    "                'momentum': 0.937,\n",
    "                'weight_decay': 0.0005,\n",
    "                'warmup_epochs': 5,\n",
    "                'warmup_momentum': 0.8,\n",
    "                'box': 7.5,\n",
    "                'cls': 0.5,\n",
    "                'dfl': 1.5,\n",
    "                'close_mosaic': 10,\n",
    "                'amp': True,  \n",
    "                'rect': True,  \n",
    "                'multi_scale': True,  \n",
    "                'val': True, \n",
    "            }\n",
    "                        \n",
    "            results = model.train(**config)\n",
    "            return model, results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def get_training_stats(self):\n",
    "        \"\"\"Get statistics about the processed dataset\"\"\"\n",
    "        stats = {\n",
    "            'total_series': len(self.instance_coords_df['series_id'].unique()),\n",
    "            'total_instances': len(self.instance_coords_df),\n",
    "            'train_images': len(self.processed_df[self.processed_df['split'] == 'train']),\n",
    "            'val_images': len(self.processed_df[self.processed_df['split'] == 'val']),\n",
    "            'level_coverage': {}\n",
    "        }\n",
    "        \n",
    "        for level in ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']:\n",
    "            count = self.instance_coords_df[f'{level}_center_x'].notna().sum()\n",
    "            stats['level_coverage'][level] = {\n",
    "                'count': count,\n",
    "                'percentage': count/len(self.instance_coords_df)*100\n",
    "            }\n",
    "            \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sagittal T2/STIR\n",
    "images_dir  = '/Users/danipopov/Projects/RSNA2024/data/train_images'\n",
    "output_dir = '/Users/danipopov/Projects/RSNA2024/data/spine_dataset_segt2'\n",
    "\n",
    "sag_t2_model = SagT2_YOLO(\n",
    "    df=sagt2_df,\n",
    "    images_dir=images_dir,\n",
    "    output_dir=output_dir,\n",
    "    img_size=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the folders execpt the weights\n",
    "shutil.rmtree(\"/Users/danipopov/Projects/RSNA2024/spine_detection/sagittal_t2_yolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example for Sagittal T2:\n",
    "image_paths = glob.glob('/Users/danipopov/Projects/RSNA2024/data/train_images/4646740/3666319702/*.dcm')\n",
    "process_multiple_images(image_paths, '/Users/danipopov/Projects/RSNA2024/models/sagittal_t2_spine_detector.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice the yolo model for the Sagittal T2 is not very good, as it detects only 10% of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Axial T2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Axial T2, we want our yolo model to detect both left and right sides of the disk in the axial plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxialT2_YOLO:\n",
    "    def __init__(self, df, images_dir, output_dir, img_size=384):\n",
    "        \"\"\"\n",
    "        Initialize Axial T2 YOLO detector\n",
    "        Args:\n",
    "            df: DataFrame with annotations\n",
    "            images_dir: Path to DICOM images\n",
    "            output_dir: Path to save processed dataset\n",
    "            img_size: Target image size for YOLO\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.images_dir = images_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Create initial directory structure\n",
    "        self.create_dataset_structure()\n",
    "        \n",
    "        # Process dataset\n",
    "        self.processed_records = self.process_instance_coordinates()\n",
    "        self.processed_df = self.process_spine_dataset()\n",
    "        \n",
    "        # Create YAML and train model\n",
    "        self.yaml_path = self.create_dataset_yaml()\n",
    "        self.model, self.results = self.train_yolo()\n",
    "\n",
    "    def create_dataset_structure(self):\n",
    "        \"\"\"Create YOLO dataset directory structure\"\"\"\n",
    "        for split in ['train', 'val']:\n",
    "            for subdir in ['images', 'labels']:\n",
    "                path = os.path.join(self.output_dir, split, subdir)\n",
    "                os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    def process_instance_coordinates(self):\n",
    "        \"\"\"\n",
    "        Process coordinates for Axial images, creating separate records for each level\n",
    "        that has both left and right coordinates\n",
    "        Returns list of records with coordinates for each complete level\n",
    "        \"\"\"\n",
    "        result_records = []\n",
    "\n",
    "        # Group by instance to process each image separately\n",
    "        for (study_id, series_id, instance_number), instance_data in self.df.groupby(['study_id', 'series_id', 'instance_number']):\n",
    "            # Process each level separately\n",
    "            levels_data = {}\n",
    "            \n",
    "            for _, row in instance_data.iterrows():\n",
    "                level = row['level']\n",
    "                condition = row['condition']\n",
    "                x, y = row['x'], row['y']\n",
    "                \n",
    "                if level not in levels_data:\n",
    "                    levels_data[level] = {'left': None, 'right': None}\n",
    "                \n",
    "                if 'Left' in condition:\n",
    "                    levels_data[level]['left'] = (x, y)\n",
    "                elif 'Right' in condition:\n",
    "                    levels_data[level]['right'] = (x, y)\n",
    "            \n",
    "            # Only create records for levels with both coordinates\n",
    "            for level, coords in levels_data.items():\n",
    "                if coords['left'] is not None and coords['right'] is not None:\n",
    "                    record = {\n",
    "                        'study_id': study_id,\n",
    "                        'series_id': series_id,\n",
    "                        'instance_number': instance_number,\n",
    "                        'level': level,\n",
    "                        'left_coord': coords['left'],\n",
    "                        'right_coord': coords['right']\n",
    "                    }\n",
    "                    result_records.append(record)\n",
    "        \n",
    "        # Print statistics\n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        total_complete = len(result_records)\n",
    "        \n",
    "        print(f\"Total complete level pairs: {total_complete}\")\n",
    "        print(\"\\nBy level statistics:\")\n",
    "        \n",
    "        for level in ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']:\n",
    "            level_count = sum(1 for r in result_records if r['level'] == level)\n",
    "            if total_complete > 0:\n",
    "                percentage = (level_count / total_complete) * 100\n",
    "            else:\n",
    "                percentage = 0\n",
    "            print(f\"{level}: {level_count} complete pairs ({percentage:.1f}%)\")\n",
    "        \n",
    "        return result_records\n",
    "\n",
    "    def create_yolo_annotation(self, record, image_width, image_height):\n",
    "        \"\"\"\n",
    "        Create YOLO format annotations for a specific level\n",
    "        Returns list of annotations for left and right sides\n",
    "        \"\"\"\n",
    "        annotations = []\n",
    "        box_width = 0.05\n",
    "        box_height = 0.05\n",
    "        \n",
    "        # Both coordinates must be present\n",
    "        x, y = record['left_coord']\n",
    "        x_norm = x / image_width\n",
    "        y_norm = y / image_height\n",
    "        annotations.append(f\"0 {x_norm:.6f} {y_norm:.6f} {box_width:.6f} {box_height:.6f}\")\n",
    "        \n",
    "        x, y = record['right_coord']\n",
    "        x_norm = x / image_width\n",
    "        y_norm = y / image_height\n",
    "        annotations.append(f\"1 {x_norm:.6f} {y_norm:.6f} {box_width:.6f} {box_height:.6f}\")\n",
    "        \n",
    "        return annotations\n",
    "\n",
    "    def process_spine_dataset(self):\n",
    "        \"\"\"Process and save dataset in YOLO format\"\"\"\n",
    "        # Split studies\n",
    "        studies = set(record['study_id'] for record in self.processed_records)\n",
    "        train_studies, val_studies = train_test_split(list(studies), train_size=0.8, random_state=42)\n",
    "        \n",
    "        processed_counts = {'train': 0, 'val': 0}\n",
    "        failed_cases = []\n",
    "        \n",
    "        for record in tqdm(self.processed_records, desc=\"Processing Axial images\"):\n",
    "            try:\n",
    "                study_id = str(int(record['study_id']))\n",
    "                series_id = str(int(record['series_id']))\n",
    "                instance_number = str(int(record['instance_number']))\n",
    "                level = record['level'].lower().replace('/', '_')\n",
    "                \n",
    "                # Construct image path\n",
    "                img_path = os.path.join(self.images_dir, study_id, series_id, f\"{instance_number}.dcm\")\n",
    "                if not os.path.exists(img_path):\n",
    "                    img_path = os.path.join(self.images_dir, study_id, series_id, instance_number)\n",
    "                    if os.path.exists(img_path + '.dcm'):\n",
    "                        img_path = img_path + '.dcm'\n",
    "                    else:\n",
    "                        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "                \n",
    "                # Read and process image\n",
    "                ds = pydicom.dcmread(img_path)\n",
    "                image = ds.pixel_array\n",
    "                h, w = image.shape\n",
    "                \n",
    "                # Create annotations\n",
    "                annotations = self.create_yolo_annotation(record, w, h)\n",
    "                \n",
    "                # Prepare image\n",
    "                image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "                image_resized = cv2.resize(image_normalized, (self.img_size, self.img_size))\n",
    "                \n",
    "                # Determine split\n",
    "                is_train = record['study_id'] in train_studies\n",
    "                split = 'train' if is_train else 'val'\n",
    "                \n",
    "                # Create unique filenames including level information\n",
    "                base_filename = f\"{study_id}_{series_id}_{instance_number}_{level}\"\n",
    "                img_filename = f\"{base_filename}.png\"\n",
    "                label_filename = f\"{base_filename}.txt\"\n",
    "                \n",
    "                # Save files\n",
    "                cv2.imwrite(os.path.join(self.output_dir, split, 'images', img_filename), \n",
    "                           image_resized)\n",
    "                with open(os.path.join(self.output_dir, split, 'labels', label_filename), 'w') as f:\n",
    "                    f.write('\\n'.join(annotations))\n",
    "                \n",
    "                processed_counts[split] += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_cases.append((study_id, series_id, instance_number, str(e)))\n",
    "        \n",
    "        print(f\"\\nProcessing Summary:\")\n",
    "        print(f\"Training images: {processed_counts['train']}\")\n",
    "        print(f\"Validation images: {processed_counts['val']}\")\n",
    "        \n",
    "        if failed_cases:\n",
    "            print(\"\\nFailed cases:\")\n",
    "            for case in failed_cases:\n",
    "                print(f\"Study {case[0]}, Series {case[1]}, Instance {case[2]}: {case[3]}\")\n",
    "        \n",
    "        return pd.DataFrame(self.processed_records)\n",
    "\n",
    "    def create_dataset_yaml(self):\n",
    "        \"\"\"Create YOLO dataset configuration file\"\"\"\n",
    "        yaml_content = {\n",
    "            'path': os.path.abspath(self.output_dir),\n",
    "            'train': 'train/images',\n",
    "            'val': 'val/images',\n",
    "            'nc': 2,  # number of classes (left and right)\n",
    "            'names': {\n",
    "                0: 'left',\n",
    "                1: 'right'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        yaml_path = os.path.join(self.output_dir, 'dataset.yaml')\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            yaml.dump(yaml_content, f, sort_keys=False)\n",
    "\n",
    "        return yaml_path\n",
    "\n",
    "    def train_yolo(self):\n",
    "        \"\"\"Train YOLO model\"\"\"\n",
    "        try:\n",
    "            model = YOLO('yolov8x.pt')\n",
    "            \n",
    "            config = {\n",
    "                'data': self.yaml_path,\n",
    "                'imgsz': self.img_size,\n",
    "                'batch': 8,\n",
    "                'epochs': 1,\n",
    "                'patience': 5,\n",
    "                'device': 'mps',\n",
    "                'workers': 8,\n",
    "                'project': 'spine_detection',\n",
    "                'name': 'axial_t2_yolo',\n",
    "                'exist_ok': True,\n",
    "                'pretrained': True,\n",
    "                'optimizer': 'AdamW',\n",
    "                'verbose': True,\n",
    "                'seed': 42,\n",
    "                'deterministic': True,\n",
    "                'dropout': 0.2,\n",
    "                'lr0': 0.001,\n",
    "                'lrf': 0.01,\n",
    "                'momentum': 0.937,\n",
    "                'weight_decay': 0.0005,\n",
    "                'warmup_epochs': 10,\n",
    "                'warmup_momentum': 0.8,\n",
    "                'box': 7.5,\n",
    "                'cls': 0.5,\n",
    "                'dfl': 1.5,\n",
    "                'close_mosaic': 10,\n",
    "                'amp': True\n",
    "            }\n",
    "            \n",
    "            results = model.train(**config)\n",
    "            return model, results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {str(e)}\")\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Axial T2\n",
    "images_dir  = '/Users/danipopov/Projects/RSNA2024/data/train_images'\n",
    "output_dir = '/Users/danipopov/Projects/RSNA2024/data/spine_dataset_axialt2'\n",
    "\n",
    "axial_t2_model = AxialT2_YOLO(\n",
    "    df=axialt2_df,\n",
    "    images_dir=images_dir,\n",
    "    output_dir=output_dir,\n",
    "    img_size=384\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the folders execpt the weights\n",
    "shutil.rmtree(\"/Users/danipopov/Projects/RSNA2024/spine_detection/axial_t2_yolo\")\n",
    "shutil.rmtree(\"/Users/danipopov/Projects/RSNA2024/data/spine_dataset_axialt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_axial_predictions(image_path, model_path='axial_t2_spine_detector.pt', \n",
    "                         conf_threshold=0.25, iou_threshold=0.45, img_size=384):\n",
    "    \"\"\"\n",
    "    Plot YOLO predictions for axial images showing left and right sides\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to DICOM image\n",
    "        model_path: Path to saved YOLO model\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "        img_size: Image size for model input\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read DICOM\n",
    "    ds = pydicom.dcmread(image_path)\n",
    "    image = ds.pixel_array\n",
    "    \n",
    "    # Normalize and resize\n",
    "    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    image_resized = cv2.resize(image_normalized, (img_size, img_size))\n",
    "    \n",
    "    # Convert grayscale to RGB\n",
    "    image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    # Plot original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot image with predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Predictions')\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict(\n",
    "        source=image_rgb,\n",
    "        conf=conf_threshold,\n",
    "        iou=iou_threshold\n",
    "    )\n",
    "    \n",
    "    # Define colors and names for left/right sides\n",
    "    side_colors = {'left': 'red', 'right': 'blue'}\n",
    "    side_names = {0: 'Left', 1: 'Right'}\n",
    "    \n",
    "    if results[0].boxes is not None:\n",
    "        boxes = results[0].boxes.cpu().numpy()\n",
    "        \n",
    "        # Sort boxes by x-coordinate (left to right)\n",
    "        box_data = []\n",
    "        for box in boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = box.conf[0]\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            box_data.append((x1, cls_id, conf, x1, y1, x2, y2))\n",
    "        \n",
    "        box_data.sort()  # Sort by x1 coordinate\n",
    "        \n",
    "        # Plot each detection\n",
    "        for i, (_, cls_id, conf, x1, y1, x2, y2) in enumerate(box_data):\n",
    "            color = side_colors['left'] if cls_id == 0 else side_colors['right']\n",
    "            side_name = side_names[cls_id]\n",
    "            \n",
    "            # Draw bounding box\n",
    "            plt.gca().add_patch(plt.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                fill=False, color=color, linewidth=2\n",
    "            ))\n",
    "            \n",
    "            # Add label\n",
    "            plt.text(\n",
    "                x2 + 5, (y1 + y2) / 2, \n",
    "                f'{side_name}: {conf:.2f}',\n",
    "                color=color, fontsize=8, verticalalignment='center',\n",
    "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none')\n",
    "            )\n",
    "            \n",
    "            # Print detection info\n",
    "            print(f\"Found {side_name} side with confidence {conf:.2f}\")\n",
    "    else:\n",
    "        print(\"No detections found\")\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_axial_images(image_paths, model_path='axial_t2_spine_detector.pt', \n",
    "                                conf_threshold=0.15, iou_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process multiple axial images and display their predictions\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of paths to DICOM images\n",
    "        model_path: Path to saved YOLO model\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "    \"\"\"\n",
    "    # First, filter images that have predictions\n",
    "    valid_images = []\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    print(\"Analyzing images...\")\n",
    "    for img_path in tqdm(image_paths):\n",
    "        ds = pydicom.dcmread(img_path)\n",
    "        image = ds.pixel_array\n",
    "        image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "        image_resized = cv2.resize(image_normalized, (384, 384))\n",
    "        image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "        \n",
    "        results = model.predict(\n",
    "            source=image_rgb,\n",
    "            conf=conf_threshold,\n",
    "            iou=iou_threshold\n",
    "        )\n",
    "        \n",
    "        if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "            valid_images.append(img_path)\n",
    "    \n",
    "    if not valid_images:\n",
    "        print(\"No images with valid predictions found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nFound {len(valid_images)} images with valid predictions\")\n",
    "    \n",
    "    # Process only images with predictions\n",
    "    for idx, img_path in enumerate(valid_images):\n",
    "        if idx == 5:\n",
    "            break\n",
    "        print(f\"\\nProcessing image: {os.path.basename(img_path)}\")\n",
    "        plot_axial_predictions(img_path, model_path, conf_threshold, iou_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example for Axial T2:\n",
    "image_paths = glob.glob('/Users/danipopov/Projects/RSNA2024/data/train_images/4646740/3201256954/*.dcm')\n",
    "process_multiple_axial_images(image_paths, '/Users/danipopov/Projects/RSNA2024/models/axial_t2_spine_detector.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will want to see the roi's of spine levels and right and left sides of disk in axial plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rois_sigattal(image_path, model_path, \n",
    "                            conf_threshold=0.25, iou_threshold=0.45, \n",
    "                            img_size=384, roi_size=64):\n",
    "    \"\"\"\n",
    "    Plot Original Image, Image with predictions, and ROIs of the detections\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to DICOM image\n",
    "        model_path: Path to YOLO model\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "        img_size: Size for model input\n",
    "        roi_size: Size of ROI crops\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read DICOM\n",
    "    ds = pydicom.dcmread(image_path)\n",
    "    image = ds.pixel_array\n",
    "    \n",
    "    # Normalize and resize\n",
    "    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    image_resized = cv2.resize(image_normalized, (img_size, img_size))\n",
    "    \n",
    "    # Convert grayscale to RGB\n",
    "    image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict(\n",
    "        source=image_rgb,\n",
    "        conf=conf_threshold,\n",
    "        iou=iou_threshold\n",
    "    )\n",
    "    \n",
    "    # Define colors and names\n",
    "    colors = ['red', 'green', 'blue', 'yellow', 'purple']\n",
    "    level_names = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "    \n",
    "    # Get number of detections for subplot layout\n",
    "    num_detections = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "    \n",
    "    if num_detections == 0:\n",
    "        print(\"No detections found\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with larger size\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Plot original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot image with predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Predictions')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if results[0].boxes is not None:\n",
    "        boxes = results[0].boxes.cpu().numpy()\n",
    "        \n",
    "        # Sort boxes by y-coordinate\n",
    "        box_data = []\n",
    "        for box in boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = box.conf[0]\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            box_data.append((y1, cls_id, conf, x1, y1, x2, y2))\n",
    "        \n",
    "        box_data.sort()  # Sort by y1 coordinate\n",
    "    \n",
    "        # Plot each detection with larger text and boxes\n",
    "        for i, (_, cls_id, conf, x1, y1, x2, y2) in enumerate(box_data):\n",
    "            color = colors[cls_id]\n",
    "            level_name = level_names[cls_id]\n",
    "            \n",
    "            # Draw bounding box with thicker line\n",
    "            plt.gca().add_patch(plt.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                fill=False, color=color, linewidth=3\n",
    "            ))\n",
    "            \n",
    "            # Add label with larger font and better positioning\n",
    "            plt.text(\n",
    "                x1, y1 - 10, \n",
    "                f'{level_name}: {conf:.2f}',\n",
    "                color=color, fontsize=10, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor=color, pad=2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Found {level_name} with confidence {conf:.2f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a separate figure for ROIs with larger size\n",
    "    if num_detections > 0:\n",
    "        # Calculate grid dimensions\n",
    "        n_cols = min(3, num_detections)\n",
    "        n_rows = (num_detections + n_cols - 1) // n_cols\n",
    "        \n",
    "        plt.figure(figsize=(15, 5 * n_rows))\n",
    "        \n",
    "        for i, (_, cls_id, conf, x1, y1, x2, y2) in enumerate(box_data):\n",
    "            plt.subplot(n_rows, n_cols, i + 1)\n",
    "            \n",
    "            # Calculate ROI coordinates\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "            half_size = roi_size // 2\n",
    "            \n",
    "            # Extract ROI\n",
    "            roi = image_resized[\n",
    "                max(0, center_y - half_size):min(img_size, center_y + half_size),\n",
    "                max(0, center_x - half_size):min(img_size, center_x + half_size)\n",
    "            ]\n",
    "            \n",
    "            # Resize ROI if needed\n",
    "            if roi.shape[0] != roi_size or roi.shape[1] != roi_size:\n",
    "                roi = cv2.resize(roi, (roi_size, roi_size))\n",
    "            \n",
    "            plt.imshow(roi, cmap='gray')\n",
    "            plt.title(f'ROI: {level_names[cls_id]}\\nConfidence: {conf:.2f}')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example Sagittal T1\n",
    "image_paths = glob.glob('/Users/danipopov/Projects/RSNA2024/data/train_images/4646740/3486248476/*.dcm')\n",
    "plot_rois_sigattal(image_paths[0] ,'/Users/danipopov/Projects/RSNA2024/models/sagittal_t1_spine_detector.pt', conf_threshold=0.25, iou_threshold=0.45, img_size=384, roi_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example Sagittal T2/STIR\n",
    "image_paths = glob.glob('/Users/danipopov/Projects/RSNA2024/data/train_images/4646740/3666319702/*.dcm')\n",
    "plot_rois_sigattal(image_paths[4] ,'/Users/danipopov/Projects/RSNA2024/models/sagittal_t2_spine_detector.pt', conf_threshold=0.25, iou_threshold=0.45, img_size=384, roi_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rois_axial(image_path, model_path, \n",
    "                            conf_threshold=0.25, iou_threshold=0.45, \n",
    "                            img_size=384, roi_size=64):\n",
    "    \"\"\"\n",
    "    Plot Original Image, Image with predictions, and ROIs of the detections\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to DICOM image\n",
    "        model_path: Path to YOLO model\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "        iou_threshold: IOU threshold for NMS\n",
    "        img_size: Size for model input\n",
    "        roi_size: Size of ROI crops\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read DICOM\n",
    "    ds = pydicom.dcmread(image_path)\n",
    "    image = ds.pixel_array\n",
    "    \n",
    "    # Normalize and resize\n",
    "    image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "    image_resized = cv2.resize(image_normalized, (img_size, img_size))\n",
    "    \n",
    "    # Convert grayscale to RGB\n",
    "    image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "    \n",
    "    # Get predictions\n",
    "    results = model.predict(\n",
    "        source=image_rgb,\n",
    "        conf=conf_threshold,\n",
    "        iou=iou_threshold\n",
    "    )\n",
    "    \n",
    "    # Define colors and names\n",
    "    colors = ['red', 'blue']\n",
    "    level_names = ['Left', 'Right']\n",
    "    \n",
    "    # Get number of detections for subplot layout\n",
    "    num_detections = len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "    \n",
    "    if num_detections == 0:\n",
    "        print(\"No detections found\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with larger size\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    \n",
    "    # Plot original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Original Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Plot image with predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(image_resized, cmap='gray')\n",
    "    plt.title('Predictions')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if results[0].boxes is not None:\n",
    "        boxes = results[0].boxes.cpu().numpy()\n",
    "        \n",
    "        # Sort boxes by y-coordinate\n",
    "        box_data = []\n",
    "        for box in boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            conf = box.conf[0]\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            box_data.append((y1, cls_id, conf, x1, y1, x2, y2))\n",
    "        \n",
    "        box_data.sort()  # Sort by y1 coordinate\n",
    "    \n",
    "        # Plot each detection with larger text and boxes\n",
    "        for i, (_, cls_id, conf, x1, y1, x2, y2) in enumerate(box_data):\n",
    "            color = colors[cls_id]\n",
    "            level_name = level_names[cls_id]\n",
    "            \n",
    "            # Draw bounding box with thicker line\n",
    "            plt.gca().add_patch(plt.Rectangle(\n",
    "                (x1, y1), x2-x1, y2-y1,\n",
    "                fill=False, color=color, linewidth=3\n",
    "            ))\n",
    "            \n",
    "            # Add label with larger font and better positioning\n",
    "            plt.text(\n",
    "                x1, y1 - 10, \n",
    "                f'{level_name}: {conf:.2f}',\n",
    "                color=color, fontsize=10, \n",
    "                bbox=dict(facecolor='white', alpha=0.8, edgecolor=color, pad=2)\n",
    "            )\n",
    "            \n",
    "            print(f\"Found {level_name} with confidence {conf:.2f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a separate figure for ROIs with larger size\n",
    "    if num_detections > 0:\n",
    "        # Calculate grid dimensions\n",
    "        n_cols = 2\n",
    "        n_rows = 1\n",
    "        \n",
    "        plt.figure(figsize=(15, 5 * n_rows))\n",
    "        \n",
    "        for i, (_, cls_id, conf, x1, y1, x2, y2) in enumerate(box_data):\n",
    "            plt.subplot(n_rows, n_cols, i + 1)\n",
    "            \n",
    "            # Calculate ROI coordinates\n",
    "            center_x = int((x1 + x2) / 2)\n",
    "            center_y = int((y1 + y2) / 2)\n",
    "            half_size = roi_size // 2\n",
    "            \n",
    "            # Extract ROI\n",
    "            roi = image_resized[\n",
    "                max(0, center_y - half_size):min(img_size, center_y + half_size),\n",
    "                max(0, center_x - half_size):min(img_size, center_x + half_size)\n",
    "            ]\n",
    "            \n",
    "            # Resize ROI if needed\n",
    "            if roi.shape[0] != roi_size or roi.shape[1] != roi_size:\n",
    "                roi = cv2.resize(roi, (roi_size, roi_size))\n",
    "            \n",
    "            plt.imshow(roi, cmap='gray')\n",
    "            plt.title(f'ROI: {level_names[cls_id]}\\nConfidence: {conf:.2f}')\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example Axial T2\n",
    "image_paths = glob.glob('/Users/danipopov/Projects/RSNA2024/data/train_images/4646740/3201256954/*.dcm')\n",
    "plot_rois_axial(image_paths[1] ,'/Users/danipopov/Projects/RSNA2024/models/axial_t2_spine_detector.pt', conf_threshold=0.25, iou_threshold=0.45, img_size=384, roi_size=96)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Organization for Model Training\n",
    "\n",
    "Before training our models, we need to build a structured dataset. We will use the `train_df` DataFrame and our YOLO models to process each `study_id`. For each study, we will:\n",
    "\n",
    "1. Select 5 representative images from each plane (Axial T2, Sagittal T1, and Sagittal T2/STIR)\n",
    "2. Extract ROIs (Regions of Interest) for:\n",
    "   - Spine levels in sagittal planes\n",
    "   - Left and right sides of the disk in axial plane\n",
    "3. Only include images where we can successfully extract all ROIs (5 spine levels plus left and right sides)\n",
    "\n",
    "The dataset will be organized in the following directory structure:\n",
    "\n",
    "```\n",
    "images_dataset/\n",
    "  ‚îî‚îÄ‚îÄ study_id/\n",
    "      ‚îú‚îÄ‚îÄ Axial_T2/\n",
    "      ‚îÇ     ‚îú‚îÄ‚îÄ img_1/\n",
    "      ‚îÇ     ‚îÇ     ‚îú‚îÄ‚îÄ 1.png\n",
    "      ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ 2.png\n",
    "      ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ 3.png\n",
    "      ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ 4.png\n",
    "      ‚îÇ     ‚îÇ     ‚îî‚îÄ‚îÄ 5.png\n",
    "      ‚îÇ     ‚îÇ\n",
    "      ‚îÇ     ‚îî‚îÄ‚îÄ img_2/\n",
    "      ‚îÇ           ‚îú‚îÄ‚îÄ 1.png\n",
    "      ‚îÇ           ‚îî‚îÄ‚îÄ 2.png\n",
    "      ‚îÇ           ‚îî‚îÄ‚îÄ 3.png\n",
    "      ‚îÇ           ‚îî‚îÄ‚îÄ 4.png\n",
    "      ‚îÇ           ‚îî‚îÄ‚îÄ 5.png\n",
    "      ‚îî‚îÄ‚îÄ Sagittal_T1/\n",
    "            ‚îú‚îÄ‚îÄ img_1/\n",
    "            ‚îÇ     ‚îú‚îÄ‚îÄ 1.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 2.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 3.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 4.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 5.png\n",
    "            ‚îî‚îÄ‚îÄ img_2/\n",
    "                  ‚îú‚îÄ‚îÄ 1.png\n",
    "                  ‚îî‚îÄ‚îÄ 2.png\n",
    "                  ‚îî‚îÄ‚îÄ 3.png\n",
    "                  ‚îî‚îÄ‚îÄ 4.png\n",
    "                  ‚îî‚îÄ‚îÄ 5.png\n",
    "        ‚îî‚îÄ‚îÄ Sagittal_T2/\n",
    "            ‚îú‚îÄ‚îÄ img_1/\n",
    "            ‚îÇ     ‚îú‚îÄ‚îÄ 1.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 2.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 3.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 4.png\n",
    "            ‚îÇ     ‚îî‚îÄ‚îÄ 5.png\n",
    "            ‚îî‚îÄ‚îÄ img_2/\n",
    "                  ‚îú‚îÄ‚îÄ 1.png\n",
    "                  ‚îî‚îÄ‚îÄ 2.png\n",
    "                  ‚îî‚îÄ‚îÄ 3.png\n",
    "                  ‚îî‚îÄ‚îÄ 4.png\n",
    "                  ‚îî‚îÄ‚îÄ 5.png\n",
    "\n",
    "``'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rois(image, boxes, roi_size=96):\n",
    "    rois = [] \n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "        center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "        roi = image[\n",
    "            max(0, center_y - roi_size // 2):min(image.shape[0], center_y + roi_size // 2),\n",
    "            max(0, center_x - roi_size // 2):min(image.shape[1], center_x + roi_size // 2)\n",
    "        ]\n",
    "        if roi.shape[0] != roi_size or roi.shape[1] != roi_size:\n",
    "            roi = cv2.resize(roi, (roi_size, roi_size))\n",
    "        rois.append(roi)\n",
    "    return rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_study(study_id, meta_df, src_path, dst_path, axial_model, sagittal_model_1, sagittal_model_2):\n",
    "    study_meta = meta_df[int(study_id)]\n",
    "    series_dict = {desc.replace('/', '_'): [] for desc in ['Axial T2', 'Sagittal T1', 'Sagittal T2/STIR']}\n",
    "    \n",
    "    # First, validate that we have enough valid images for all series\n",
    "    valid_study = True\n",
    "    \n",
    "    for series_id, desc in zip(study_meta['series_ids'], study_meta['series_descriptions']):\n",
    "        series_dict[desc.replace('/', '_')].append(series_id)\n",
    "    \n",
    "    for desc, series_list in series_dict.items():\n",
    "        # Collect all images for this description\n",
    "        all_images = []\n",
    "        for series_id in series_list:\n",
    "            series_path = os.path.join(src_path, study_id, series_id)\n",
    "            all_images.extend(glob.glob(os.path.join(series_path, '*.dcm')))\n",
    "        \n",
    "        if len(all_images) == 0:\n",
    "            valid_study = False\n",
    "            print(f\"No images found for {study_id} - {desc}\")\n",
    "            break\n",
    "            \n",
    "        # Sort images and get model for this description\n",
    "        all_images.sort()\n",
    "        if 'Axial' in desc:\n",
    "            model = axial_model\n",
    "            required_detections = 2  # Left and right\n",
    "        elif 'T1' in desc:\n",
    "            model = sagittal_model_1\n",
    "            required_detections = 5  # All spine levels\n",
    "        else:  # 'Sagittal T2/STIR'\n",
    "            model = sagittal_model_2\n",
    "            required_detections = 5  # All spine levels\n",
    "            \n",
    "        # Find valid images (those with all required detections)\n",
    "        valid_images = []\n",
    "        for img_path in all_images:\n",
    "            ds = pydicom.dcmread(img_path)\n",
    "            image = ds.pixel_array\n",
    "            image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "            image_resized = cv2.resize(image_normalized, (384, 384))\n",
    "            image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "            \n",
    "            results = model.predict(source=image_rgb, conf=0.25, iou=0.45)\n",
    "            \n",
    "            if results[0].boxes is not None and len(results[0].boxes) == required_detections:\n",
    "                valid_images.append(img_path)\n",
    "        \n",
    "        # Check if we have enough valid images\n",
    "        if len(valid_images) < 5:\n",
    "            valid_study = False\n",
    "            print(f\"Not enough valid images for {study_id} - {desc}. Found {len(valid_images)}, need 5\")\n",
    "            break\n",
    "    \n",
    "    # If study is not valid, return without creating any directories or saving images\n",
    "    if not valid_study:\n",
    "        return False\n",
    "        \n",
    "    # If we get here, we have enough valid images for all series\n",
    "    # Now process and save the images\n",
    "    for desc, series_list in series_dict.items():\n",
    "        desc_path = os.path.join(dst_path, study_id, desc)\n",
    "        os.makedirs(desc_path, exist_ok=True)\n",
    "        \n",
    "        all_images = []\n",
    "        for series_id in series_list:\n",
    "            series_path = os.path.join(src_path, study_id, series_id)\n",
    "            all_images.extend(glob.glob(os.path.join(series_path, '*.dcm')))\n",
    "            \n",
    "        all_images.sort()\n",
    "        \n",
    "        if 'Axial' in desc:\n",
    "            model = axial_model\n",
    "            required_detections = 2\n",
    "        elif 'T1' in desc:\n",
    "            model = sagittal_model_1\n",
    "            required_detections = 5\n",
    "        else:\n",
    "            model = sagittal_model_2\n",
    "            required_detections = 5\n",
    "            \n",
    "        valid_images = []\n",
    "        for img_path in all_images:\n",
    "            ds = pydicom.dcmread(img_path)\n",
    "            image = ds.pixel_array\n",
    "            image_normalized = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "            image_resized = cv2.resize(image_normalized, (384, 384))\n",
    "            image_rgb = np.stack([image_resized] * 3, axis=-1)\n",
    "            \n",
    "            results = model.predict(source=image_rgb, conf=0.25, iou=0.45)\n",
    "            \n",
    "            if results[0].boxes is not None and len(results[0].boxes) == required_detections:\n",
    "                valid_images.append((img_path, image_resized, results[0].boxes))\n",
    "                \n",
    "        # Select 5 evenly spaced images from valid images\n",
    "        indices = np.linspace(0, len(valid_images)-1, 5, dtype=int)\n",
    "        selected_images = [valid_images[i] for i in indices]\n",
    "        \n",
    "        # Process selected images\n",
    "        for idx, (img_path, image_resized, boxes) in enumerate(selected_images):\n",
    "            rois = extract_rois(image_resized, boxes)\n",
    "            \n",
    "            img_folder = os.path.join(desc_path, f'img_{idx+1}')\n",
    "            os.makedirs(img_folder, exist_ok=True)\n",
    "            \n",
    "            for roi_idx, roi in enumerate(rois):\n",
    "                cv2.imwrite(os.path.join(img_folder, f'{roi_idx+1}.png'), roi)\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(train_df, meta_df, src_path, dst_path, axial_model_path, sagittal_model_path_1, sagittal_model_path_2):\n",
    "    axial_model = YOLO(axial_model_path)\n",
    "    sagittal_model_1 = YOLO(sagittal_model_path_1)\n",
    "    sagittal_model_2 = YOLO(sagittal_model_path_2)\n",
    "    \n",
    "    valid_studies = []\n",
    "    for i, study_id in enumerate(tqdm(train_df['study_id'].unique())):\n",
    "        if process_study(str(study_id), meta_df, src_path, dst_path, \n",
    "                        axial_model, sagittal_model_1, sagittal_model_2):\n",
    "            valid_studies.append(study_id)\n",
    "        \n",
    "        if i % 1 == 0:  # Clear output every iteration\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    print(f\"Processed {len(valid_studies)} valid studies out of {len(train_df['study_id'].unique())}\")\n",
    "    return valid_studies\n",
    "            \n",
    "# Usage\n",
    "src_path = '/Users/danipopov/Projects/RSNA2024/data/train_images'\n",
    "dst_path = '/Users/danipopov/Projects/RSNA2024/data/images_dataset'\n",
    "axial_model_path = '/Users/danipopov/Projects/RSNA2024/models/axial_t2_spine_detector.pt'\n",
    "sagittal_model_path_1 = '/Users/danipopov/Projects/RSNA2024/models/sagittal_t1_spine_detector.pt'\n",
    "sagittal_model_path_2 = '/Users/danipopov/Projects/RSNA2024/models/sagittal_t2_spine_detector.pt'\n",
    "\n",
    "valid_studies = create_dataset(train_df, meta_df, src_path, dst_path, \n",
    "                             axial_model_path, sagittal_model_path_1, sagittal_model_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_studies = glob.glob(f'/Users/danipopov/Projects/RSNA2024/data/images_dataset/*')\n",
    "print(f\"Processed {len(valid_studies)} valid studies out of {len(train_df['study_id'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine our newly created dataset, which includes 1,825 study IDs. For each study, we have extracted 5 images per plane (Sagittal T1, Sagittal T2/STIR, and Axial T2), along with their corresponding Regions of Interest (ROIs). This examination will help us validate the quality and consistency of our preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the image_dataset \n",
    "image_dir = '/Users/danipopov/Projects/RSNA2024/data/images_dataset'\n",
    "study_dir = os.path.join(image_dir, '293713262')\n",
    "# Sagittal T1\n",
    "sagt1_dir = os.path.join(study_dir, 'Sagittal T1')\n",
    "image_dir_sag1 = sorted(glob.glob(os.path.join(sagt1_dir, 'img_*')))\n",
    "roi_dir_sag1 = sorted(glob.glob(os.path.join(image_dir_sag1[0], '*.png')))\n",
    "# Sagittal T2/STIR\n",
    "sagt2_dir = os.path.join(study_dir, 'Sagittal T2_STIR')\n",
    "image_dir_sag2 = sorted(glob.glob(os.path.join(sagt2_dir, 'img_*')))\n",
    "roi_dir_sag2 = sorted(glob.glob(os.path.join(image_dir_sag2[0], '*.png')))\n",
    "# Axial T2\n",
    "axial_dir = os.path.join(study_dir, 'Axial T2')\n",
    "image_dir_axial = sorted(glob.glob(os.path.join(axial_dir, 'img_*')))\n",
    "roi_dir_axial = sorted(glob.glob(os.path.join(image_dir_axial[0], '*.png')))\n",
    "\n",
    "# YOLO model weights\n",
    "axial_model_path = '/Users/danipopov/Projects/RSNA2024/models/axial_t2_spine_detector.pt'\n",
    "sagittal_model_path_1 = '/Users/danipopov/Projects/RSNA2024/models/sagittal_t1_spine_detector.pt'\n",
    "sagittal_model_path_2 = '/Users/danipopov/Projects/RSNA2024/models/sagittal_t2_spine_detector.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_rois(rois_sagt1, rois_sagt2, rois_axialt2):\n",
    "    # Create figure with 3 rows (one for each plane)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot Sagittal T1 ROIs\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.suptitle('ROIs for Different Planes', fontsize=16, y=0.95)\n",
    "    for i, roi_path in enumerate(rois_sagt1):\n",
    "        plt.subplot(3, 5, i+1)\n",
    "        roi = cv2.imread(roi_path, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(roi, cmap='gray')\n",
    "        plt.title(f'L{i+1}/L{i+2}' if i < 4 else 'L5/S1', fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.subplot(3, 5, 3).set_title('Sagittal T1', fontsize=12, pad=20)\n",
    "    \n",
    "    # Plot Sagittal T2/STIR ROIs\n",
    "    for i, roi_path in enumerate(rois_sagt2):\n",
    "        plt.subplot(3, 5, i+6)\n",
    "        roi = cv2.imread(roi_path, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(roi, cmap='gray')\n",
    "        plt.title(f'L{i+1}/L{i+2}' if i < 4 else 'L5/S1', fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.subplot(3, 5, 8).set_title('Sagittal T2/STIR', fontsize=12, pad=20)\n",
    "    \n",
    "    # Plot Axial T2 ROIs\n",
    "    for i, roi_path in enumerate(rois_axialt2):\n",
    "        plt.subplot(3, 5, i+11)\n",
    "        roi = cv2.imread(roi_path, cv2.IMREAD_GRAYSCALE)\n",
    "        plt.imshow(roi, cmap='gray')\n",
    "        plt.title('Left' if i == 0 else 'Right', fontsize=10)\n",
    "        plt.axis('off')\n",
    "    plt.subplot(3, 5, 13).set_title('Axial T2', fontsize=12, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "show_rois(roi_dir_sag1, roi_dir_sag2, roi_dir_axial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the dataset, we can proceed to the next step: building our dataset class and DataLoader with PyTorch for subsequent model training. This will provide an efficient way to load and batch our preprocessed images during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpineDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing labels\n",
    "            image_dir: Root directory containing the ROI images\n",
    "            transform: Optional transforms to apply to images\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.series_descriptions = ['Axial T2', 'Sagittal T1', 'Sagittal T2_STIR']\n",
    "        # Get list of valid study IDs (directories in image_dir)\n",
    "        self.valid_studies = [d for d in os.listdir(image_dir) \n",
    "                            if os.path.isdir(os.path.join(image_dir, d))]\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.valid_studies)\n",
    "        \n",
    "    def load_series_rois(self, study_path, desc):\n",
    "        \"\"\"\n",
    "        Load ROIs for a specific series description\n",
    "        Returns: numpy array of ROIs [num_images * num_rois_per_image, H, W]\n",
    "        \"\"\"\n",
    "        series_path = os.path.join(study_path, desc)\n",
    "        all_rois = []\n",
    "        \n",
    "        # Get all image folders (img_1, img_2, etc.)\n",
    "        img_folders = sorted(glob.glob(os.path.join(series_path, 'img_*')))\n",
    "        \n",
    "        for img_folder in img_folders:\n",
    "            # Get all ROIs for this image\n",
    "            roi_paths = sorted(glob.glob(os.path.join(img_folder, '*.png')))\n",
    "            for roi_path in roi_paths:\n",
    "                roi = cv2.imread(roi_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if self.transform:\n",
    "                    roi = self.transform(image=roi)['image']\n",
    "                all_rois.append(roi)\n",
    "                \n",
    "        return np.stack(all_rois)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.valid_studies[idx]\n",
    "        study_path = os.path.join(self.image_dir, study_id)\n",
    "        \n",
    "        # Get labels for this study\n",
    "        labels = self.df[self.df['study_id'] == int(study_id)].iloc[0, 1:].values.astype(np.int64)\n",
    "        \n",
    "        # Load ROIs for each series\n",
    "        all_rois = []\n",
    "        for desc in self.series_descriptions:\n",
    "            series_rois = self.load_series_rois(study_path, desc)\n",
    "            all_rois.append(series_rois)\n",
    "            \n",
    "        # Stack all ROIs together\n",
    "        rois = np.concatenate(all_rois, axis=0)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        rois = torch.FloatTensor(rois)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        \n",
    "        return rois, labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't forget that our train_df contains missing values and categorical data. Since we'll be using cross-entropy loss for training our models, we need to convert these categorical values to integers to represent each label for each study_id. Additionally, we'll fill the missing values with -100, which is the default padding value for cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the training DataFrame\n",
    "train_df_copy = train_df.copy()\n",
    "\n",
    "# Print initial missing values statistics\n",
    "print(\"\\nMissing values before filling:\")\n",
    "print(train_df_copy.isna().sum())\n",
    "\n",
    "# Fill missing values with -100\n",
    "train_df_copy.fillna(-100, inplace=True)\n",
    "\n",
    "# Map categorical values to integers\n",
    "label_map = {'Normal/Mild': 1, 'Moderate': 2, 'Severe': 3}\n",
    "train_df_copy = train_df_copy.replace(label_map)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"\\nMissing values after filling:\")\n",
    "print(train_df_copy.isna().sum())\n",
    "\n",
    "# Display first few rows of processed DataFrame\n",
    "print(\"\\nFirst few rows of processed DataFrame:\")\n",
    "display(train_df_copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "train_transform = A.Compose([\n",
    "    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.75),\n",
    "    \n",
    "    A.OneOf([\n",
    "        A.MotionBlur(blur_limit=5),\n",
    "        A.MedianBlur(blur_limit=5),\n",
    "        A.GaussianBlur(blur_limit=5),\n",
    "    ], p=0.75),\n",
    "    \n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, border_mode=0, p=0.75),\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Normalize(mean=0.5, std=0.5),\n",
    "])\n",
    "\n",
    "dataset = SpineDataset(\n",
    "    df=train_df_copy,\n",
    "    image_dir='/Users/danipopov/Projects/RSNA2024/data/images_dataset',\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Test the dataloader\n",
    "images, labels = next(iter(dataloader))\n",
    "print(f\"Batch images shape: {images.shape}\")\n",
    "print(f\"Batch labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the first batch of our dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(images, labels):\n",
    "    \"\"\"\n",
    "    Display all ROIs from a batch\n",
    "    Args:\n",
    "        images: Tensor of shape [B, 60, 64, 64]\n",
    "        labels: Tensor of shape [B, 25]\n",
    "    \"\"\"\n",
    "    # Get first batch item [60, 64, 64]\n",
    "    images = images[0]  \n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    plt.suptitle('ROIs from Different Planes', fontsize=16)\n",
    "    \n",
    "    # Plot Axial T2 ROIs (first 10 images: 5 pairs of left/right)\n",
    "    for i in range(10):\n",
    "        plt.subplot(6, 10, i + 1)\n",
    "        plt.imshow(images[i].numpy(), cmap='gray')\n",
    "        plt.title(f'Axial: {\"Left\" if i%2==0 else \"Right\"}\\nImg {i//2+1}', fontsize=8)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Plot Sagittal T1 ROIs (next 25 images: 5 images √ó 5 levels)\n",
    "    for i in range(25):\n",
    "        plt.subplot(6, 10, i + 11)\n",
    "        plt.imshow(images[i+10].numpy(), cmap='gray')\n",
    "        level = f'L{i%5+1}/L{i%5+2}' if i%5 < 4 else 'L5/S1'\n",
    "        plt.title(f'Sag T1: {level}\\nImg {i//5+1}', fontsize=8)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    # Plot Sagittal T2 ROIs (last 25 images: 5 images √ó 5 levels)\n",
    "    for i in range(25):\n",
    "        plt.subplot(6, 10, i + 36)\n",
    "        plt.imshow(images[i+35].numpy(), cmap='gray')\n",
    "        level = f'L{i%5+1}/L{i%5+2}' if i%5 < 4 else 'L5/S1'\n",
    "        plt.title(f'Sag T2: {level}\\nImg {i//5+1}', fontsize=8)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print labels for the batch\n",
    "    print(\"Labels:\", labels[0])\n",
    "\n",
    "# Test the visualization\n",
    "images, labels = next(iter(dataloader))\n",
    "show_batch(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! üòÅ We can see that our DataLoader is working as expected, providing us with properly organized ROIs for each plane and their corresponding labels for each study_id.\n",
    "\n",
    "Now we can proceed to the next step: designing our model architecture. Given the nature of our data (60 ROIs of size 64x64 per study) and our classification task, I will use the ResNet50 architecture from timm library.\n",
    "\n",
    "timm (torch image models) library is a collection of state-of-the-art computer vision models for PyTorch from Hugging Face. It provides:\n",
    "- Pre-trained models optimized for various tasks\n",
    "- Easy-to-use interfaces for model customization\n",
    "- Consistent API across different architectures\n",
    "- Regular updates with new models and improvements\n",
    "\n",
    "ResNet50 is an excellent choice for our project because:\n",
    "1. **Powerful Architecture**:\n",
    "   - Deeper network with 50 layers\n",
    "   - Enhanced feature extraction capability\n",
    "   - Bottleneck blocks for efficient computation\n",
    "   - Strong skip connections to prevent vanishing gradients\n",
    "\n",
    "2. **Feature Extraction**:\n",
    "   - More sophisticated feature hierarchies\n",
    "   - Better at capturing complex patterns\n",
    "   - Proven success in medical imaging tasks\n",
    "   - Can handle our 64x64 ROI size efficiently\n",
    "\n",
    "3. **Practical Benefits**:\n",
    "   - Better performance than ResNet18\n",
    "   - Good balance of depth and computational cost\n",
    "   - Still manageable training time\n",
    "   - Extensive pretrained weights available\n",
    "   - Well-documented with strong community support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pretrained ResNet50 as starting point\n",
    "model = timm.create_model('resnet50.a1_in1k', pretrained=True)\n",
    "\n",
    "# Modify first conv layer to accept 60 channels\n",
    "model.conv1 = nn.Conv2d(60, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# Replace the final classification layer\n",
    "# 75 because we have 25 labels and for each label we will want to predict \n",
    "# the probability of the label being present or absent (25 * 3 = 75)\n",
    "model.fc = nn.Linear(model.fc.in_features, 75)\n",
    "\n",
    "# Initialize the new layers\n",
    "model.conv1.weight.data.normal_(0, 0.01)\n",
    "model.fc.bias.data.fill_(0)\n",
    "model.fc.weight.data.normal_(0, 0.01)\n",
    "\n",
    "# All layers will be trainable\n",
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluateion metrics\n",
    "\n",
    "Before we proceed to the training process, we will need to define what metrics we will use to evaluate our model's performance on this dataset.\n",
    "\n",
    "Metrics:\n",
    "1. **Accuracy**: Overall correctness of the model's predictions\n",
    "2. **Precision**: Proportion of true positives among all predicted positives\n",
    "3. **Recall**: Proportion of true positives that were correctly identified\n",
    "4. **F1-score**: Harmonic mean of precision and recall\n",
    "5. **AUC-ROC**: Area under the Receiver Operating Characteristic curve\n",
    "6. **Confusion Matrix**: Visualization of true vs. false positives and negatives for each class\n",
    "\n",
    "These metrics will help us comprehensively evaluate our model's performance across different aspects of classification accuracy and reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(preds, targets):\n",
    "    \"\"\"\n",
    "    Calculate accuracy, precision, recall, and F1 score for a single column\n",
    "    Args:\n",
    "        preds: tensor of shape (batch_size, 3) - logits for one column\n",
    "        targets: tensor of shape (batch_size,) - ground truth for one column\n",
    "    Returns:\n",
    "        accuracy, precision, recall, f1\n",
    "    \"\"\"\n",
    "    device = preds.device\n",
    "    preds = torch.argmax(preds, dim=1)  # Convert logits to predictions\n",
    "    correct = (preds == targets).float()\n",
    "    accuracy = correct.mean()\n",
    "    \n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for class_id in range(3):  # For each class (Normal, Mild, Severe)\n",
    "        true_positives = ((preds == class_id) & (targets == class_id)).float().sum()\n",
    "        predicted_positives = (preds == class_id).float().sum()\n",
    "        actual_positives = (targets == class_id).float().sum()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        class_precision = true_positives / predicted_positives if predicted_positives > 0 else torch.tensor(0.0, device=device)\n",
    "        class_recall = true_positives / actual_positives if actual_positives > 0 else torch.tensor(0.0, device=device)\n",
    "        class_f1 = 2 * (class_precision * class_recall) / (class_precision + class_recall) if (class_precision + class_recall) > 0 else torch.tensor(0.0, device=device)\n",
    "        \n",
    "        precision.append(class_precision)\n",
    "        recall.append(class_recall)\n",
    "        f1.append(class_f1)\n",
    "    \n",
    "    # Average metrics across classes\n",
    "    precision = torch.stack(precision).mean()\n",
    "    recall = torch.stack(recall).mean()\n",
    "    f1 = torch.stack(f1).mean()\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_heatmap(preds, targets, conditions, levels, title):\n",
    "    preds = preds.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "    \n",
    "    preds = preds.reshape(-1, 25, 3)\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "    \n",
    "    num_conditions = len(conditions)\n",
    "    num_levels = len(levels)\n",
    "    \n",
    "    heatmap_data = np.zeros((num_conditions, num_levels))\n",
    "    \n",
    "    for i in range(num_conditions):\n",
    "        for j in range(num_levels):\n",
    "            idx = i * num_levels + j\n",
    "            correct_predictions = (preds[:, idx] == targets[:, idx]).mean()\n",
    "            heatmap_data[i, j] = correct_predictions\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', xticklabels=levels, yticklabels=conditions, fmt='.3f')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Disk Levels')\n",
    "    plt.ylabel('Conditions')\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update MetricTracker to include all metrics\n",
    "class MetricTracker:\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': [],\n",
    "            'train_precision': [],\n",
    "            'val_precision': [],\n",
    "            'train_recall': [],\n",
    "            'val_recall': [],\n",
    "            'train_f1': [],\n",
    "            'val_f1': [],\n",
    "            'train_auc': [],\n",
    "            'val_auc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "    \n",
    "    def update(self, metric_name, value):\n",
    "        self.metrics[metric_name].append(value)\n",
    "    \n",
    "    def save(self, fold, save_path):\n",
    "        np.save(f'{save_path}/metrics_fold_{fold}.npy', self.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(fold_num, plot_path=None):\n",
    "    \"\"\"Plot and save all training metrics for a given fold\"\"\"\n",
    "    if plot_path is None:\n",
    "        plot_path = '/Users/danipopov/Projects/RSNA2024/plots/training_metrics'\n",
    "    os.makedirs(plot_path, exist_ok=True)\n",
    "    \n",
    "    metrics = np.load(f'/Users/danipopov/Projects/RSNA2024/metrics/metrics_fold_{fold_num}.npy', \n",
    "                     allow_pickle=True).item()\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0,0].plot(metrics['train_loss'], label='Train')\n",
    "    axes[0,0].plot(metrics['val_loss'], label='Validation')\n",
    "    axes[0,0].set_title('Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0,1].plot(metrics['train_acc'], label='Train')\n",
    "    axes[0,1].plot(metrics['val_acc'], label='Validation')\n",
    "    axes[0,1].set_title('Accuracy')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    # Precision plot\n",
    "    axes[1,0].plot(metrics['train_precision'], label='Train')\n",
    "    axes[1,0].plot(metrics['val_precision'], label='Validation')\n",
    "    axes[1,0].set_title('Precision')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True)\n",
    "    \n",
    "    # Recall plot\n",
    "    axes[1,1].plot(metrics['train_recall'], label='Train')\n",
    "    axes[1,1].plot(metrics['val_recall'], label='Validation')\n",
    "    axes[1,1].set_title('Recall')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    # F1 Score plot\n",
    "    axes[2,0].plot(metrics['train_f1'], label='Train')\n",
    "    axes[2,0].plot(metrics['val_f1'], label='Validation')\n",
    "    axes[2,0].set_title('F1 Score')\n",
    "    axes[2,0].set_xlabel('Epoch')\n",
    "    axes[2,0].legend()\n",
    "    axes[2,0].grid(True)\n",
    "    \n",
    "    # Learning Rate plot\n",
    "    axes[2,1].plot(metrics['learning_rates'], label='Learning Rate')\n",
    "    axes[2,1].set_title('Learning Rate')\n",
    "    axes[2,1].set_xlabel('Epoch')\n",
    "    axes[2,1].legend()\n",
    "    axes[2,1].grid(True)\n",
    "    \n",
    "    plt.suptitle(f'Training Metrics - Fold {fold_num}', y=1.02, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'{plot_path}/training_metrics_fold_{fold_num}.png', \n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fold_visualizations(fold_num, base_path=None):\n",
    "    \"\"\"Generate and save all visualizations for a specific fold\"\"\"\n",
    "    if base_path is None:\n",
    "        base_path = '/Users/danipopov/Projects/RSNA2024/plots'\n",
    "    \n",
    "    # Load predictions\n",
    "    pred_path = f'/Users/danipopov/Projects/RSNA2024/models/predictions/fold_{fold_num}_predictions.pt'\n",
    "    predictions = torch.load(pred_path)\n",
    "    outputs = predictions['outputs']\n",
    "    labels = predictions['labels']\n",
    "    \n",
    "    # Create directories\n",
    "    heatmap_path = f'{base_path}/heatmaps'\n",
    "    roc_path = f'{base_path}/roc_curves'\n",
    "    os.makedirs(heatmap_path, exist_ok=True)\n",
    "    os.makedirs(roc_path, exist_ok=True)\n",
    "    \n",
    "    # Generate and save heatmap\n",
    "    conditions = [\n",
    "        'Spinal Canal Stenosis',\n",
    "        'Left Neural Foraminal Narrowing',\n",
    "        'Right Neural Foraminal Narrowing',\n",
    "        'Left Subarticular Stenosis',\n",
    "        'Right Subarticular Stenosis'\n",
    "    ]\n",
    "    levels = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']\n",
    "    \n",
    "    heatmap_fig = generate_heatmap(\n",
    "        preds=outputs,\n",
    "        targets=labels,\n",
    "        conditions=conditions,\n",
    "        levels=levels,\n",
    "        title=f'Fold {fold_num} Performance Heatmap'\n",
    "    )\n",
    "    heatmap_fig.savefig(f'{heatmap_path}/heatmap_fold_{fold_num}.png')\n",
    "    plt.close(heatmap_fig)\n",
    "    \n",
    "    # Generate ROC curves for each condition\n",
    "    class_names = ['Normal', 'Mild', 'Severe']\n",
    "    for i, condition in enumerate(conditions):\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for severity in range(3):\n",
    "            start_idx = i * 3\n",
    "            probs = torch.softmax(outputs[:, start_idx:start_idx+3], dim=1)\n",
    "            true_labels = labels[:, i]\n",
    "            \n",
    "            # Calculate ROC curve\n",
    "            from sklearn.metrics import roc_curve, auc\n",
    "            fpr, tpr, _ = roc_curve(\n",
    "                (true_labels == severity).cpu().numpy(),\n",
    "                probs[:, severity].cpu().numpy()\n",
    "            )\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            plt.plot(\n",
    "                fpr, \n",
    "                tpr, \n",
    "                label=f'{class_names[severity]} (AUC = {roc_auc:.2f})'\n",
    "            )\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves for {condition} - Fold {fold_num}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'{roc_path}/roc_curves_{condition.replace(\" \", \"_\")}_fold_{fold_num}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fold_results(fold_num, base_path=None):\n",
    "    \"\"\"Generate and save all visualizations for a specific fold\"\"\"\n",
    "    if base_path is None:\n",
    "        base_path = '/Users/danipopov/Projects/RSNA2024/plots'\n",
    "    \n",
    "    # Create base directory\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    # Generate training metrics plots\n",
    "    metrics_path = f'{base_path}/training_metrics'\n",
    "    os.makedirs(metrics_path, exist_ok=True)\n",
    "    plot_training_metrics(fold_num, metrics_path)\n",
    "    \n",
    "    # Generate heatmap and ROC curves\n",
    "    generate_fold_visualizations(fold_num, base_path)\n",
    "    \n",
    "    print(f\"All visualizations for fold {fold_num} have been saved to {base_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=15, min_delta=0, verbose=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_df, image_dir, train_transform, test_transform, N_SPLITS, EPOCHS, device):\n",
    "    \"\"\"\n",
    "    Train the model using N-fold cross validation with improved training loop\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_df: DataFrame with labels\n",
    "        image_dir: Root directory containing all study folders\n",
    "        transform: Albumentations transforms\n",
    "        N_SPLITS: Number of folds for cross validation\n",
    "        EPOCHS: Number of training epochs\n",
    "        device: torch device\n",
    "    \"\"\"\n",
    "    fold_results = []\n",
    "    \n",
    "    # Get all valid study IDs\n",
    "    valid_studies = [d for d in os.listdir(image_dir) \n",
    "                    if os.path.isdir(os.path.join(image_dir, d))]\n",
    "    \n",
    "    weights = torch.tensor([1.0, 2.0, 4.0]).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights).to(device)\n",
    "    \n",
    "    # Setup KFold cross validation\n",
    "    skf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(valid_studies)):\n",
    "        print('=' * 50)\n",
    "        print(f\"Training Fold {fold+1}/{N_SPLITS}\")\n",
    "        print('=' * 50)\n",
    "\n",
    "        # Initialize metric tracker for this fold\n",
    "        tracker = MetricTracker()\n",
    "\n",
    "        # Create datasets\n",
    "        train_dataset = SpineDataset(\n",
    "            df=train_df,\n",
    "            image_dir=image_dir,  \n",
    "            transform= train_transform\n",
    "        )\n",
    "        val_dataset = SpineDataset(\n",
    "            df=train_df,\n",
    "            image_dir=image_dir,\n",
    "            transform=test_transform  \n",
    "        )\n",
    "        \n",
    "        # Set train/val splits\n",
    "        train_dataset.valid_studies = [valid_studies[i] for i in train_idx]\n",
    "        val_dataset.valid_studies = [valid_studies[i] for i in val_idx]\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=16,\n",
    "            shuffle=True, \n",
    "            num_workers=0, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=16, \n",
    "            shuffle=False,\n",
    "            num_workers=0, \n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Initialize model for this fold\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialize new layers properly\n",
    "        def init_weights(m):\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        model.conv1.apply(init_weights)\n",
    "        model.fc.apply(init_weights)\n",
    "        \n",
    "        # Optimizer with lower learning rate\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=0.0001,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Cosine annealing scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=10,\n",
    "            T_mult=2\n",
    "        )\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping = EarlyStopping(\n",
    "            patience=15,\n",
    "            min_delta=1e-4,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        scaler = torch.cuda.amp.GradScaler()  # For mixed precision training\n",
    "        \n",
    "        # Inside the training function, modify the progress bar sections:\n",
    "\n",
    "        # Training phase progress bar\n",
    "        for epoch in range(EPOCHS):\n",
    "            print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "            \n",
    "            # Training phase\n",
    "            model.train()\n",
    "            total_train_loss = 0\n",
    "            train_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}\n",
    "            train_steps = len(train_loader)\n",
    "\n",
    "            # Modified progress bar\n",
    "            with tqdm(total=train_steps, desc=f'Training Epoch {epoch+1}', \n",
    "                    bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') as train_pbar:\n",
    "                \n",
    "                for idx, (images, labels) in enumerate(train_loader):\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = 0\n",
    "                        batch_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}\n",
    "\n",
    "                        for col in range(labels.shape[1]):\n",
    "                            pred = outputs[:, col*3: col*3+3]\n",
    "                            ground_truth = labels[:, col]\n",
    "                            loss += criterion(pred, ground_truth)\n",
    "                        \n",
    "                            # Calculate metrics for this column\n",
    "                            acc, prec, rec, f1 = calculate_metrics(pred, ground_truth)\n",
    "                            batch_metrics['accuracy'] += acc.item()\n",
    "                            batch_metrics['precision'] += prec.item()\n",
    "                            batch_metrics['recall'] += rec.item()\n",
    "                            batch_metrics['f1'] += f1.item()\n",
    "                        \n",
    "                        # Average metrics across columns\n",
    "                        for k in batch_metrics:\n",
    "                            batch_metrics[k] /= labels.shape[1]\n",
    "                            train_metrics[k] += batch_metrics[k]\n",
    "\n",
    "                        loss /= labels.shape[1]\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    total_train_loss += loss.item()\n",
    "                    train_pbar.set_postfix({\n",
    "                        'loss': f'{loss.item():.4f}',\n",
    "                        'acc': f'{batch_metrics[\"accuracy\"]:.4f}'\n",
    "                    })\n",
    "                    train_pbar.update(1)\n",
    "            \n",
    "            # Average training metrics\n",
    "            avg_train_loss = total_train_loss / train_steps\n",
    "            for k in train_metrics:\n",
    "                train_metrics[k] /= train_steps\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            total_val_loss = 0\n",
    "            val_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}\n",
    "            val_steps = len(val_loader)\n",
    "            \n",
    "            all_val_outputs = []\n",
    "            all_val_labels = []\n",
    "\n",
    "            # Modified validation progress bar\n",
    "            with tqdm(total=val_steps, desc=f'Validation Epoch {epoch+1}', \n",
    "                    bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}') as val_pbar:\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for idx, (images, labels) in enumerate(val_loader):\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(images)\n",
    "                            loss = 0\n",
    "                            batch_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}\n",
    "                            \n",
    "                            for col in range(labels.shape[1]):\n",
    "                                pred = outputs[:, col*3: col*3+3]\n",
    "                                ground_truth = labels[:, col]\n",
    "                                loss += criterion(pred, ground_truth)\n",
    "                            \n",
    "                                # Calculate metrics for this column\n",
    "                                acc, prec, rec, f1 = calculate_metrics(pred, ground_truth)\n",
    "                                batch_metrics['accuracy'] += acc.item()\n",
    "                                batch_metrics['precision'] += prec.item()\n",
    "                                batch_metrics['recall'] += rec.item()\n",
    "                                batch_metrics['f1'] += f1.item()\n",
    "                            \n",
    "                            for k in batch_metrics:\n",
    "                                batch_metrics[k] /= labels.shape[1]\n",
    "                                val_metrics[k] += batch_metrics[k]\n",
    "\n",
    "                            loss /= labels.shape[1]\n",
    "                            total_val_loss += loss.item()\n",
    "                        \n",
    "                        # Store predictions and labels for ROC-AUC and heatmap\n",
    "                        all_val_outputs.append(outputs)\n",
    "                        all_val_labels.append(labels)\n",
    "\n",
    "                        val_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                        val_pbar.update(1)\n",
    "            \n",
    "            avg_val_loss = total_val_loss / val_steps\n",
    "            val_pbar.close()\n",
    "            \n",
    "            # Average validation metrics\n",
    "            avg_val_loss = total_val_loss / val_steps\n",
    "            for k in val_metrics:\n",
    "                val_metrics[k] /= val_steps\n",
    "\n",
    "            # Update metric tracker\n",
    "            tracker.update('train_loss', avg_train_loss)\n",
    "            tracker.update('val_loss', avg_val_loss)\n",
    "            tracker.update('train_acc', train_metrics['accuracy'])\n",
    "            tracker.update('val_acc', val_metrics['accuracy'])\n",
    "            tracker.update('train_precision', train_metrics['precision'])\n",
    "            tracker.update('val_precision', val_metrics['precision'])\n",
    "            tracker.update('train_recall', train_metrics['recall'])\n",
    "            tracker.update('val_recall', val_metrics['recall'])\n",
    "            tracker.update('train_f1', train_metrics['f1'])\n",
    "            tracker.update('val_f1', val_metrics['f1'])\n",
    "            tracker.update('learning_rates', current_lr)\n",
    "\n",
    "            # Learning rate scheduling (move this before metric tracking)\n",
    "            scheduler.step()\n",
    "            current_lr = optimizer.param_groups[0]['lr']  # Update current_lr\n",
    "\n",
    "            # Save metrics\n",
    "            save_path = '/Users/danipopov/Projects/RSNA2024/metrics'\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            tracker.save(fold+1, save_path)\n",
    "            \n",
    "            # Generate and save heatmap at end of epoch\n",
    "            if epoch == EPOCHS - 1 or early_stopping.early_stop:\n",
    "                all_outputs = torch.cat(all_val_outputs)\n",
    "                all_labels = torch.cat(all_val_labels)\n",
    "                \n",
    "                # Save predictions and labels for later visualization\n",
    "                save_path = f'/Users/danipopov/Projects/RSNA2024/models/predictions'\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save({\n",
    "                    'outputs': all_outputs,\n",
    "                    'labels': all_labels\n",
    "                }, f'{save_path}/fold_{fold+1}_predictions.pt')                \n",
    "            \n",
    "            # Print epoch results\n",
    "            print(f'\\nEpoch {epoch+1}:')\n",
    "            print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "            print(f'Train Acc: {train_metrics[\"accuracy\"]:.4f}, Val Acc: {val_metrics[\"accuracy\"]:.4f}')\n",
    "            print(f'Train Precision: {train_metrics[\"precision\"]:.4f}, Val Precision: {val_metrics[\"precision\"]:.4f}')\n",
    "            print(f'Train Recall: {train_metrics[\"recall\"]:.4f}, Val Recall: {val_metrics[\"recall\"]:.4f}')\n",
    "            print(f'Train F1: {train_metrics[\"f1\"]:.4f}, Val F1: {val_metrics[\"f1\"]:.4f}')\n",
    "            print(f'Learning Rate: {current_lr:.6f}')\n",
    "             \n",
    "            # Early stopping check\n",
    "            early_stopping(avg_val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "            # Save best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_val_loss,\n",
    "                    'metrics': {\n",
    "                        'train_metrics': train_metrics,\n",
    "                        'val_metrics': val_metrics\n",
    "                    },\n",
    "                }, f'/Users/danipopov/Projects/RSNA2024/models/fold_models/best_model_fold_{fold+1}.pth')\n",
    "                print(f'Saved new best model with validation loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        # Save final metrics for this fold\n",
    "        save_path = '/Users/danipopov/Projects/RSNA2024/metrics'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        tracker.save(fold+1, save_path)\n",
    "        \n",
    "        fold_results.append(best_val_loss)\n",
    "        print(f'Fold {fold+1} Best Loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        # Print final metrics for this fold\n",
    "        print(f\"\\nFinal Metrics for Fold {fold+1}:\")\n",
    "        print(f\"Validation Accuracy: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Validation Precision: {val_metrics['precision']:.4f}\")\n",
    "        print(f\"Validation Recall: {val_metrics['recall']:.4f}\")\n",
    "        print(f\"Validation F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    print('\\nTraining completed!')\n",
    "    print(f'Average Best Loss across folds: {sum(fold_results)/len(fold_results):.4f}')\n",
    "    \n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results = train(\n",
    "    model=model,\n",
    "    train_df=train_df_copy,\n",
    "    image_dir='/Users/danipopov/Projects/RSNA2024/data/images_dataset',\n",
    "    train_transform=train_transform,\n",
    "    test_transform=test_transform,\n",
    "    N_SPLITS=3,\n",
    "    EPOCHS=25,\n",
    "    device='mps'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate visualizations for each fold\n",
    "for fold in range(3):\n",
    "    visualize_fold_results(fold + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results üîç üìàüìä\n",
    "\n",
    "Great! Now that we've built and trained our model, let's evaluate its performance on the test dataset.\n",
    "\n",
    "Before we proceed with testing, I want to address how we handled the data imbalance problem. We implemented two key strategies:\n",
    "1. Used a weighted loss function to give more importance to underrepresented classes\n",
    "2. Applied k-fold cross validation to make the model more robust and reduce overfitting\n",
    "\n",
    "For comprehensive analysis, we saved several metrics during training:\n",
    "1. Training and validation metrics for each epoch\n",
    "2. Heatmaps showing performance across different spinal conditions and levels\n",
    "3. ROC curves analyzing performance for each spinal degenerative condition\n",
    "\n",
    "Let's examine these results in detail for each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_result_image(image_path, title, figsize=(15, 15)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_fold_roc_curves(fold_number, image_paths, figsize=(14, 10)):\n",
    "    n_images = len(image_paths)\n",
    "    rows = (n_images + 2) // 3  \n",
    "    cols = min(3, n_images)     \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    fig.suptitle(f'ROC Curves for Fold {fold_number}', fontsize=16)\n",
    "    \n",
    "    # Flatten axes array if we have multiple rows\n",
    "    if rows > 1:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Plot each ROC curve\n",
    "    for idx, img_path in enumerate(image_paths):\n",
    "        if Path(img_path).exists():\n",
    "            img = mpimg.imread(img_path)\n",
    "            if rows == 1:\n",
    "                ax = axes[idx] if cols > 1 else axes\n",
    "            else:\n",
    "                ax = axes[idx]\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Condition {idx+1}')\n",
    "    \n",
    "    # Remove empty subplots if any\n",
    "    if rows > 1:\n",
    "        for idx in range(len(image_paths), len(axes)):\n",
    "            fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fold 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training metrics subplot\n",
    "display_result_image('plots/training_metrics/training_metrics_fold_1.png', 'Training Metrics for Fold 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Metrics Analysis - Fold 1 üìà\n",
    "\n",
    "The training results show several important trends across 25 epochs:\n",
    "\n",
    "Loss\n",
    "- The model shows strong convergence with the loss decreasing significantly in the first 5 epochs\n",
    "- Training and validation losses align well, starting from ~0.6 and stabilizing around 0.42\n",
    "- The close alignment between training and validation loss suggests no significant overfitting\n",
    "\n",
    "Accuracy\n",
    "- Validation accuracy quickly reaches ~77% and remains stable\n",
    "- Training accuracy gradually improves to match validation accuracy\n",
    "- Final accuracy for both training and validation converges at approximately 77%\n",
    "\n",
    "Precision and Recall\n",
    "- Precision improves from 31% to 38% over the training period\n",
    "- Recall shows similar improvement, reaching approximately 41%\n",
    "- Both metrics show steady improvement without significant oscillation\n",
    "- The small gap between training and validation suggests good generalization\n",
    "\n",
    "F1 Score\n",
    "- F1 score, which balances precision and recall, improves from 31% to 38%\n",
    "- The validation F1 score closely follows the training curve\n",
    "- The steady increase indicates consistent improvement in overall model performance\n",
    "\n",
    "Learning Rate\n",
    "- Implements a cyclical learning rate strategy\n",
    "- Peaks at 0.0001 with controlled decreases\n",
    "- The cycling pattern helps avoid local minima and promotes better convergence\n",
    "\n",
    "Overall Assessment\n",
    "The model demonstrates stable training with good convergence and no significant overfitting. While the accuracy metrics are promising at 77%, the precision and recall metrics (around 38-41%) suggest room for improvement in handling class imbalance. The close alignment between training and validation metrics across all measures indicates good generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For heatmap subplot\n",
    "display_result_image('plots/heatmaps/heatmap_fold_1.png', 'Heatmap for Fold 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Heatmap Analysis - Fold 1 üìä\n",
    "\n",
    "The heatmap visualizes the model's performance across different spinal conditions and disk levels, revealing several key insights:\n",
    "\n",
    "Spinal Canal Stenosis\n",
    "- Shows consistently strong performance across all disk levels\n",
    "- Highest accuracy at L5/S1 (0.977) and L1/L2 (0.956)\n",
    "- Even at its lowest performance (L4/L5: 0.768), maintains good reliability\n",
    "- Overall best-performing condition among all pathologies\n",
    "\n",
    "Neural Foraminal Narrowing (Left & Right)\n",
    "- Both sides show similar performance patterns\n",
    "- Excellent performance in upper spine levels (L1/L2: ~0.967)\n",
    "- Gradual decrease in accuracy moving down the spine\n",
    "- Lower performance at L5/S1 (Left: 0.612, Right: 0.627)\n",
    "- Right side slightly outperforms left side in most levels\n",
    "\n",
    "Subarticular Stenosis (Left & Right)\n",
    "- Shows moderate to good performance\n",
    "- Strongest at upper levels (L1/L2: ~0.83)\n",
    "- Lowest performance at L4/L5 (Left: 0.550, Right: 0.522)\n",
    "- Slight improvement at L5/S1 compared to L4/L5\n",
    "- Generally lower accuracy compared to other conditions\n",
    "\n",
    "General Observations\n",
    "1. Performance tends to be strongest at upper spine levels (L1/L2, L2/L3)\n",
    "2. Most conditions show decreased accuracy at L4/L5\n",
    "3. Bilateral conditions (left/right) show similar patterns\n",
    "4. L4/L5 level consistently shows the lowest performance across all conditions\n",
    "5. Model performs best with Spinal Canal Stenosis and struggles more with Subarticular Stenosis\n",
    "\n",
    "This pattern suggests that the model is more reliable for central spinal conditions compared to lateral pathologies, and performs better in upper spinal regions compared to lower ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ROC curves subplot\n",
    "roc_paths = [\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Left_Neural_Foraminal_Narrowing_fold_1.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Left_Subarticular_Stenosis_fold_1.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Right_Neural_Foraminal_Narrowing_fold_1.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Right_Subarticular_Stenosis_fold_1.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Spinal_Canal_Stenosis_fold_1.png'\n",
    "]\n",
    "display_fold_roc_curves(1, roc_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve Analysis - Fold 1üìä\n",
    "\n",
    "The ROC curves demonstrate the model's ability to distinguish between different severity levels across various spinal conditions. Here's a detailed analysis:\n",
    "\n",
    "Left Neural Foraminal Narrowing\n",
    "- Excellent performance with high AUC scores\n",
    "- Mild cases: AUC = 0.92 (best performing)\n",
    "- Severe cases: AUC = 0.90\n",
    "- Shows strong early detection capability with steep initial curve rise\n",
    "\n",
    "Right Neural Foraminal Narrowing\n",
    "- Similar but slightly lower performance compared to left side\n",
    "- Mild cases: AUC = 0.89\n",
    "- Severe cases: AUC = 0.84\n",
    "- Good discrimination ability for both severity levels\n",
    "\n",
    "Left Subarticular Stenosis\n",
    "- Good performance with balanced detection\n",
    "- Mild cases: AUC = 0.88\n",
    "- Severe cases: AUC = 0.80\n",
    "- Shows better detection of mild cases compared to severe\n",
    "\n",
    "Right Subarticular Stenosis\n",
    "- Lowest performing among all conditions\n",
    "- Mild cases: AUC = 0.80\n",
    "- Severe cases: AUC = 0.77\n",
    "- More gradual curve progression indicating lower confidence in predictions\n",
    "\n",
    "Spinal Canal Stenosis\n",
    "- Strong consistent performance\n",
    "- Mild cases: AUC = 0.89\n",
    "- Severe cases: AUC = 0.88\n",
    "- Nearly identical performance for both severity levels\n",
    "- Smooth curve progression indicating stable predictions\n",
    "\n",
    "Key Observations\n",
    "1. All conditions show AUC scores above 0.77, indicating good to excellent classification performance\n",
    "2. Left-sided conditions generally perform better than right-sided ones\n",
    "3. Neural Foraminal Narrowing shows the best overall performance\n",
    "4. Mild cases are generally detected with higher accuracy than severe cases\n",
    "5. Subarticular Stenosis shows the most room for improvement, particularly on the right side\n",
    "\n",
    "The model demonstrates robust classification ability across all conditions, with particularly strong performance in detecting Neural Foraminal Narrowing. The consistently high AUC scores suggest reliable clinical applicability, though there's some variation in performance between different conditions and severity levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fold 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training metrics subplot\n",
    "display_result_image('plots/training_metrics/training_metrics_fold_2.png', 'Training Metrics for Fold 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Metrics Analysis - Fold 2 üìà\n",
    "\n",
    "The training results show several important trends across 25 epochs:\n",
    "\n",
    "Loss\n",
    "- Similar to Fold 1, strong initial convergence in first 5 epochs\n",
    "- Training and validation losses start from ~0.6 and stabilize around 0.40-0.42\n",
    "- Slight divergence between training and validation loss after epoch 15, but difference remains small\n",
    "- Training loss continues to decrease gradually while validation loss plateaus\n",
    "\n",
    "Accuracy\n",
    "- Validation accuracy reaches ~77% quickly and shows more fluctuation than Fold 1\n",
    "- Training accuracy improves steadily to ~77%\n",
    "- Final accuracy converges around 77% for both, matching Fold 1's performance\n",
    "- More variance in validation accuracy compared to Fold 1\n",
    "\n",
    "Precision and Recall\n",
    "- Precision improves from 30% to 38% over the training period\n",
    "- Recall increases from 33% to 41-42%\n",
    "- More fluctuation in validation metrics compared to Fold 1\n",
    "- Training metrics show steadier improvement than validation\n",
    "- Final gap between training and validation slightly larger than in Fold 1\n",
    "\n",
    "F1 Score\n",
    "- F1 score improves from 30% to 38%\n",
    "- More variance in validation F1 score compared to Fold 1\n",
    "- Training F1 score shows steady improvement\n",
    "- Final performance matches Fold 1 despite more fluctuation\n",
    "\n",
    "Learning Rate\n",
    "- Identical cyclical learning rate strategy as Fold 1\n",
    "- Peaks at 0.0001 with controlled decreases\n",
    "- Maintains consistent learning pattern across folds\n",
    "\n",
    "Overall Assessment\n",
    "Fold 2 shows similar overall performance to Fold 1 but with more fluctuation in validation metrics. The model still demonstrates good convergence, reaching comparable final metrics (77% accuracy, 38% precision/F1, 41% recall). The increased variance in validation metrics suggests this fold might have encountered more challenging examples, but the final performance remains stable. The slight divergence between training and validation metrics after epoch 15 bears monitoring but doesn't indicate severe overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For heatmap subplot\n",
    "display_result_image('plots/heatmaps/heatmap_fold_2.png', 'Heatmap for Fold 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Heatmap Analysis - Fold 2 üìä\n",
    "\n",
    "The heatmap visualizes the model's performance across different spinal conditions and disk levels, revealing several key insights:\n",
    "\n",
    "Spinal Canal Stenosis\n",
    "- Maintains excellent performance across all disk levels\n",
    "- Highest accuracy at L1/L2 (0.957) and L5/S1 (0.967)\n",
    "- Lowest at L4/L5 (0.720), but still acceptable performance\n",
    "- Shows very similar pattern to Fold 1, confirming consistency\n",
    "- Remains the most reliable condition for detection\n",
    "\n",
    "Neural Foraminal Narrowing (Left & Right)\n",
    "- Both sides demonstrate strong performance patterns\n",
    "- Left side shows exceptional performance at upper levels (L1/L2: 0.977)\n",
    "- Right side slightly lower but still excellent (L1/L2: 0.952)\n",
    "- Performance decreases in lower spine levels\n",
    "- L5/S1 performance (Left: 0.613, Right: 0.638) matches Fold 1 pattern\n",
    "- More balanced performance between left and right compared to Fold 1\n",
    "\n",
    "Subarticular Stenosis (Left & Right)\n",
    "- Good performance in upper spine levels\n",
    "- Left side peaks at L1/L2 (0.863)\n",
    "- Right side shows similar pattern (L1/L2: 0.849)\n",
    "- Significant drop at L4/L5 (Left: 0.464, Right: 0.484)\n",
    "- Recovers slightly at L5/S1 (Left: 0.673, Right: 0.688)\n",
    "- Performance slightly better than Fold 1 for most levels\n",
    "\n",
    "General Observations\n",
    "1. Upper spine levels (L1/L2, L2/L3) consistently show strongest performance\n",
    "2. L4/L5 remains the most challenging level across all conditions\n",
    "3. Bilateral conditions show more balanced performance than in Fold 1\n",
    "4. Overall pattern matches Fold 1, suggesting model stability\n",
    "5. Slight improvement in Subarticular Stenosis detection compared to Fold 1\n",
    "\n",
    "This fold confirms the model's strengths and challenges seen in Fold 1, with marginally better performance in some areas. The consistency between folds suggests the model has learned robust features for diagnosis, particularly for central spinal conditions and upper spinal regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ROC curves subplot\n",
    "roc_paths = [\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Left_Neural_Foraminal_Narrowing_fold_2.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Left_Subarticular_Stenosis_fold_2.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Right_Neural_Foraminal_Narrowing_fold_2.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Right_Subarticular_Stenosis_fold_2.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Spinal_Canal_Stenosis_fold_2.png'\n",
    "]\n",
    "display_fold_roc_curves(2, roc_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve Analysis - Fold 2 üìä\n",
    "\n",
    "The ROC curves demonstrate the model's performance across different conditions and severity levels, showing some variations from Fold 1:\n",
    "\n",
    "Left Neural Foraminal Narrowing\n",
    "- Outstanding performance with improved AUC scores\n",
    "- Mild cases: AUC = 0.93 (slight improvement from Fold 1)\n",
    "- Severe cases: AUC = 0.91 (better than Fold 1)\n",
    "- Very steep initial curve rise indicating excellent early detection\n",
    "- More consistent performance between mild and severe cases\n",
    "\n",
    "Right Neural Foraminal Narrowing\n",
    "- Strong performance with slight improvement\n",
    "- Mild cases: AUC = 0.90 (better than Fold 1)\n",
    "- Severe cases: AUC = 0.83 (similar to Fold 1)\n",
    "- Clear separation between mild and severe detection capabilities\n",
    "- Good early detection rate for mild cases\n",
    "\n",
    "Left Subarticular Stenosis\n",
    "- Moderate to good performance\n",
    "- Mild cases: AUC = 0.85 (slightly lower than Fold 1)\n",
    "- Severe cases: AUC = 0.72 (lower than Fold 1)\n",
    "- Larger gap between mild and severe detection\n",
    "- More gradual curve progression compared to Neural Foraminal Narrowing\n",
    "\n",
    "Right Subarticular Stenosis\n",
    "- Shows similar challenges to Fold 1\n",
    "- Mild cases: AUC = 0.73 (lower than Fold 1)\n",
    "- Severe cases: AUC = 0.77 (matching Fold 1)\n",
    "- More stepwise progression in the curves\n",
    "- Unusual pattern where severe cases perform better than mild\n",
    "\n",
    "Spinal Canal Stenosis\n",
    "- Excellent and improved performance\n",
    "- Mild cases: AUC = 0.92 (better than Fold 1)\n",
    "- Severe cases: AUC = 0.93 (significant improvement)\n",
    "- Very consistent performance between severity levels\n",
    "- Sharp initial rise indicating strong detection confidence\n",
    "\n",
    "Key Observations\n",
    "1. Overall AUC scores range from 0.72 to 0.93, showing good to excellent performance\n",
    "2. Neural conditions show more consistent performance across folds\n",
    "3. Spinal Canal Stenosis shows marked improvement in Fold 2\n",
    "4. Subarticular Stenosis remains the most challenging condition\n",
    "5. Better balance between mild and severe cases in most conditions\n",
    "\n",
    "The model maintains its strong performance in Fold 2, with some conditions showing improvement while others remain challenging. The consistency in Neural Foraminal Narrowing and Spinal Canal Stenosis detection across folds suggests robust learning for these conditions. The variation in Subarticular Stenosis performance between folds indicates this remains an area for potential improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fold 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training metrics subplot\n",
    "display_result_image('plots/training_metrics/training_metrics_fold_3.png', 'Training Metrics for Fold 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Metrics Analysis - Fold 3 üìà\n",
    "\n",
    "The training results show several important trends across 25 epochs:\n",
    "\n",
    "Loss\n",
    "- Strong initial convergence in first 5 epochs, similar to previous folds\n",
    "- Training loss starts at ~0.575 and stabilizes around 0.39\n",
    "- Validation loss plateaus around 0.43\n",
    "- Slight divergence between training and validation loss after epoch 15\n",
    "- More stable validation loss compared to Fold 2\n",
    "\n",
    "Accuracy\n",
    "- Both training and validation accuracy reach ~77%\n",
    "- More fluctuation in validation accuracy compared to previous folds\n",
    "- Training accuracy shows steady improvement, reaching 77.8%\n",
    "- Validation accuracy shows more variance but maintains good performance\n",
    "- Final convergence similar to Folds 1 and 2\n",
    "\n",
    "Precision and Recall\n",
    "- Precision improves from 32% to 39.5% for training\n",
    "- Validation precision stabilizes around 38%\n",
    "- Recall shows improvement from 35% to 43% for training\n",
    "- Validation recall maintains around 40-41%\n",
    "- Larger gap between training and validation metrics in later epochs\n",
    "\n",
    "F1 Score\n",
    "- Training F1 score shows steady improvement from 32% to 40%\n",
    "- Validation F1 score stabilizes around 38%\n",
    "- More pronounced divergence between training and validation after epoch 15\n",
    "- Final performance comparable to previous folds\n",
    "\n",
    "Learning Rate\n",
    "- Maintains same cyclical learning rate strategy as previous folds\n",
    "- Peaks at 0.0001 with controlled decreases\n",
    "- Consistent pattern across all folds\n",
    "\n",
    "Overall Assessment\n",
    "Fold 3 shows similar overall performance to previous folds but with some notable differences:\n",
    "1. Slightly higher final training metrics\n",
    "2. More pronounced gap between training and validation performance\n",
    "3. More fluctuation in validation metrics\n",
    "4. Strong initial convergence maintained\n",
    "5. Potential signs of mild overfitting in later epochs\n",
    "\n",
    "While the model maintains good performance, the increased divergence between training and validation metrics suggests this fold might benefit from additional regularization or earlier stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For heatmap subplot\n",
    "display_result_image('plots/heatmaps/heatmap_fold_3.png', 'Heatmap for Fold 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Heatmap Analysis - Fold 3 üìä\n",
    "\n",
    "The heatmap for Fold 3 reveals performance patterns across spinal conditions and disk levels:\n",
    "\n",
    "Spinal Canal Stenosis\n",
    "- Maintains excellent performance consistent with previous folds\n",
    "- Highest accuracy at L1/L2 (0.961) and L5/S1 (0.965)\n",
    "- Notable drop at L4/L5 (0.701), the lowest performance point\n",
    "- Strong performance at upper levels (L2/L3: 0.895)\n",
    "- Pattern matches previous folds with slight variations\n",
    "\n",
    "Neural Foraminal Narrowing (Left & Right)\n",
    "- Left side shows exceptional performance at upper levels (L1/L2: 0.967)\n",
    "- Right side maintains strong performance (L1/L2: 0.959)\n",
    "- Both sides show gradual decrease moving down the spine\n",
    "- L4/L5 performance (Left: 0.630, Right: 0.653) shows typical drop\n",
    "- L5/S1 remains challenging (Left: 0.632, Right: 0.627)\n",
    "\n",
    "Subarticular Stenosis (Left & Right)\n",
    "- Good performance in upper spine regions\n",
    "- Left side: L1/L2 (0.863), Right side: L1/L2 (0.870)\n",
    "- Consistent performance between left and right at L2/L3 (~0.80)\n",
    "- Significant drop at L4/L5 (Left: 0.503, Right: 0.549)\n",
    "- Moderate recovery at L5/S1 (Left: 0.648, Right: 0.679)\n",
    "\n",
    "General Observations\n",
    "1. Upper spine levels (L1/L2, L2/L3) consistently show strongest performance\n",
    "2. L4/L5 remains the most challenging level across all conditions\n",
    "3. Performance pattern follows similar trends to previous folds\n",
    "4. Right-sided conditions show slightly better performance at lower levels\n",
    "5. Vertical gradient pattern clearly visible, showing decreasing performance down the spine\n",
    "\n",
    "Comparison with Previous Folds\n",
    "- Overall performance aligns well with Folds 1 and 2\n",
    "- Slightly better consistency between left and right conditions\n",
    "- L4/L5 performance remains a consistent challenge\n",
    "- Upper spine level accuracy maintains high standards\n",
    "- Subarticular Stenosis shows similar patterns to previous folds\n",
    "\n",
    "The heatmap confirms the model's reliable performance patterns across different folds, suggesting robust learning of spinal pathology features, particularly in upper spinal regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ROC curves subplot\n",
    "roc_paths = [\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Left_Neural_Foraminal_Narrowing_fold_3.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Left_Subarticular_Stenosis_fold_3.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Right_Neural_Foraminal_Narrowing_fold_3.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Right_Subarticular_Stenosis_fold_3.png',\n",
    "    '/Users/danipopov/Projects/RSNA2024/plots/roc_curves/roc_curves_Spinal_Canal_Stenosis_fold_3.png'\n",
    "]\n",
    "display_fold_roc_curves(3, roc_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve Analysis - Fold 3 üìä\n",
    "\n",
    "The ROC curves illustrate the model's ability to differentiate between various severity levels across different spinal conditions. Here‚Äôs a detailed analysis:\n",
    "\n",
    "Left Neural Foraminal Narrowing\n",
    "- Excellent performance with high AUC scores\n",
    "- Mild cases: AUC = 0.90\n",
    "- Severe cases: AUC = 0.89\n",
    "- Strong early detection capability with a steep initial curve rise\n",
    "- Consistent performance across severity levels\n",
    "\n",
    "Left Subarticular Stenosis\n",
    "- Moderate performance with some challenges\n",
    "- Mild cases: AUC = 0.84\n",
    "- Severe cases: AUC = 0.73\n",
    "- The curve shows a gradual increase, indicating some difficulty in distinguishing severe cases\n",
    "- Larger gap between mild and severe detection capabilities\n",
    "\n",
    "Right Neural Foraminal Narrowing\n",
    "- Strong performance similar to the left side\n",
    "- Mild cases: AUC = 0.90\n",
    "- Severe cases: AUC = 0.83\n",
    "- Good discrimination ability for both severity levels\n",
    "- The curve indicates reliable detection, especially for mild cases\n",
    "\n",
    "Right Subarticular Stenosis\n",
    "- Lower performance compared to other conditions\n",
    "- Mild cases: AUC = 0.76\n",
    "- Severe cases: AUC = 0.73\n",
    "- The curve shows a more gradual progression, indicating lower confidence in predictions\n",
    "- More challenging to distinguish between severity levels\n",
    "\n",
    "Spinal Canal Stenosis\n",
    "- Consistent and strong performance\n",
    "- Mild cases: AUC = 0.89\n",
    "- Severe cases: AUC = 0.88\n",
    "- Smooth curve progression indicating stable predictions\n",
    "- High confidence in distinguishing between severity levels\n",
    "\n",
    "Key Observations\n",
    "1. AUC scores range from 0.73 to 0.90, indicating good to excellent classification performance.\n",
    "2. Neural Foraminal Narrowing shows the best overall performance across all severity levels.\n",
    "3. Subarticular Stenosis remains the most challenging condition, particularly for severe cases.\n",
    "4. The model demonstrates robust classification ability, especially for mild cases.\n",
    "5. Consistency in performance across folds suggests reliable clinical applicability.\n",
    "\n",
    "Overall, the model maintains strong performance in Fold 3, with some conditions showing improvement while others highlight areas for potential enhancement. The consistent high AUC scores across most conditions indicate effective learning and generalization capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions üí°\n",
    "\n",
    "In this section, we recap the steps taken to build and train the model:\n",
    "\n",
    "1. We began with the instance images and coordinates provided by the radiologist in the `train_label_coordinates.csv` file to build and train our YOLOv8 models for detecting disk levels and sides (left or right).\n",
    "2. We then used the YOLO models to create a new dataset that iterates over all patient images, attempting to detect disk levels and sides. From the images where the models successfully detected these features, we selected 5 images per plane. For each image, we extracted the bounding boxes (64x64) of the disk levels or sides to obtain the Regions of Interest (ROIs).\n",
    "3. We built a dataset class for training our model. For each study ID, we passed 60 images of ROIs across all planes: 50 images for the sagittal plane and 10 images for the axial plane. For each sagittal image, we extracted 5 ROIs of disk levels, and for the axial plane, we extracted 2 ROIs of disk sides.\n",
    "4. We chose the ResNet50 architecture as the base model and added a new head for the classification task.\n",
    "5. We trained the model for 25 epochs across 3 folds and obtained the following results:\n",
    "    - **Loss**: The model shows strong initial convergence in the first 5 epochs, with training and validation losses starting from ~0.6 and stabilizing around 0.40-0.42.\n",
    "    - **Accuracy**: The model quickly reaches ~77% accuracy, showing more fluctuation than Fold 1.\n",
    "    - **Precision and Recall**: Precision improves from 30% to 38% over the training period, while recall increases from 33% to 41-42%.\n",
    "    - **F1 Score**: The F1 score improves from 30% to 38%.\n",
    "    - **Learning Rate**: The model maintains the same cyclical learning rate strategy as in Fold 1.\n",
    "\n",
    "We notice that this approach yielded good results because we focused on helping the model learn the specific disks rather than the entire spine image, we notice that acroos all folds the model shows good and similar performance. We will also evaluate our model on the Kaggle test set to see how it performs against the competition metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
